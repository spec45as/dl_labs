{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab_rnn_maxim.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "qZavkai4ny2z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Семинар по рекуррентным нейронным сетям\n",
        "На этом семинаре мы обучим несколько рекуррентных архитектур для решения задачи сентимент-анализа, то есть предсказания метки тональности предложения.\n",
        "\n",
        "В общем случае рекуррентная нейронная сеть предназначена для обработки последовательности произвольной длины. Однако при реализации метода оказывается проще зафиксировать длину последовательности (даже в pytorch с их динамическими графами :) Поэтому мы пока поступим так, но вернемся к этому вопросу ниже.\n",
        "\n",
        "Сначала мы разберемся с RNN в pytorch, а затем сами реализуем наиболее популярную архитектуру.\n",
        "\n",
        "Задание сделано так, чтобы его можно было выполнять на CPU, однако RNN - это ресурсоемкая вещь, поэтому на GPU с ними работать приятнее. Можете попробовать использовать [https://colab.research.google.com](https://colab.research.google.com) - бесплатное облако с GPU."
      ]
    },
    {
      "metadata": {
        "id": "B8Fx5WVeny22",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Гиперпараметры"
      ]
    },
    {
      "metadata": {
        "id": "pLyheyJany23",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab_size = 20000 \n",
        "index_from = 3\n",
        "n_hidden = 128\n",
        "n_emb = 300\n",
        "seq_len = 32\n",
        "\n",
        "batch_size = 128\n",
        "learning_rate = 0.001\n",
        "num_epochs = 50\n",
        "\n",
        "use_gpu = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sbq5TEeYny2_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Загрузка данных\n",
        "Функция load_matrix_imdb скачивает матричные данные, перемешивает и загружает их в numpy-массивы.\n",
        "\n",
        "Если у вас не установлен wget, скачайте [архив imdb.npz](https://s3.amazonaws.com/text-datasets/imdb.npz)"
      ]
    },
    {
      "metadata": {
        "id": "WVAMyX5soJem",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if cuda_output and exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-L7wlD78oLEK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !pip install Pillow==4.0.0\n",
        "# !pip install PIL\n",
        "# !pip install image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fE9kgDoAoB-O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import operator\n",
        "\n",
        "def load_matrix_imdb(path='imdb.npz', num_words=None, skip_top=0,\n",
        "              maxlen=None, seed=113,\n",
        "              start_char=1, oov_char=2, index_from=3, **kwargs):\n",
        "    \"\"\"\n",
        "    Modified code from Keras\n",
        "    Loads data matrixes from npz file, crops and pads seqs and returns\n",
        "    shuffled (x_train, y_train), (x_test, y_test)\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        print(\"Downloading matrix data into current folder\")\n",
        "        os.system(\"wget https://s3.amazonaws.com/text-datasets/imdb.npz\")\n",
        "        \n",
        "    with np.load(path) as f:\n",
        "        x_train, labels_train = f['x_train'], f['y_train']\n",
        "        x_test, labels_test = f['x_test'], f['y_test']\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    indices = np.arange(len(x_train))\n",
        "    np.random.shuffle(indices)\n",
        "    x_train = x_train[indices]\n",
        "    labels_train = labels_train[indices]\n",
        "\n",
        "    indices = np.arange(len(x_test))\n",
        "    np.random.shuffle(indices)\n",
        "    x_test = x_test[indices]\n",
        "    labels_test = labels_test[indices]\n",
        "\n",
        "    xs = np.concatenate([x_train, x_test])\n",
        "    labels = np.concatenate([labels_train, labels_test])\n",
        "\n",
        "    if start_char is not None:\n",
        "        xs = [[start_char] + [w + index_from for w in x] for x in xs]\n",
        "    elif index_from:\n",
        "        xs = [[w + index_from for w in x] for x in xs]\n",
        "\n",
        "    if not num_words:\n",
        "        num_words = max([max(x) for x in xs])\n",
        "    if not maxlen:\n",
        "        maxlen = max([len(x) for x in xs])\n",
        "\n",
        "    # by convention, use 2 as OOV word\n",
        "    # reserve 'index_from' (=3 by default) characters:\n",
        "    # 0 (padding), 1 (start), 2 (OOV)\n",
        "    xs_new = []\n",
        "    for x in xs:\n",
        "        x = x[:maxlen] # crop long sequences\n",
        "        if oov_char is not None: # replace rare or frequent symbols \n",
        "            x = [w if (skip_top <= w < num_words) else oov_char for w in x]\n",
        "        else: # or filter rare and frequent symbols\n",
        "            x = [w for w in x if skip_top <= w < num_words]\n",
        "        x_padded = np.zeros(maxlen)#, dtype = 'int32')\n",
        "        x_padded[-len(x):] = x\n",
        "        xs_new.append(x_padded)    \n",
        "            \n",
        "    idx = len(x_train)\n",
        "    x_train, y_train = np.array(xs_new[:idx]), np.array(labels[:idx])\n",
        "    x_test, y_test = np.array(xs_new[idx:]), np.array(labels[idx:])\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sa21A-ohny3B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YnANGv7Fny3G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "(X_train, y_train), (X_test, y_test) = load_matrix_imdb(num_words=vocab_size,\n",
        "                                                        maxlen=seq_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C2GZ6lU0ny3M",
        "colab_type": "code",
        "outputId": "e7d95664-5063-4b03-c8ff-36166c6beda2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "set(y_train) # binary classification"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "iyAg6hIvny3R",
        "colab_type": "code",
        "outputId": "818f1838-e055-435a-d288-7e4b073485f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((25000, 32), (25000, 32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "bP7HxYTOny3X",
        "colab_type": "code",
        "outputId": "e0a5e9b3-087a-406c-a33a-c30f3c02ff56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "cell_type": "code",
      "source": [
        "X_train[0] # sequence of coded words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.000e+00, 1.400e+01, 2.200e+01, 1.600e+01, 4.300e+01, 5.300e+02,\n",
              "       9.730e+02, 1.622e+03, 1.385e+03, 6.500e+01, 4.580e+02, 4.468e+03,\n",
              "       6.600e+01, 3.941e+03, 4.000e+00, 1.730e+02, 3.600e+01, 2.560e+02,\n",
              "       5.000e+00, 2.500e+01, 1.000e+02, 4.300e+01, 8.380e+02, 1.120e+02,\n",
              "       5.000e+01, 6.700e+02, 2.000e+00, 9.000e+00, 3.500e+01, 4.800e+02,\n",
              "       2.840e+02, 5.000e+00])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "GuaMcE8eny3q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dset = torch.utils.data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "test_dset = torch.utils.data.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "                                           \n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CvaRLY6kny31",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Сборка и обучение RNN в pytorch"
      ]
    },
    {
      "metadata": {
        "id": "GNK3XJutny32",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wCEkcHB3ny38",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Наша нейросеть будет обрабатывать входную последовательность по словам (word level). Мы будем использовать простую и стандарную рекуррентную архитектуру для сентимент-анализа: слой представлений, слой LSTM и полносвязный слой, предсказывающий выход по последнему скрытому состоянию.\n",
        "\n",
        "Ниже даны шаблоны реализации нейросети и ее обучения. Допишите класс и функции обучения так, чтобы класс реализовывал описанную архитектуру, а вызов функции train не выдавал ошибок :)"
      ]
    },
    {
      "metadata": {
        "id": "hr9-Lg-Zny3-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size, \\\n",
        "                 batch_size, use_gpu, rec_layer, dropout):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.use_gpu = use_gpu\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = rec_layer(embedding_dim, hidden_dim, dropout=dropout)\n",
        "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
        "    ### your code here\n",
        "        self.hidden = self.init_hidden()\n",
        "      \n",
        "    def init_hidden(self):\n",
        "      h_0 = torch.zeros(1, self.batch_size, self.hidden_dim)\n",
        "      c_0 = torch.zeros(1, self.batch_size, self.hidden_dim)\n",
        "      if self.use_gpu:\n",
        "        h_0 = h_0.cuda()\n",
        "        c_0 = c_0.cuda()\n",
        "        \n",
        "      return (h_0, c_0)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "      lstm_output, self.hidden = self.lstm(self.word_embeddings(sentence), self.hidden)\n",
        "      return torch.sigmoid(self.hidden2label(lstm_output[-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VeDdVacM4WiS",
        "colab_type": "code",
        "outputId": "2838c538-9e63-4848-df54-16b8300cf92a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "cell_type": "code",
      "source": [
        "# raise Exception(\"pause plz\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b273af6c25df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pause plz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mException\u001b[0m: pause plz"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "KTWqynHkny4D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = LSTMClassifier(embedding_dim=n_emb,\n",
        "                             hidden_dim=n_hidden,\n",
        "                              vocab_size=vocab_size,\n",
        "                              label_size=1,\n",
        "                             batch_size=batch_size, \n",
        "                             use_gpu=use_gpu,\n",
        "                             rec_layer = nn.LSTM)\n",
        "if use_gpu:\n",
        "    model = model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gBBtDlKZny4J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Исходный код LSTM](http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM)"
      ]
    },
    {
      "metadata": {
        "id": "trkGVdUjny4K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#?model.lstm.forward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G1LTSjW_ny4R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "lossfun = nn.BCELoss(reduction='sum')\n",
        "#lossfun = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Is6mxfulny4Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_epoch(loader, model, lossfun, optimizer, use_gpu):\n",
        "    model.train()\n",
        "    for train_inputs, train_labels in loader:\n",
        "        train_labels = torch.squeeze(train_labels)\n",
        "\n",
        "        if use_gpu:\n",
        "            train_inputs, train_labels = train_inputs.cuda(), train_labels.cuda()\n",
        "\n",
        "        model.zero_grad()\n",
        "        model.batch_size = len(train_labels)\n",
        "        model.hidden = model.init_hidden()\n",
        "     \n",
        "        output = model(train_inputs.long().t())\n",
        "#         print(output.view(-1).shape)\n",
        "#         print(train_labels.shape)\n",
        "        \n",
        "        loss = lossfun(output.view(-1), train_labels.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        del train_inputs, train_labels\n",
        "        if use_gpu:\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "def evaluate(loader, model, lossfun, use_gpu):\n",
        "    model.eval()\n",
        "    total_acc = 0.0\n",
        "    total_loss = 0.0\n",
        "    total = 0.0\n",
        "    \n",
        "    for train_inputs, train_labels in loader:\n",
        "      with torch.no_grad():\n",
        "        train_labels = torch.squeeze(train_labels)\n",
        "        if use_gpu:\n",
        "          train_inputs, train_labels = train_inputs.cuda(), train_labels.cuda()\n",
        "\n",
        "        ### your code here\n",
        "        model.batch_size = len(train_labels)\n",
        "        model.hidden = model.init_hidden()\n",
        "        output = model(train_inputs.long().t())\n",
        "\n",
        "        loss = lossfun(output.view(-1), train_labels.float())\n",
        "        total_loss = total_loss + loss.data.item()\n",
        "\n",
        "        # calc testing acc\n",
        "        ### your code here\n",
        "        if not use_gpu:\n",
        "          total_acc = total_acc + (train_labels.numpy() == (output > 0.5).data.long().view(-1).numpy()).sum()\n",
        "        else:\n",
        "          total_acc = total_acc + (train_labels.cpu().numpy() == (output > 0.5).data.long().view(-1).cpu().numpy()).sum()\n",
        "        \n",
        "        total = total + len(train_labels)\n",
        "        \n",
        "        del train_inputs, train_labels\n",
        "        if use_gpu:\n",
        "          torch.cuda.empty_cache()\n",
        "        \n",
        "    return total_loss / total, total_acc / total\n",
        "\n",
        "def train(train_loader, test_loader, model, lossfun, optimizer, use_gpu, num_epochs):\n",
        "    train_loss_ = []\n",
        "    test_loss_ = []\n",
        "    train_acc_ = []\n",
        "    test_acc_ = []\n",
        "    for epoch in range(num_epochs):\n",
        "        train_epoch(train_loader, model, lossfun, optimizer, use_gpu)\n",
        "        train_loss, train_acc = evaluate(train_loader, model, lossfun, use_gpu)\n",
        "        train_loss_.append(train_loss)\n",
        "        train_acc_.append(train_acc)\n",
        "        test_loss, test_acc = evaluate(test_loader, model, lossfun, use_gpu)\n",
        "        test_loss_.append(test_loss)\n",
        "        test_acc_.append(test_acc)\n",
        "\n",
        "        print('[Epoch: %3d/%3d] Training Loss: %.3f, Testing Loss: %.3f, Training Acc: %.3f, Testing Acc: %.3f'\n",
        "              % (epoch, num_epochs, train_loss_[epoch], test_loss_[epoch], train_acc_[epoch], test_acc_[epoch]))\n",
        "    return train_loss_, train_acc_, test_loss_, test_acc_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AmFvYARJny4n",
        "colab_type": "code",
        "outputId": "e2501888-cc57-4069-f8a8-d9f9931aa496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "cell_type": "code",
      "source": [
        "a, b, c, d = train(train_loader, test_loader, model, lossfun, optimizer, use_gpu, 15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch:   0/ 15] Training Loss: 0.575, Testing Loss: 0.610, Training Acc: 0.725, Testing Acc: 0.671\n",
            "[Epoch:   1/ 15] Training Loss: 0.424, Testing Loss: 0.560, Training Acc: 0.821, Testing Acc: 0.708\n",
            "[Epoch:   2/ 15] Training Loss: 0.317, Testing Loss: 0.579, Training Acc: 0.882, Testing Acc: 0.710\n",
            "[Epoch:   3/ 15] Training Loss: 0.184, Testing Loss: 0.658, Training Acc: 0.941, Testing Acc: 0.715\n",
            "[Epoch:   4/ 15] Training Loss: 0.108, Testing Loss: 0.785, Training Acc: 0.971, Testing Acc: 0.712\n",
            "[Epoch:   5/ 15] Training Loss: 0.083, Testing Loss: 0.928, Training Acc: 0.975, Testing Acc: 0.710\n",
            "[Epoch:   6/ 15] Training Loss: 0.052, Testing Loss: 1.156, Training Acc: 0.985, Testing Acc: 0.712\n",
            "[Epoch:   7/ 15] Training Loss: 0.035, Testing Loss: 1.237, Training Acc: 0.991, Testing Acc: 0.701\n",
            "[Epoch:   8/ 15] Training Loss: 0.023, Testing Loss: 1.400, Training Acc: 0.994, Testing Acc: 0.708\n",
            "[Epoch:   9/ 15] Training Loss: 0.011, Testing Loss: 1.514, Training Acc: 0.998, Testing Acc: 0.714\n",
            "[Epoch:  10/ 15] Training Loss: 0.016, Testing Loss: 1.610, Training Acc: 0.996, Testing Acc: 0.695\n",
            "[Epoch:  11/ 15] Training Loss: 0.009, Testing Loss: 1.817, Training Acc: 0.998, Testing Acc: 0.708\n",
            "[Epoch:  12/ 15] Training Loss: 0.014, Testing Loss: 1.835, Training Acc: 0.997, Testing Acc: 0.708\n",
            "[Epoch:  13/ 15] Training Loss: 0.007, Testing Loss: 1.892, Training Acc: 0.998, Testing Acc: 0.704\n",
            "[Epoch:  14/ 15] Training Loss: 0.005, Testing Loss: 1.753, Training Acc: 0.999, Testing Acc: 0.705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bhricBJgny4u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Нерегуляризованные LSTM часто быстро переобучаются. Чтобы с этим бороться, часто используют L2-регуляризацию и дропаут.\n",
        "Однако способов накладывать дропаут на рекуррентный слой достаточно много, и далеко не все хорошо работают. Мы реализуем дропаут, описанный в [статье Гала и Гарамани](https://arxiv.org/abs/1512.05287).\n",
        "Для этого нам потребуется самостоятельно реализовать LSTM."
      ]
    },
    {
      "metadata": {
        "id": "R108_5o-ny4y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Самостоятельная реализация LSTM\n",
        "\n",
        "Для начала реализуйте LSTM, не обращая внимание на параметр dropout, и протестируйте модель. На каждом шаге ячейка LSTM обновляет скрытое состояние и память по следующим формулам:\n",
        "$$\n",
        "i = \\sigma(h_{t-1}W^i + x_t U^i+b_i) \\quad\n",
        "o = \\sigma(h_{t-1}W^o + x_t U^o+b_o) \n",
        "$$\n",
        "$$\n",
        "f = \\sigma(h_{t-1}W^f + x_t U^f+b_f) \\quad \n",
        "g = tanh(h_{t-1} W^g + x_t U^g+b_g) \n",
        "$$\n",
        "$$\n",
        "c_t = f \\odot c_{t-1} +  i \\odot  g \\quad\n",
        "h_t =  o \\odot tanh(c_t) \\nonumber\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "id": "Alextc0mny5I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Теперь реализуйте дропаут для рекуррентного слоя. Как и в сетях прямого распространения, дропаут можно накладывать на вход и скрытое состояние ($x_t$ и $h_t$). Ключевая идея дропаута Гала состоит в том, что бинарная маска должна быть одинаковая для всех моментов времени (но своя для разных объектов). Кроме того, статья утверждает, что одновременно с бинарным дропаутом нужно использовать L$_2$-регуляризацию. Ее тоже можно включить (параметр weight_decay в оптимизаторе).\n",
        "\n",
        "Формулы ячейки LSTM с бинарным дропаутом ($b_x$ и $b_h$ - бинарные маски):\n",
        "\n",
        "$$\n",
        "i = \\sigma((h_{t-1} \\odot b_h) W^i + (x_t \\odot b_x) U^i+b_i) \\quad\n",
        "o = \\sigma((h_{t-1} \\odot b_h)W^o + (x_t \\odot b_x) U^o+b_o) \n",
        "$$\n",
        "$$\n",
        "f = \\sigma((h_{t-1} \\odot b_h)W^f + (x_t \\odot b_x) U^f+b_f) \\quad \n",
        "g = tanh((h_{t-1} \\odot b_h) W^g + (x_t \\odot b_x) U^g+b_g) \n",
        "$$\n",
        "$$\n",
        "c_t = f \\odot c_{t-1} +  i \\odot  g \\quad\n",
        "h_t =  o \\odot tanh(c_t)\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "id": "11gGUZfmny41",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.nn.parameter import Parameter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XYCrblKnFUz3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout=0):\n",
        "        super(MyLSTM, self).__init__()\n",
        "        ### your code here\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout = dropout\n",
        "          \n",
        "        if use_gpu:\n",
        "          self.u_input_weights = nn.Linear(input_size, 4 * hidden_size).cuda()\n",
        "          self.w_hidden_weights = nn.Linear(hidden_size, 4 * hidden_size).cuda()\n",
        "        else:\n",
        "          self.u_input_weights = nn.Linear(input_size, 4 * hidden_size)\n",
        "          self.w_hidden_weights = nn.Linear(hidden_size, 4 * hidden_size)\n",
        "        self.reset_params()\n",
        "        \n",
        "        \n",
        "    def reset_params(self):\n",
        "        \"\"\"\n",
        "        initialization as in Pytorch\n",
        "        do not forget to call this method!\n",
        "        \"\"\"\n",
        "        stdv = 1.0 / np.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "            \n",
        "    def forward(self, input, hidden):\n",
        "        ### your code here\n",
        "        h = hidden[0]\n",
        "        c = hidden[1]\n",
        "        \n",
        "        if self.training and self.dropout > 0.0:\n",
        "          dropout = nn.Dropout(p=self.dropout)\n",
        "          if use_gpu:\n",
        "            b_x = dropout(torch.ones(self.input_size)).cuda()\n",
        "            b_h = dropout(torch.ones_like(h)).cuda()\n",
        "          else:\n",
        "            b_x = dropout(torch.ones(self.input_size))\n",
        "            b_h = dropout(torch.ones_like(h))\n",
        "          \n",
        "        for ix in range(len(input)):\n",
        "          x = input[ix]\n",
        "          \n",
        "          if self.training and self.dropout > 0.0:\n",
        "            lstm_gates = (self.u_input_weights(x*b_x) + self.w_hidden_weights(h*b_h)).chunk(chunks=4, dim=2)\n",
        "          else:\n",
        "            lstm_gates = (self.u_input_weights(x) + self.w_hidden_weights(h)).chunk(chunks=4, dim=2)\n",
        "            \n",
        "          #https://developer.nvidia.com/sites/default/files/pictures/2018/lstm.png\n",
        "          lstm_gates = (self.u_input_weights(x) + self.w_hidden_weights(h)).chunk(chunks=4, dim=2)\n",
        "          input_gate, forget_gate, cell_gate, output_gate = lstm_gates\n",
        "          \n",
        "          input_gate = torch.sigmoid(input_gate)\n",
        "          forget_gate = torch.sigmoid(forget_gate)\n",
        "          cell_gate = torch.tanh(cell_gate)\n",
        "          output_gate = torch.sigmoid(output_gate)\n",
        "        \n",
        "          c = (forget_gate * c) + (input_gate * cell_gate)\n",
        "          h = output_gate * torch.tanh(c)\n",
        "          \n",
        "          if ix == 0:\n",
        "            h_output = h\n",
        "            c_output = c\n",
        "          else:\n",
        "            h_output = torch.cat([h_output, h], 0)\n",
        "\n",
        "        return h_output, c_output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lRli3WTeny5O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Протестируйте полученную модель, сравните итоговое качество на тестовой выборке с нерегуляризованной моделью."
      ]
    },
    {
      "metadata": {
        "id": "1DXRF8x9MyKb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DROP_OUT_VALUE = [0, 0.25, 0.5, 0.75, 0.9]\n",
        "WEIGHT_DECAY = [0, 0.01, 0.001, 0.0001]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6-A_-4AhNEpZ",
        "colab_type": "code",
        "outputId": "9e8e1bd9-7321-4bf4-8446-755c98524c4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4417
        }
      },
      "cell_type": "code",
      "source": [
        "for dropout_val in DROP_OUT_VALUE:\n",
        "  for weight_decay_val in WEIGHT_DECAY:\n",
        "    model = LSTMClassifier(embedding_dim=n_emb,\n",
        "                         hidden_dim=n_hidden,\n",
        "                          vocab_size=vocab_size,\n",
        "                          label_size=1,\n",
        "                         batch_size=batch_size, \n",
        "                         use_gpu=use_gpu,\n",
        "                         rec_layer=MyLSTM,\n",
        "                         dropout=dropout_val\n",
        "                          )\n",
        "    if use_gpu:\n",
        "        model = model.cuda()\n",
        "        \n",
        "    if weight_decay_val:\n",
        "      optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay_val)\n",
        "    else:\n",
        "      optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    loss_function = nn.BCELoss(size_average=False)\n",
        "    print(\"Experiment run for dropout = {}, weight_decay = {}\".format(dropout_val, weight_decay_val))\n",
        "    a, b, c, d = train(train_loader, test_loader, model, lossfun, optimizer, use_gpu, 10)\n",
        "    print()\n",
        "    "
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Experiment run for dropout = 0, weight_decay = 0\n",
            "[Epoch:   0/ 10] Training Loss: 0.568, Testing Loss: 0.612, Training Acc: 0.726, Testing Acc: 0.669\n",
            "[Epoch:   1/ 10] Training Loss: 0.407, Testing Loss: 0.562, Training Acc: 0.827, Testing Acc: 0.710\n",
            "[Epoch:   2/ 10] Training Loss: 0.311, Testing Loss: 0.612, Training Acc: 0.872, Testing Acc: 0.719\n",
            "[Epoch:   3/ 10] Training Loss: 0.187, Testing Loss: 0.663, Training Acc: 0.935, Testing Acc: 0.717\n",
            "[Epoch:   4/ 10] Training Loss: 0.115, Testing Loss: 0.779, Training Acc: 0.966, Testing Acc: 0.713\n",
            "[Epoch:   5/ 10] Training Loss: 0.057, Testing Loss: 1.037, Training Acc: 0.984, Testing Acc: 0.714\n",
            "[Epoch:   6/ 10] Training Loss: 0.041, Testing Loss: 1.061, Training Acc: 0.991, Testing Acc: 0.708\n",
            "[Epoch:   7/ 10] Training Loss: 0.024, Testing Loss: 1.340, Training Acc: 0.994, Testing Acc: 0.707\n",
            "[Epoch:   8/ 10] Training Loss: 0.017, Testing Loss: 1.407, Training Acc: 0.997, Testing Acc: 0.707\n",
            "[Epoch:   9/ 10] Training Loss: 0.012, Testing Loss: 1.457, Training Acc: 0.998, Testing Acc: 0.706\n",
            "\n",
            "Experiment run for dropout = 0, weight_decay = 0.01\n",
            "[Epoch:   0/ 10] Training Loss: 0.606, Testing Loss: 0.629, Training Acc: 0.675, Testing Acc: 0.645\n",
            "[Epoch:   1/ 10] Training Loss: 0.498, Testing Loss: 0.568, Training Acc: 0.760, Testing Acc: 0.697\n",
            "[Epoch:   2/ 10] Training Loss: 0.430, Testing Loss: 0.565, Training Acc: 0.804, Testing Acc: 0.715\n",
            "[Epoch:   3/ 10] Training Loss: 0.395, Testing Loss: 0.566, Training Acc: 0.823, Testing Acc: 0.712\n",
            "[Epoch:   4/ 10] Training Loss: 0.308, Testing Loss: 0.578, Training Acc: 0.876, Testing Acc: 0.720\n",
            "[Epoch:   5/ 10] Training Loss: 0.258, Testing Loss: 0.626, Training Acc: 0.903, Testing Acc: 0.714\n",
            "[Epoch:   6/ 10] Training Loss: 0.208, Testing Loss: 0.682, Training Acc: 0.920, Testing Acc: 0.719\n",
            "[Epoch:   7/ 10] Training Loss: 0.151, Testing Loss: 0.706, Training Acc: 0.956, Testing Acc: 0.714\n",
            "[Epoch:   8/ 10] Training Loss: 0.150, Testing Loss: 0.902, Training Acc: 0.947, Testing Acc: 0.703\n",
            "[Epoch:   9/ 10] Training Loss: 0.088, Testing Loss: 0.914, Training Acc: 0.974, Testing Acc: 0.709\n",
            "\n",
            "Experiment run for dropout = 0, weight_decay = 0.001\n",
            "[Epoch:   0/ 10] Training Loss: 0.566, Testing Loss: 0.607, Training Acc: 0.712, Testing Acc: 0.674\n",
            "[Epoch:   1/ 10] Training Loss: 0.470, Testing Loss: 0.568, Training Acc: 0.795, Testing Acc: 0.700\n",
            "[Epoch:   2/ 10] Training Loss: 0.369, Testing Loss: 0.574, Training Acc: 0.843, Testing Acc: 0.708\n",
            "[Epoch:   3/ 10] Training Loss: 0.240, Testing Loss: 0.622, Training Acc: 0.910, Testing Acc: 0.721\n",
            "[Epoch:   4/ 10] Training Loss: 0.172, Testing Loss: 0.670, Training Acc: 0.944, Testing Acc: 0.719\n",
            "[Epoch:   5/ 10] Training Loss: 0.121, Testing Loss: 0.848, Training Acc: 0.964, Testing Acc: 0.707\n",
            "[Epoch:   6/ 10] Training Loss: 0.069, Testing Loss: 0.992, Training Acc: 0.981, Testing Acc: 0.712\n",
            "[Epoch:   7/ 10] Training Loss: 0.063, Testing Loss: 1.008, Training Acc: 0.985, Testing Acc: 0.703\n",
            "[Epoch:   8/ 10] Training Loss: 0.045, Testing Loss: 1.098, Training Acc: 0.991, Testing Acc: 0.708\n",
            "[Epoch:   9/ 10] Training Loss: 0.030, Testing Loss: 1.457, Training Acc: 0.993, Testing Acc: 0.711\n",
            "\n",
            "Experiment run for dropout = 0, weight_decay = 0.0001\n",
            "[Epoch:   0/ 10] Training Loss: 0.582, Testing Loss: 0.624, Training Acc: 0.706, Testing Acc: 0.657\n",
            "[Epoch:   1/ 10] Training Loss: 0.426, Testing Loss: 0.557, Training Acc: 0.820, Testing Acc: 0.710\n",
            "[Epoch:   2/ 10] Training Loss: 0.319, Testing Loss: 0.561, Training Acc: 0.887, Testing Acc: 0.714\n",
            "[Epoch:   3/ 10] Training Loss: 0.195, Testing Loss: 0.655, Training Acc: 0.934, Testing Acc: 0.716\n",
            "[Epoch:   4/ 10] Training Loss: 0.134, Testing Loss: 0.769, Training Acc: 0.954, Testing Acc: 0.712\n",
            "[Epoch:   5/ 10] Training Loss: 0.086, Testing Loss: 0.827, Training Acc: 0.980, Testing Acc: 0.705\n",
            "[Epoch:   6/ 10] Training Loss: 0.044, Testing Loss: 1.333, Training Acc: 0.987, Testing Acc: 0.712\n",
            "[Epoch:   7/ 10] Training Loss: 0.026, Testing Loss: 1.309, Training Acc: 0.995, Testing Acc: 0.706\n",
            "[Epoch:   8/ 10] Training Loss: 0.017, Testing Loss: 1.400, Training Acc: 0.997, Testing Acc: 0.706\n",
            "[Epoch:   9/ 10] Training Loss: 0.019, Testing Loss: 1.632, Training Acc: 0.996, Testing Acc: 0.709\n",
            "\n",
            "Experiment run for dropout = 0.25, weight_decay = 0\n",
            "[Epoch:   0/ 10] Training Loss: 0.556, Testing Loss: 0.603, Training Acc: 0.724, Testing Acc: 0.670\n",
            "[Epoch:   1/ 10] Training Loss: 0.396, Testing Loss: 0.567, Training Acc: 0.831, Testing Acc: 0.710\n",
            "[Epoch:   2/ 10] Training Loss: 0.290, Testing Loss: 0.580, Training Acc: 0.894, Testing Acc: 0.712\n",
            "[Epoch:   3/ 10] Training Loss: 0.179, Testing Loss: 0.681, Training Acc: 0.938, Testing Acc: 0.714\n",
            "[Epoch:   4/ 10] Training Loss: 0.130, Testing Loss: 0.770, Training Acc: 0.961, Testing Acc: 0.702\n",
            "[Epoch:   5/ 10] Training Loss: 0.065, Testing Loss: 0.918, Training Acc: 0.985, Testing Acc: 0.710\n",
            "[Epoch:   6/ 10] Training Loss: 0.037, Testing Loss: 1.118, Training Acc: 0.993, Testing Acc: 0.711\n",
            "[Epoch:   7/ 10] Training Loss: 0.020, Testing Loss: 1.397, Training Acc: 0.996, Testing Acc: 0.708\n",
            "[Epoch:   8/ 10] Training Loss: 0.019, Testing Loss: 1.495, Training Acc: 0.995, Testing Acc: 0.706\n",
            "[Epoch:   9/ 10] Training Loss: 0.006, Testing Loss: 1.579, Training Acc: 0.999, Testing Acc: 0.707\n",
            "\n",
            "Experiment run for dropout = 0.25, weight_decay = 0.01\n",
            "[Epoch:   0/ 10] Training Loss: 0.613, Testing Loss: 0.636, Training Acc: 0.675, Testing Acc: 0.635\n",
            "[Epoch:   1/ 10] Training Loss: 0.519, Testing Loss: 0.591, Training Acc: 0.755, Testing Acc: 0.690\n",
            "[Epoch:   2/ 10] Training Loss: 0.463, Testing Loss: 0.564, Training Acc: 0.801, Testing Acc: 0.700\n",
            "[Epoch:   3/ 10] Training Loss: 0.378, Testing Loss: 0.585, Training Acc: 0.833, Testing Acc: 0.714\n",
            "[Epoch:   4/ 10] Training Loss: 0.307, Testing Loss: 0.594, Training Acc: 0.877, Testing Acc: 0.714\n",
            "[Epoch:   5/ 10] Training Loss: 0.272, Testing Loss: 0.586, Training Acc: 0.907, Testing Acc: 0.712\n",
            "[Epoch:   6/ 10] Training Loss: 0.228, Testing Loss: 0.713, Training Acc: 0.910, Testing Acc: 0.695\n",
            "[Epoch:   7/ 10] Training Loss: 0.170, Testing Loss: 0.705, Training Acc: 0.951, Testing Acc: 0.701\n",
            "[Epoch:   8/ 10] Training Loss: 0.109, Testing Loss: 0.976, Training Acc: 0.962, Testing Acc: 0.705\n",
            "[Epoch:   9/ 10] Training Loss: 0.086, Testing Loss: 0.946, Training Acc: 0.976, Testing Acc: 0.701\n",
            "\n",
            "Experiment run for dropout = 0.25, weight_decay = 0.001\n",
            "[Epoch:   0/ 10] Training Loss: 0.582, Testing Loss: 0.616, Training Acc: 0.692, Testing Acc: 0.655\n",
            "[Epoch:   1/ 10] Training Loss: 0.453, Testing Loss: 0.560, Training Acc: 0.796, Testing Acc: 0.707\n",
            "[Epoch:   2/ 10] Training Loss: 0.364, Testing Loss: 0.589, Training Acc: 0.835, Testing Acc: 0.717\n",
            "[Epoch:   3/ 10] Training Loss: 0.258, Testing Loss: 0.646, Training Acc: 0.898, Testing Acc: 0.711\n",
            "[Epoch:   4/ 10] Training Loss: 0.160, Testing Loss: 0.728, Training Acc: 0.945, Testing Acc: 0.720\n",
            "[Epoch:   5/ 10] Training Loss: 0.102, Testing Loss: 0.823, Training Acc: 0.971, Testing Acc: 0.711\n",
            "[Epoch:   6/ 10] Training Loss: 0.073, Testing Loss: 0.875, Training Acc: 0.984, Testing Acc: 0.706\n",
            "[Epoch:   7/ 10] Training Loss: 0.054, Testing Loss: 1.105, Training Acc: 0.986, Testing Acc: 0.710\n",
            "[Epoch:   8/ 10] Training Loss: 0.037, Testing Loss: 1.220, Training Acc: 0.991, Testing Acc: 0.711\n",
            "[Epoch:   9/ 10] Training Loss: 0.025, Testing Loss: 1.241, Training Acc: 0.996, Testing Acc: 0.710\n",
            "\n",
            "Experiment run for dropout = 0.25, weight_decay = 0.0001\n",
            "[Epoch:   0/ 10] Training Loss: 0.590, Testing Loss: 0.623, Training Acc: 0.695, Testing Acc: 0.649\n",
            "[Epoch:   1/ 10] Training Loss: 0.445, Testing Loss: 0.570, Training Acc: 0.804, Testing Acc: 0.704\n",
            "[Epoch:   2/ 10] Training Loss: 0.323, Testing Loss: 0.571, Training Acc: 0.876, Testing Acc: 0.715\n",
            "[Epoch:   3/ 10] Training Loss: 0.224, Testing Loss: 0.640, Training Acc: 0.927, Testing Acc: 0.716\n",
            "[Epoch:   4/ 10] Training Loss: 0.125, Testing Loss: 0.791, Training Acc: 0.961, Testing Acc: 0.718\n",
            "[Epoch:   5/ 10] Training Loss: 0.070, Testing Loss: 0.950, Training Acc: 0.981, Testing Acc: 0.714\n",
            "[Epoch:   6/ 10] Training Loss: 0.043, Testing Loss: 1.083, Training Acc: 0.991, Testing Acc: 0.707\n",
            "[Epoch:   7/ 10] Training Loss: 0.047, Testing Loss: 1.268, Training Acc: 0.988, Testing Acc: 0.697\n",
            "[Epoch:   8/ 10] Training Loss: 0.018, Testing Loss: 1.329, Training Acc: 0.997, Testing Acc: 0.711\n",
            "[Epoch:   9/ 10] Training Loss: 0.038, Testing Loss: 1.383, Training Acc: 0.990, Testing Acc: 0.694\n",
            "\n",
            "Experiment run for dropout = 0.5, weight_decay = 0\n",
            "[Epoch:   0/ 10] Training Loss: 0.563, Testing Loss: 0.609, Training Acc: 0.715, Testing Acc: 0.670\n",
            "[Epoch:   1/ 10] Training Loss: 0.407, Testing Loss: 0.572, Training Acc: 0.816, Testing Acc: 0.713\n",
            "[Epoch:   2/ 10] Training Loss: 0.292, Testing Loss: 0.608, Training Acc: 0.886, Testing Acc: 0.719\n",
            "[Epoch:   3/ 10] Training Loss: 0.199, Testing Loss: 0.629, Training Acc: 0.937, Testing Acc: 0.715\n",
            "[Epoch:   4/ 10] Training Loss: 0.124, Testing Loss: 0.733, Training Acc: 0.967, Testing Acc: 0.710\n",
            "[Epoch:   5/ 10] Training Loss: 0.068, Testing Loss: 0.954, Training Acc: 0.983, Testing Acc: 0.712\n",
            "[Epoch:   6/ 10] Training Loss: 0.047, Testing Loss: 1.108, Training Acc: 0.988, Testing Acc: 0.703\n",
            "[Epoch:   7/ 10] Training Loss: 0.040, Testing Loss: 1.305, Training Acc: 0.990, Testing Acc: 0.700\n",
            "[Epoch:   8/ 10] Training Loss: 0.018, Testing Loss: 1.299, Training Acc: 0.998, Testing Acc: 0.702\n",
            "[Epoch:   9/ 10] Training Loss: 0.017, Testing Loss: 1.348, Training Acc: 0.997, Testing Acc: 0.699\n",
            "\n",
            "Experiment run for dropout = 0.5, weight_decay = 0.01\n",
            "[Epoch:   0/ 10] Training Loss: 0.593, Testing Loss: 0.625, Training Acc: 0.688, Testing Acc: 0.655\n",
            "[Epoch:   1/ 10] Training Loss: 0.491, Testing Loss: 0.582, Training Acc: 0.761, Testing Acc: 0.693\n",
            "[Epoch:   2/ 10] Training Loss: 0.421, Testing Loss: 0.572, Training Acc: 0.804, Testing Acc: 0.711\n",
            "[Epoch:   3/ 10] Training Loss: 0.361, Testing Loss: 0.564, Training Acc: 0.846, Testing Acc: 0.718\n",
            "[Epoch:   4/ 10] Training Loss: 0.302, Testing Loss: 0.587, Training Acc: 0.882, Testing Acc: 0.718\n",
            "[Epoch:   5/ 10] Training Loss: 0.258, Testing Loss: 0.595, Training Acc: 0.912, Testing Acc: 0.715\n",
            "[Epoch:   6/ 10] Training Loss: 0.222, Testing Loss: 0.688, Training Acc: 0.919, Testing Acc: 0.702\n",
            "[Epoch:   7/ 10] Training Loss: 0.137, Testing Loss: 0.732, Training Acc: 0.959, Testing Acc: 0.715\n",
            "[Epoch:   8/ 10] Training Loss: 0.103, Testing Loss: 0.942, Training Acc: 0.968, Testing Acc: 0.706\n",
            "[Epoch:   9/ 10] Training Loss: 0.077, Testing Loss: 0.920, Training Acc: 0.979, Testing Acc: 0.711\n",
            "\n",
            "Experiment run for dropout = 0.5, weight_decay = 0.001\n",
            "[Epoch:   0/ 10] Training Loss: 0.573, Testing Loss: 0.609, Training Acc: 0.712, Testing Acc: 0.674\n",
            "[Epoch:   1/ 10] Training Loss: 0.439, Testing Loss: 0.562, Training Acc: 0.801, Testing Acc: 0.713\n",
            "[Epoch:   2/ 10] Training Loss: 0.333, Testing Loss: 0.567, Training Acc: 0.861, Testing Acc: 0.726\n",
            "[Epoch:   3/ 10] Training Loss: 0.265, Testing Loss: 0.599, Training Acc: 0.904, Testing Acc: 0.714\n",
            "[Epoch:   4/ 10] Training Loss: 0.173, Testing Loss: 0.719, Training Acc: 0.944, Testing Acc: 0.712\n",
            "[Epoch:   5/ 10] Training Loss: 0.107, Testing Loss: 0.881, Training Acc: 0.967, Testing Acc: 0.710\n",
            "[Epoch:   6/ 10] Training Loss: 0.072, Testing Loss: 1.009, Training Acc: 0.980, Testing Acc: 0.713\n",
            "[Epoch:   7/ 10] Training Loss: 0.058, Testing Loss: 1.000, Training Acc: 0.987, Testing Acc: 0.709\n",
            "[Epoch:   8/ 10] Training Loss: 0.038, Testing Loss: 1.153, Training Acc: 0.992, Testing Acc: 0.711\n",
            "[Epoch:   9/ 10] Training Loss: 0.026, Testing Loss: 1.302, Training Acc: 0.995, Testing Acc: 0.709\n",
            "\n",
            "Experiment run for dropout = 0.5, weight_decay = 0.0001\n",
            "[Epoch:   0/ 10] Training Loss: 0.595, Testing Loss: 0.629, Training Acc: 0.696, Testing Acc: 0.658\n",
            "[Epoch:   1/ 10] Training Loss: 0.429, Testing Loss: 0.562, Training Acc: 0.811, Testing Acc: 0.707\n",
            "[Epoch:   2/ 10] Training Loss: 0.304, Testing Loss: 0.583, Training Acc: 0.882, Testing Acc: 0.717\n",
            "[Epoch:   3/ 10] Training Loss: 0.205, Testing Loss: 0.637, Training Acc: 0.931, Testing Acc: 0.716\n",
            "[Epoch:   4/ 10] Training Loss: 0.126, Testing Loss: 0.763, Training Acc: 0.962, Testing Acc: 0.713\n",
            "[Epoch:   5/ 10] Training Loss: 0.065, Testing Loss: 1.025, Training Acc: 0.982, Testing Acc: 0.712\n",
            "[Epoch:   6/ 10] Training Loss: 0.038, Testing Loss: 1.144, Training Acc: 0.991, Testing Acc: 0.709\n",
            "[Epoch:   7/ 10] Training Loss: 0.028, Testing Loss: 1.272, Training Acc: 0.994, Testing Acc: 0.707\n",
            "[Epoch:   8/ 10] Training Loss: 0.033, Testing Loss: 1.305, Training Acc: 0.991, Testing Acc: 0.705\n",
            "[Epoch:   9/ 10] Training Loss: 0.014, Testing Loss: 1.466, Training Acc: 0.997, Testing Acc: 0.707\n",
            "\n",
            "Experiment run for dropout = 0.75, weight_decay = 0\n",
            "[Epoch:   0/ 10] Training Loss: 0.577, Testing Loss: 0.617, Training Acc: 0.709, Testing Acc: 0.672\n",
            "[Epoch:   1/ 10] Training Loss: 0.444, Testing Loss: 0.573, Training Acc: 0.812, Testing Acc: 0.696\n",
            "[Epoch:   2/ 10] Training Loss: 0.316, Testing Loss: 0.563, Training Acc: 0.884, Testing Acc: 0.716\n",
            "[Epoch:   3/ 10] Training Loss: 0.201, Testing Loss: 0.625, Training Acc: 0.934, Testing Acc: 0.713\n",
            "[Epoch:   4/ 10] Training Loss: 0.111, Testing Loss: 0.796, Training Acc: 0.965, Testing Acc: 0.716\n",
            "[Epoch:   5/ 10] Training Loss: 0.087, Testing Loss: 0.981, Training Acc: 0.974, Testing Acc: 0.702\n",
            "[Epoch:   6/ 10] Training Loss: 0.036, Testing Loss: 1.188, Training Acc: 0.991, Testing Acc: 0.708\n",
            "[Epoch:   7/ 10] Training Loss: 0.025, Testing Loss: 1.260, Training Acc: 0.995, Testing Acc: 0.706\n",
            "[Epoch:   8/ 10] Training Loss: 0.015, Testing Loss: 1.580, Training Acc: 0.997, Testing Acc: 0.709\n",
            "[Epoch:   9/ 10] Training Loss: 0.017, Testing Loss: 1.417, Training Acc: 0.997, Testing Acc: 0.704\n",
            "\n",
            "Experiment run for dropout = 0.75, weight_decay = 0.01\n",
            "[Epoch:   0/ 10] Training Loss: 0.598, Testing Loss: 0.624, Training Acc: 0.687, Testing Acc: 0.647\n",
            "[Epoch:   1/ 10] Training Loss: 0.504, Testing Loss: 0.571, Training Acc: 0.769, Testing Acc: 0.690\n",
            "[Epoch:   2/ 10] Training Loss: 0.416, Testing Loss: 0.562, Training Acc: 0.818, Testing Acc: 0.712\n",
            "[Epoch:   3/ 10] Training Loss: 0.370, Testing Loss: 0.568, Training Acc: 0.843, Testing Acc: 0.713\n",
            "[Epoch:   4/ 10] Training Loss: 0.286, Testing Loss: 0.615, Training Acc: 0.881, Testing Acc: 0.720\n",
            "[Epoch:   5/ 10] Training Loss: 0.242, Testing Loss: 0.647, Training Acc: 0.910, Testing Acc: 0.712\n",
            "[Epoch:   6/ 10] Training Loss: 0.176, Testing Loss: 0.679, Training Acc: 0.944, Testing Acc: 0.715\n",
            "[Epoch:   7/ 10] Training Loss: 0.153, Testing Loss: 0.746, Training Acc: 0.950, Testing Acc: 0.706\n",
            "[Epoch:   8/ 10] Training Loss: 0.097, Testing Loss: 0.840, Training Acc: 0.974, Testing Acc: 0.710\n",
            "[Epoch:   9/ 10] Training Loss: 0.080, Testing Loss: 0.940, Training Acc: 0.977, Testing Acc: 0.708\n",
            "\n",
            "Experiment run for dropout = 0.75, weight_decay = 0.001\n",
            "[Epoch:   0/ 10] Training Loss: 0.591, Testing Loss: 0.624, Training Acc: 0.689, Testing Acc: 0.663\n",
            "[Epoch:   1/ 10] Training Loss: 0.456, Testing Loss: 0.563, Training Acc: 0.791, Testing Acc: 0.709\n",
            "[Epoch:   2/ 10] Training Loss: 0.368, Testing Loss: 0.554, Training Acc: 0.850, Testing Acc: 0.713\n",
            "[Epoch:   3/ 10] Training Loss: 0.266, Testing Loss: 0.600, Training Acc: 0.899, Testing Acc: 0.722\n",
            "[Epoch:   4/ 10] Training Loss: 0.174, Testing Loss: 0.704, Training Acc: 0.941, Testing Acc: 0.717\n",
            "[Epoch:   5/ 10] Training Loss: 0.113, Testing Loss: 0.827, Training Acc: 0.964, Testing Acc: 0.716\n",
            "[Epoch:   6/ 10] Training Loss: 0.076, Testing Loss: 1.002, Training Acc: 0.978, Testing Acc: 0.710\n",
            "[Epoch:   7/ 10] Training Loss: 0.137, Testing Loss: 0.956, Training Acc: 0.953, Testing Acc: 0.688\n",
            "[Epoch:   8/ 10] Training Loss: 0.037, Testing Loss: 1.127, Training Acc: 0.993, Testing Acc: 0.710\n",
            "[Epoch:   9/ 10] Training Loss: 0.026, Testing Loss: 1.397, Training Acc: 0.994, Testing Acc: 0.715\n",
            "\n",
            "Experiment run for dropout = 0.75, weight_decay = 0.0001\n",
            "[Epoch:   0/ 10] Training Loss: 0.589, Testing Loss: 0.629, Training Acc: 0.705, Testing Acc: 0.653\n",
            "[Epoch:   1/ 10] Training Loss: 0.451, Testing Loss: 0.570, Training Acc: 0.809, Testing Acc: 0.697\n",
            "[Epoch:   2/ 10] Training Loss: 0.349, Testing Loss: 0.569, Training Acc: 0.869, Testing Acc: 0.705\n",
            "[Epoch:   3/ 10] Training Loss: 0.199, Testing Loss: 0.652, Training Acc: 0.929, Testing Acc: 0.720\n",
            "[Epoch:   4/ 10] Training Loss: 0.119, Testing Loss: 0.788, Training Acc: 0.962, Testing Acc: 0.716\n",
            "[Epoch:   5/ 10] Training Loss: 0.079, Testing Loss: 0.887, Training Acc: 0.981, Testing Acc: 0.709\n",
            "[Epoch:   6/ 10] Training Loss: 0.044, Testing Loss: 1.168, Training Acc: 0.989, Testing Acc: 0.706\n",
            "[Epoch:   7/ 10] Training Loss: 0.028, Testing Loss: 1.223, Training Acc: 0.995, Testing Acc: 0.707\n",
            "[Epoch:   8/ 10] Training Loss: 0.035, Testing Loss: 1.321, Training Acc: 0.991, Testing Acc: 0.707\n",
            "[Epoch:   9/ 10] Training Loss: 0.015, Testing Loss: 1.677, Training Acc: 0.997, Testing Acc: 0.707\n",
            "\n",
            "Experiment run for dropout = 0.9, weight_decay = 0\n",
            "[Epoch:   0/ 10] Training Loss: 0.553, Testing Loss: 0.598, Training Acc: 0.735, Testing Acc: 0.677\n",
            "[Epoch:   1/ 10] Training Loss: 0.399, Testing Loss: 0.578, Training Acc: 0.829, Testing Acc: 0.709\n",
            "[Epoch:   2/ 10] Training Loss: 0.306, Testing Loss: 0.574, Training Acc: 0.888, Testing Acc: 0.712\n",
            "[Epoch:   3/ 10] Training Loss: 0.182, Testing Loss: 0.654, Training Acc: 0.941, Testing Acc: 0.718\n",
            "[Epoch:   4/ 10] Training Loss: 0.119, Testing Loss: 0.777, Training Acc: 0.964, Testing Acc: 0.706\n",
            "[Epoch:   5/ 10] Training Loss: 0.070, Testing Loss: 0.927, Training Acc: 0.983, Testing Acc: 0.706\n",
            "[Epoch:   6/ 10] Training Loss: 0.038, Testing Loss: 1.147, Training Acc: 0.990, Testing Acc: 0.713\n",
            "[Epoch:   7/ 10] Training Loss: 0.032, Testing Loss: 1.374, Training Acc: 0.992, Testing Acc: 0.712\n",
            "[Epoch:   8/ 10] Training Loss: 0.025, Testing Loss: 1.490, Training Acc: 0.994, Testing Acc: 0.708\n",
            "[Epoch:   9/ 10] Training Loss: 0.011, Testing Loss: 1.570, Training Acc: 0.998, Testing Acc: 0.707\n",
            "\n",
            "Experiment run for dropout = 0.9, weight_decay = 0.01\n",
            "[Epoch:   0/ 10] Training Loss: 0.626, Testing Loss: 0.667, Training Acc: 0.674, Testing Acc: 0.646\n",
            "[Epoch:   1/ 10] Training Loss: 0.487, Testing Loss: 0.594, Training Acc: 0.770, Testing Acc: 0.699\n",
            "[Epoch:   2/ 10] Training Loss: 0.416, Testing Loss: 0.568, Training Acc: 0.810, Testing Acc: 0.710\n",
            "[Epoch:   3/ 10] Training Loss: 0.398, Testing Loss: 0.571, Training Acc: 0.817, Testing Acc: 0.702\n",
            "[Epoch:   4/ 10] Training Loss: 0.310, Testing Loss: 0.614, Training Acc: 0.872, Testing Acc: 0.703\n",
            "[Epoch:   5/ 10] Training Loss: 0.262, Testing Loss: 0.609, Training Acc: 0.898, Testing Acc: 0.714\n",
            "[Epoch:   6/ 10] Training Loss: 0.188, Testing Loss: 0.718, Training Acc: 0.930, Testing Acc: 0.712\n",
            "[Epoch:   7/ 10] Training Loss: 0.138, Testing Loss: 0.750, Training Acc: 0.961, Testing Acc: 0.706\n",
            "[Epoch:   8/ 10] Training Loss: 0.122, Testing Loss: 0.906, Training Acc: 0.957, Testing Acc: 0.709\n",
            "[Epoch:   9/ 10] Training Loss: 0.109, Testing Loss: 0.926, Training Acc: 0.966, Testing Acc: 0.700\n",
            "\n",
            "Experiment run for dropout = 0.9, weight_decay = 0.001\n",
            "[Epoch:   0/ 10] Training Loss: 0.603, Testing Loss: 0.641, Training Acc: 0.689, Testing Acc: 0.650\n",
            "[Epoch:   1/ 10] Training Loss: 0.454, Testing Loss: 0.565, Training Acc: 0.797, Testing Acc: 0.701\n",
            "[Epoch:   2/ 10] Training Loss: 0.402, Testing Loss: 0.588, Training Acc: 0.829, Testing Acc: 0.691\n",
            "[Epoch:   3/ 10] Training Loss: 0.266, Testing Loss: 0.591, Training Acc: 0.903, Testing Acc: 0.717\n",
            "[Epoch:   4/ 10] Training Loss: 0.171, Testing Loss: 0.704, Training Acc: 0.941, Testing Acc: 0.720\n",
            "[Epoch:   5/ 10] Training Loss: 0.120, Testing Loss: 0.787, Training Acc: 0.964, Testing Acc: 0.716\n",
            "[Epoch:   6/ 10] Training Loss: 0.080, Testing Loss: 0.903, Training Acc: 0.980, Testing Acc: 0.708\n",
            "[Epoch:   7/ 10] Training Loss: 0.060, Testing Loss: 0.973, Training Acc: 0.989, Testing Acc: 0.704\n",
            "[Epoch:   8/ 10] Training Loss: 0.059, Testing Loss: 1.062, Training Acc: 0.986, Testing Acc: 0.699\n",
            "[Epoch:   9/ 10] Training Loss: 0.046, Testing Loss: 1.413, Training Acc: 0.987, Testing Acc: 0.714\n",
            "\n",
            "Experiment run for dropout = 0.9, weight_decay = 0.0001\n",
            "[Epoch:   0/ 10] Training Loss: 0.548, Testing Loss: 0.593, Training Acc: 0.732, Testing Acc: 0.681\n",
            "[Epoch:   1/ 10] Training Loss: 0.418, Testing Loss: 0.570, Training Acc: 0.813, Testing Acc: 0.709\n",
            "[Epoch:   2/ 10] Training Loss: 0.301, Testing Loss: 0.588, Training Acc: 0.878, Testing Acc: 0.721\n",
            "[Epoch:   3/ 10] Training Loss: 0.208, Testing Loss: 0.617, Training Acc: 0.933, Testing Acc: 0.717\n",
            "[Epoch:   4/ 10] Training Loss: 0.130, Testing Loss: 0.765, Training Acc: 0.962, Testing Acc: 0.714\n",
            "[Epoch:   5/ 10] Training Loss: 0.078, Testing Loss: 0.879, Training Acc: 0.981, Testing Acc: 0.711\n",
            "[Epoch:   6/ 10] Training Loss: 0.057, Testing Loss: 0.981, Training Acc: 0.988, Testing Acc: 0.706\n",
            "[Epoch:   7/ 10] Training Loss: 0.037, Testing Loss: 1.159, Training Acc: 0.992, Testing Acc: 0.709\n",
            "[Epoch:   8/ 10] Training Loss: 0.028, Testing Loss: 1.253, Training Acc: 0.995, Testing Acc: 0.704\n",
            "[Epoch:   9/ 10] Training Loss: 0.020, Testing Loss: 1.398, Training Acc: 0.996, Testing Acc: 0.706\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NfuqNNGWwRgq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "L$_2$-регуляризация (параметр weight_decay в оптимизаторе) больше всего повлияла на loss\\acc результат экспериментов\n",
        "Лучший результат при weight_decay=0.01, однако случайный поиск даст более точный результат\n",
        "Параметр dropout почти не повлиял на производительность сети"
      ]
    }
  ]
}