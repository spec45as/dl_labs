{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вариационный автокодировщик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задании предлагается реализовать вариационный и обычный автокодировщики, обучить их на MNIST, сравнить между собой эти модели, сделать выводы по результатам сравнения и выводы про каждую модель по отдельности.\n",
    "\n",
    "Необходимая теория приведена ниже. Для более глубокого погружения в тему также есть список литературы с комментариями.\n",
    "\n",
    "В этом задании нельзя использовать функций плотностей распределений и репараметризации из стандартных библиотек."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Актуальная версия доступна по адресу https://github.com/nadiinchi/dl_labs/blob/master/lab_vae.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теория"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Постановка задачи\n",
    "Дана выборка независимых одинаково распределенных величин из истинного распределения $x_i \\sim p_d(x)$, $i = 1, \\dots, N$.\n",
    "\n",
    "Задача - построить вероятностную модель $p_\\theta(x)$ истинного распределения $p_d(x)$.\n",
    "\n",
    "Распределение $p_\\theta(x)$ должно позволять как оценить плотность вероятности для данного объекта $x$, так и сэмплировать $x \\sim p_\\theta(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вероятностная модель\n",
    "$z \\in \\mathbb{R}^d$ - локальная латентная переменная, т. е. своя для каждого объекта $x$.\n",
    "\n",
    "Генеративный процесс вариационного автокодировщика:\n",
    "1. Сэмплируем $z \\sim p(z)$.\n",
    "2. Сэмплируем $x \\sim p_\\theta(x | z)$.\n",
    "\n",
    "Параметры распределения $p_\\theta(x | z)$ задаются нейросетью с весами $\\theta$, получающей на вход вектор $z$.\n",
    "\n",
    "Индуцированная генеративным процессом плотность вероятности объекта $x$:\n",
    "\n",
    "$$p_\\theta(x) = \\mathbb{E}_{z \\sim p(z)} p_\\theta(x | z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Параметризация модели\n",
    "Априорное распределение на скрытые перменные - стандартное нормальное распределение: $p(z) = \\mathcal{N}(z | 0, I)$.\n",
    "\n",
    "Распределения на компоненты $x$ условно независимы относительно $z$: $p_\\theta(x | z) = \\prod\\limits_{i = 1}^D p_\\theta(x_i | z)$.\n",
    "\n",
    "Если i-ый признак объекта вещественный, то $p_\\theta(x_i | z) = \\mathcal{N}(x_i | \\mu_i(z, \\theta), \\sigma^2_i(z, \\theta))$.\n",
    "Здесь $\\mu(z, \\theta)$ и $\\sigma(z, \\theta)$ - детерминированные функции, задаваемые нейросетями с параметрами $\\theta$.\n",
    "\n",
    "Если i-ый признак категориальный, то $p_\\theta(x_i | z) = Cat(Softmax(\\omega_i(z, \\theta)))$, где $\\omega_i(z, \\theta)$ - тоже детерминированная функция задаваемая нейросетью.\n",
    "\n",
    "Отдельно можно рассмотреть бинарные признаки, для которых категориальное распределение превращается в распределение Бернулли с одним параметром."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вариационная нижняя оценка логарифма правдоподобия\n",
    "\n",
    "Для максимизации правдоподобия максимизируем вариационную нижнюю оценку на логарифм правдоподобия:\n",
    "$$\\log p_\\theta(x) = \\mathbb{E}_{z \\sim q_\\phi(z | x)} \\log p_\\theta(x) = \n",
    "\\mathbb{E}_{z \\sim q_\\phi(z | x)} \\log \\frac{p_\\theta(x, z) q_\\phi(z | x)}{q_\\phi(z | x) p_\\theta(z | x)} = \n",
    "\\mathbb{E}_{z \\sim q_\\phi(z | x)} \\log \\frac{p_\\theta(x, z)}{q_\\phi(z | x)} + KL(q_\\phi(z | x) || p_\\theta(z | x))$$\n",
    "$$\\log p_\\theta(x) \\geqslant \\mathbb{E}_{z \\sim q_\\phi(z | x)} \\log \\frac{p_\\theta(x | z)p(z)}{q_\\phi(z | x)} = \n",
    "\\mathbb{E}_{z \\sim q_\\phi(z | x)} \\log p_\\theta(x | z) - KL(q_\\phi(z | x) || p(z)) = L(x; \\phi, \\theta)\n",
    "\\to \\max\\limits_{\\phi, \\theta}$$\n",
    "\n",
    "$q_\\phi(z | x)$ называется предложным (proposal) или распознающим (recognition) распределением. Это гауссиана, чьи параметры задаются нейросетью с весами $\\phi$:\n",
    "$q_\\phi(z | x) = \\mathcal{N}(z | \\mu_\\phi(x), \\sigma^2_\\phi(x)I)$.\n",
    "Обычно нейросеть моделирует не $\\sigma_\\phi(x)$, а $\\log\\sigma_\\phi(x)$ или другую величину, более инвариантную к масштабу и определенную на всех вещественных числах так, чтобы $\\sigma_\\phi(x)$ было всегда положительным.\n",
    "\n",
    "Зазор между вариационной нижней оценкой $L(x; \\phi, \\theta)$ на логарифм правдоподобия модели и самим логарифмом правдоподобия $\\log p_\\theta(x)$ - это KL-дивергенция между предолжным и апостериорным распределением на $z$: $KL(q_\\phi(z | x) || p_\\theta(z | x))$. Максимальное значение $L(x; \\phi, \\theta)$ при фиксированных параметрах модели $\\theta$ достигается при $q_\\phi(z | x) = p_\\theta(z | x)$, но явное вычисление $p_\\theta(z | x)$ требует слишком большого числа ресурсов, поэтому вместо этого вычисления вариационная нижняя оценка оптимизируется также по $\\phi$. Чем ближе $q_\\phi(z | x)$ к $p_\\theta(z | x)$, тем точнее вариационная нижняя оценка.\n",
    "Есть статьи, показывающие, что истинное апостериорное распределение $p_\\theta(z | x)$ часто не может быть представлено одной гауссианой, поэтому зазор между нижней оценкой и логарифмом правдоподобия не достигает $0$. Тем не менее, этот зазор практически не влияет на процесс оптимизации модели и его результат по сравнению с другими факторами.\n",
    "\n",
    "Первое слагаемое вариационной нижней оценки $\\mathbb{E}_{z \\sim q_\\phi(z | x)} \\log p_\\theta(x | z)$ называется ошибкой восстановления (reconstruction loss).\n",
    "Модель, соответствующая этой части - это автокодирощик с одним стохастическим слоем, пытающийся восстановить входной объект $x$.\n",
    "Если распределение $q_\\phi(z | x)$ - дельта-функция, то автокодировщик со стохастическим слоем превращается в обычный автокодировщик.\n",
    "Поэтому $q_\\phi(z | x)$ и $p_\\theta(x | z)$ иногда называют энкодером и декодером соответственно.\n",
    "\n",
    "Слагаемое $KL(q_\\phi(z | x) || p(z))$ иногда называют регуляризатором.\n",
    "Оно вынуждает $z \\sim q_\\phi(z | x)$ быть близким к $0$ и $q_\\phi(z | x)$ быть близким к $p_\\theta(z | x)$.\n",
    "Иногда коэффициент при KL-дивергенции полагают не равным единице или даже используют другой регуляризатор.\n",
    "Естественно, после этого обучение модели перестает соответствовать максимизации правдоподобия вышеописанной вероятностной модели данных.\n",
    "Это существенно снижает интерпретируемость модели, устраняет теоретические гарантии для неё.\n",
    "KL-дивергенция между двумя нормальными распределениями может быть вычислена аналитически.\n",
    "\n",
    "Для максимизации $L(x; \\phi, \\theta)$ используется стохастический градиентный подъем.\n",
    "Градиент ошибки восстановления по $\\theta$ вычисляется с помощью метода обратного распространения ошибки.\n",
    "$$\\frac{\\partial}{\\partial \\theta} L(x; \\phi, \\theta) = \\mathbb{E}_{z \\sim q_\\phi(z | x)} \\frac{\\partial}{\\partial \\theta} \\log p_\\theta(x | z)$$\n",
    "\n",
    "Градиент KL-дивергенции по $\\phi$ может быть вычислен аналитически.\n",
    "Для вычисления градиента ошибки восстановления по $\\phi$ используется репараметризация (reparametrization trick):\n",
    "$$\\varepsilon \\sim \\mathcal{N}(\\varepsilon | 0, I)$$\n",
    "$$z = \\mu + \\sigma \\varepsilon \\Rightarrow z \\sim \\mathcal{N}(z | \\mu, \\sigma^2I)$$\n",
    "$$\\frac{\\partial}{\\partial \\phi} L(x; \\phi, \\theta) = \\mathbb{E}_{\\varepsilon \\sim \\mathcal{N}(\\varepsilon | 0, I)} \\frac{\\partial}{\\partial \\phi} \\log p_\\theta(x | \\mu_\\phi(x) + \\sigma_\\phi(x) \\varepsilon) - \\frac{\\partial}{\\partial \\phi} KL(q_\\phi(z | x) || p(z))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка правдоподобия модели\n",
    "\n",
    "Правдоподобие модели $p_\\theta(x) = \\mathbb{E}_{z \\sim p(z)} p_\\theta(x | z)$ оценивают на отложенной выборке.\n",
    "\n",
    "Оценка может быть получена с помощью метода Монте-Карло:\n",
    "\n",
    "$$z_i \\sim p(z), i = 1, \\dots, K$$\n",
    "$$p_\\theta(x) \\approx \\frac{1}{K} \\sum\\limits_{i = 1}^K p_\\theta(x | z_i)$$\n",
    "\n",
    "Альтернативный способ оценки - метод importance sampling. В качестве предложного распределения метода используется предложное распределение модели. Известно, что хороший выбор предложного распределения уменьшает дисперсию оценки. В случае вариационного автокодировщика оценки Монте-Карло, основанные на малом числе сэмплов, обычно занижены, поэтому imporatance sampling также позволяет получить более высокую и точную оценку правдоподобия с помощью меньшего числа сэмплов.\n",
    "\n",
    "$$z_i \\sim q_\\phi(z | x), i = 1, \\dots, K$$\n",
    "$$p_\\theta(x) = \\mathbb{E}_{z \\sim p(z)} p_\\theta(x | z) = \\mathbb{E}_{z \\sim q_\\phi(z | x)} \\frac{p_\\theta(x | z) p(z)}{q_\\phi(z | x)} \\approx \\frac{1}{K} \\sum\\limits_{i = 1}^K \\frac{p_\\theta(x | z_i) p(z_i)}{q_\\phi(z_i | x)}$$\n",
    "\n",
    "Для оценки логарифма правдоподобия усреднение вероятностей происходит под логарифмом:\n",
    "$$\\log p_\\theta(x) \\approx \\log \\frac{1}{K} \\sum\\limits_{i = 1}^K p_\\theta(x | z_i),\\,\\,\\,\\,z_i \\sim p(z)$$\n",
    "$$\\log p_\\theta(x) \\approx \\log \\frac{1}{K} \\sum\\limits_{i = 1}^K \\frac{p_\\theta(x | z_i) p(z_i)}{q_\\phi(z_i | x)},\\,\\,\\,\\,z_i \\sim q_\\phi(z | x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Литература\n",
    "1. Auto-Encoding Variational Bayes https://arxiv.org/pdf/1312.6114.pdf - оригинальная статья про вариационный автокодировщик.\n",
    "2. Learning Structured Output Representation using Deep Conditional Generative Models https://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf - обусловленный вариационный автокодировщик для генерации из обусловленного распределения.\n",
    "3. Importance Weighted Autoencoders https://arxiv.org/pdf/1509.00519.pdf - вариационный автокодировщик с более точной вариационной нижней оценкой.\n",
    "4. Ladder Variation Autoencoders http://papers.nips.cc/paper/6275-ladder-variational-autoencoders.pdf - теперь у каждого объекта есть не одно скрытое представление, а много.\n",
    "5. Variational Inference with Normalizing Flows https://arxiv.org/pdf/1505.05770.pdf, Improved Variational Inference with Inverse Autoregressive Flow http://papers.nips.cc/paper/6581-improved-variational-inference-with-inverse-autoregressive-flow.pdf - более богатые семейства предложных распредлений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка, предобработка и визуалиация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MNIST('mnist', download=True, train=True)\n",
    "train_data = TensorDataset(data.train_data.view(-1, 28 * 28).float() / 255, data.train_labels)\n",
    "data = MNIST('mnist', download=True, train=False)\n",
    "test_data = TensorDataset(data.test_data.view(-1, 28 * 28).float() / 255, data.test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show_images(x):\n",
    "    plt.figure(figsize=(12, 12 / 10 * (x.shape[0] // 10 + 1)))\n",
    "    x = x.view(-1, 28, 28)\n",
    "    for i in range(x.shape[0]):\n",
    "        plt.subplot(x.shape[0] // 10 + 1, 10, i + 1)\n",
    "        plt.imshow(x.data[i].numpy(), cmap='Greys_r', vmin=0, vmax=1, interpolation='lanczos')\n",
    "        plt.axis('off')\n",
    "\n",
    "show_images(Variable(train_data[:10][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для корректности вероятностной модели исходное изображение также должно быть бинаризовано.\n",
    "\n",
    "Бинаризация может производиться как округлением данных в датасете, так и сэмплированием из распределения Бернулли каждого пикселя. Округление приводит к более гладким фигурам в обучающей выборке, поэтому будем использовать его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(Variable(torch.bernoulli(train_data[:10][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(Variable(train_data[:10][0].round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.data_tensor = train_data.data_tensor.round()\n",
    "test_data.data_tensor = test_data.data_tensor.round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вспомогательные функции для обучения и тестирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_test_loss(compute_loss, batch_size=100, max_batches=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Функция вычисляет усредненное значение функции потерь по тестовым данным.\n",
    "    Вход: compute_loss, функция, принимающая батч в виде матрицы torch.FloatTensor\n",
    "    и возвращающая float - функцию потерь на батче.\n",
    "    Вход: batch_size, int.\n",
    "    Вход: max_batches, int - если задано, включает режим оценки функции потерь\n",
    "    с помощью сэмплирования батчей вместо полного прохода по данным и указывает,\n",
    "    после какого батча прекратить вычисления.\n",
    "    Вход: verbose, bool - указывает, печатать ли текущее состояние в процессе работы.\n",
    "    Возвращаемое значение: float - оценка функции потерь на тестовых данных.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=(max_batches is None))\n",
    "    num_batches = len(dataloader)\n",
    "    avg_loss = 0\n",
    "    for i, (batch, _) in enumerate(dataloader):\n",
    "        loss = compute_loss(batch)\n",
    "        avg_loss += (loss - avg_loss) / (i + 1)\n",
    "        if verbose and (i + 1) % 10 == 0:\n",
    "            print('\\rTest loss:', avg_loss,\n",
    "                  'Batch', i + 1, 'of', num_batches, ' ' * 10, end='', flush=True)\n",
    "        if verbose and (i + 1) % 100 == 0:\n",
    "            print(flush=True)\n",
    "        if max_batches and i >= max_batches:\n",
    "            break\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, tests=[], batch_size=100, num_epochs=5, learning_rate=1e-3, maximization=True):\n",
    "    \"\"\"\n",
    "    Обучает модель.\n",
    "    Вход: model, Module - объект, модель.\n",
    "    У этого объекта должна быть функция batch_loss от batch - FloatTensor и K - int,\n",
    "    возвращающая скаляр Variable - функцию потерь на батче, которая должна быть\n",
    "    оптимизирована.\n",
    "    Вход: tests - список тестов, выполняемых после каждого 100-го батча.\n",
    "    Каждый элемент списка - словарь с полями 'name' - уникальным идентификатором\n",
    "    теста и 'func' - функцией от модели.\n",
    "    Вход: batch_size, int.\n",
    "    Вход: num_epochs, int.\n",
    "    Вход: learning_rate, float.\n",
    "    Возвращаемое значение: словарь с полями 'model' - обученная модель,\n",
    "    'train_losses_list' - список функций потерь на каждом батче и \n",
    "    'test_results' - список результатов тестирования. Каждый результат\n",
    "    тестирования - словарь вида name: value, где name - имя теста,\n",
    "    value - результат его выполнения.\n",
    "    \"\"\"\n",
    "    gd = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    train_losses = []\n",
    "    test_results = []\n",
    "    for _ in range(num_epochs):\n",
    "        for i, (batch, _) in enumerate(dataloader):\n",
    "            total = len(dataloader)\n",
    "            loss = model.batch_loss(batch)\n",
    "            if maximization:\n",
    "                (-loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            train_losses.append(loss.data.numpy()[0])\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print('\\rTrain loss:', train_losses[-1],\n",
    "                      'Batch', i + 1, 'of', total, ' ' * 10, end='', flush=True)\n",
    "            if (i + 1) % 100 == 0:\n",
    "                cur_test_result = {}\n",
    "                for test in tests:\n",
    "                    cur_test_result[test['name']] = test['func'](model)\n",
    "                test_results.append(cur_test_result)\n",
    "                print(flush=True)\n",
    "            gd.step()\n",
    "            gd.zero_grad()\n",
    "    return {\n",
    "        'model': model,\n",
    "        'train_losses_list': train_losses,\n",
    "        'test_results': test_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 15\n",
    "digit_size = 28\n",
    "\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "def draw_manifold(generator):\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    for i, yi in enumerate(grid_x):\n",
    "        for j, xi in enumerate(grid_y):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "\n",
    "            x_decoded = generator(z_sample)\n",
    "            digit = x_decoded\n",
    "            figure[i * digit_size: (i + 1) * digit_size,\n",
    "                   j * digit_size: (j + 1) * digit_size] = digit\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(figure, cmap='Greys_r', vmin=0, vmax=1, interpolation='lanczos')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_latent_space(data, target, encoder):\n",
    "    z_test = encoder(data)\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.scatter(z_test[:, 0], z_test[:, 1], c=target, cmap='gist_rainbow', alpha=0.75)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обычный автокодировщик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, d, D):\n",
    "        \"\"\"\n",
    "        Инициализирует веса модели.\n",
    "        Вход: d, int - размерность латентного пространства.\n",
    "        Вход: D, int - размерность пространства объектов.\n",
    "        \"\"\"\n",
    "        super(type(self), self).__init__()\n",
    "        self.d = d\n",
    "        self.D = D\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.D, 200),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(200, self.d)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.d, 200),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(200, self.D),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # glorot uniform initialization\n",
    "        lin_layers = []\n",
    "        lin_layers += [layer for layer in self.encoder if isinstance(layer, nn.Linear)]\n",
    "        lin_layers += [layer for layer in self.decoder if isinstance(layer, nn.Linear)]\n",
    "        for lin in lin_layers:\n",
    "            f_in, f_out = lin.weight.shape\n",
    "            lin.weight.data.uniform_()\n",
    "            lin.weight.data.mul_(2).sub_(1).mul_(torch.np.sqrt(6 / (f_in + f_out)))\n",
    "            lin.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Генерирует код по объектам.\n",
    "        Вход: x, Variable - матрица размера n x D.\n",
    "        Возвращаемое значение: Variable - матрица размера n x d.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        pass\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        По матрице латентных представлений z возвращает матрицу объектов x.\n",
    "        Вход: z, Variable - матрица n x d латентных представлений.\n",
    "        Возвращаемое значение: Variable, матрица объектов n x D.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        pass\n",
    "\n",
    "    def batch_loss(self, batch):\n",
    "        \"\"\"\n",
    "        Вычисляет функцию потерь по батчу.\n",
    "        Функция потерь - сумма L2-ошибки восстановления по батчу и\n",
    "        L2 регуляризации скрытых представлений с весом 1.\n",
    "        Возвращаемое значение должно быть дифференцируемо по параметрам модели (!).\n",
    "        Подсказка: значение функции потерь не должно зависеть от размера батча.\n",
    "        Вход: batch, FloatTensor - матрица объектов размера n x D.\n",
    "        Возвращаемое значение: Variable, скаляр - функция потерь по батчу.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        pass\n",
    "\n",
    "    def generate_samples(self, num_samples):\n",
    "        \"\"\"\n",
    "        Генерирует сэмплы объектов x. Использует стандартное нормальное\n",
    "        распределение в пространстве представлений.\n",
    "        Вход: num_samples, int - число сэмплов, которые надо сгененрировать.\n",
    "        Возвращаемое значение: Variable, матрица размера num_samples x D.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ae_tests = [\n",
    "    {\n",
    "        'name': 'test_loss',\n",
    "        'func': lambda model:\n",
    "                model_test_loss(lambda batch:\n",
    "                                float(model.batch_loss(batch)),\n",
    "                                max_batches=20)\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model_d2 = train_model(AE(2, 784), tests=ae_tests, maximization=False, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model_d10 = train_model(AE(10, 784), tests=ae_tests, maximization=False, num_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка качества моделей\n",
    "Визуальная оценка генерируемых объектов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(ae_model_d2['model'].generate_samples(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(ae_model_d10['model'].generate_samples(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализация латентного пространства (с точки зрения декодера)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_manifold_ae(model):\n",
    "    generator = lambda z: model.decode(Variable(torch.from_numpy(z).float())).view(28, 28).data.numpy()\n",
    "    return draw_manifold(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_manifold_ae(ae_model_d2['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализация латентного пространства (с точки зрения энкодера)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_latent_space(test_data.data_tensor[::10], test_data.target_tensor[::10],\n",
    "                  lambda data: ae_model_d2['model'].encode(Variable(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_encoder_d10 = lambda data: TSNE().fit_transform(ae_model_d10['model'].encode(Variable(data)).data.numpy())\n",
    "draw_latent_space(test_data.data_tensor[::25], test_data.target_tensor[::25], ae_encoder_d10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Автокодировщик: теперь вариационный!\n",
    "\n",
    "В качестве функции потерь используем бинарную кроссэнтропию.\n",
    "Это означает, что мы предполагаем, что каждый пискель - бинарная случайная величина.\n",
    "Генеративная сеть выдает вероятность каждого пикселя быть равным $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_likelihood(x_true, x_distr):\n",
    "    \"\"\"\n",
    "    Вычисляет логарфм правдоподобия объектов x_true, который равен минус бинарной\n",
    "    кроссэнтропии между эмприческим распределением на данные и индуцированным\n",
    "    моделью распределением.\n",
    "    Каждому объекту из x_true соответствуют K сэмплированных распределений\n",
    "    на x из x_distr.\n",
    "    i-му объекту соответствуют распределения с номерами i * K, ..., (i + 1) * K - 1.\n",
    "    Требуется вычислить оценку логарифма правдоподобия для каждого объекта.\n",
    "    Подсказка: не забывайте про вычислительную стабильность!\n",
    "    Подсказка: делить логарифм правдоподобия на число компонент объекта не надо.\n",
    "\n",
    "    Вход: x_true, Variable - матрица объектов размера n x D.\n",
    "    Вход: x_distr, Variable - матрица параметров распределений Бернулли\n",
    "    размера (n * K) x D.\n",
    "    Выход: Variable, матрица размера n x K - оценки логарифма правдоподобия\n",
    "    каждого сэмпла.\n",
    "    \"\"\"\n",
    "    K = x_distr.shape[0] // x_true.shape[0]\n",
    "    # ваш код здесь\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_mean_exp(mtx):\n",
    "    \"\"\"\n",
    "    Возвращает логарифм среднего по каждому столбцу от экспоненты данной матрицы.\n",
    "    Подсказка: не забывайте про вычислительную стабильность!\n",
    "    Вход: mtx, Varibale - матрица размера n x k.\n",
    "    Возвращаемое значение: Variable, вектор длины n.\n",
    "    \"\"\"\n",
    "    # ваш код здесь\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kl(q_distr, p_distr):\n",
    "    \"\"\"\n",
    "    Вычисляется KL-дивергенция KL(q || p) между n парами гауссиан.\n",
    "    Вход: q_distr, tuple(Variable, Varable). Каждое Variable - матрица размера n x d.\n",
    "    Первое - mu, второе - sigma.\n",
    "    Вход: p_distr, tuple(Variable, Varable). Аналогично.\n",
    "    Возвращаемое значение: Variable, вектор размерности n, каждое значение которого - \n",
    "    - KL-дивергенция между соответствующей парой распределений.\n",
    "    \"\"\"\n",
    "    p_mu, p_sigma = p_distr\n",
    "    q_mu, q_sigma = q_distr\n",
    "    # ваш код здесь\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, d, D):\n",
    "        \"\"\"\n",
    "        Инициализирует веса модели.\n",
    "        Вход: d, int - размерность латентного пространства.\n",
    "        Вход: D, int - размерность пространства объектов.\n",
    "        \"\"\"\n",
    "        super(type(self), self).__init__()\n",
    "        self.d = d\n",
    "        self.D = D\n",
    "        self.proposal_network = nn.Sequential(\n",
    "            nn.Linear(self.D, 200),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.proposal_mu_head = nn.Linear(200, self.d)\n",
    "        self.proposal_sigma_head = nn.Linear(200, self.d)\n",
    "        self.generative_network = nn.Sequential(\n",
    "            nn.Linear(self.d, 200),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(200, self.D),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # glorot uniform initialization\n",
    "        lin_layers = [self.proposal_mu_head, self.proposal_sigma_head]\n",
    "        lin_layers += [layer for layer in self.generative_network if isinstance(layer, nn.Linear)]\n",
    "        lin_layers += [layer for layer in self.proposal_network if isinstance(layer, nn.Linear)]\n",
    "        for lin in lin_layers:\n",
    "            f_in, f_out = lin.weight.shape\n",
    "            lin.weight.data.uniform_()\n",
    "            lin.weight.data.mul_(2).sub_(1).mul_(torch.np.sqrt(6 / (f_in + f_out)))\n",
    "            lin.bias.data.zero_()\n",
    "\n",
    "    def proposal_distr(self, x):\n",
    "        \"\"\"\n",
    "        Генерирует предложное распределение на z.\n",
    "        Подсказка: областью значений sigma должны быть положительные числа.\n",
    "        Для этого при генерации sigma следует использовать exp или softplus\n",
    "        в качестве последнего преобразования.\n",
    "        Вход: x, Variable - матрица размера n x D.\n",
    "        Возвращаемое значение: tuple(Variable, Variable),\n",
    "        Каждое Variable - матрица размера n x d.\n",
    "        Первое - mu, второе - sigma.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        pass\n",
    "        return mu, sigma\n",
    "\n",
    "    def prior_distr(self, n):\n",
    "        \"\"\"\n",
    "        Генерирует априорное распределение на z.\n",
    "        Вход: n, int - число распределений.\n",
    "        Возвращаемое значение: tuple(Variable, Variable),\n",
    "        Каждое Variable - матрица размера n x d.\n",
    "        Первое - mu, второе - sigma.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        return mu, sigma\n",
    "\n",
    "    def sample_latent(self, distr, K=1):\n",
    "        \"\"\"\n",
    "        Генерирует сэмплы из гауссовского распределения на z.\n",
    "        Сэмплы должны быть дифференцируемы по параметрам распределения!\n",
    "        Вход: distr, tuple(Variable, Varable). Каждое Variable - матрица размера n x d.\n",
    "        Первое - mu, второе - sigma.\n",
    "        Вход: K, int - число сэмплов для каждого объекта.\n",
    "        Возвращаемое значение: Variable, матрица размера n x d.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        pass\n",
    "\n",
    "    def sample_prior(self, num_samples):\n",
    "        \"\"\"\n",
    "        Генерирует сэмплы из априорного распределения на z.\n",
    "        Вход: num_samples, int - число сэмплов, которые надо сгененрировать.\n",
    "        Возвращаемое значение: Variable, матрица размера num_samples x d.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        pass\n",
    "\n",
    "    def generative_distr(self, z):\n",
    "        \"\"\"\n",
    "        По матрице латентных представлений z возвращает матрицу параметров\n",
    "        распределения Бернулли для сэмплирования объектов x.\n",
    "        Вход: z, Variable - матрица n x d латентных представлений.\n",
    "        Возвращаемое значение: Variable, матрица параметров распределения\n",
    "        Бернулли размера n x D.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        pass\n",
    "\n",
    "    def batch_loss(self, batch):\n",
    "        \"\"\"\n",
    "        Вычисляет вариационную нижнюю оценку логарифма правдоподобия по батчу.\n",
    "        Вариационная нижняя оценка должна быть дифференцируема по параметрам модели (!),\n",
    "        т. е. надо использовать репараметризацию.\n",
    "        Подсказка: вариационная нижняя оценка логарифма правдоподобия модели\n",
    "        не должна зависеть от размера батча.\n",
    "        Вход: batch, FloatTensor - матрица объектов размера n x D.\n",
    "        Возвращаемое значение: Variable, скаляр - вариационная нижняя оценка логарифма\n",
    "        правдоподобия по батчу.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        pass\n",
    "\n",
    "    def generate_samples(self, num_samples):\n",
    "        \"\"\"\n",
    "        Генерирует сэмплы из индуцируемого моделью распределения на объекты x.\n",
    "        Вход: num_samples, int - число сэмплов, которые надо сгененрировать.\n",
    "        Возвращаемое значение: Variable, матрица размера num_samples x D.\n",
    "        \"\"\"\n",
    "        # ваш код здесь\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценки функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian_log_pdf(distr, samples):\n",
    "    \"\"\"\n",
    "    Функция вычисляет логарифм плотности вероятности в точке относительно соответствующего\n",
    "    нормального распределения, заданного покомпонентно своими средним и среднеквадратичным отклонением.\n",
    "    Вход: distr, tuple(Variable, Varable). Каждое Variable - матрица размера n x d.\n",
    "    Первое - mu, второе - sigma.\n",
    "    Вход: samples, Variable - матрица размера (n * K) x d сэмплов в скрытом пространстве.\n",
    "    Возвращаемое значение: Variable, вектор длины n * K, каждый элемент которого - логарифм\n",
    "    плотности вероятности точки относительно соответствующего распределения. Точке с номером i\n",
    "    соответствует распределение с номером i // K.\n",
    "    \"\"\"\n",
    "    mu, sigma = distr\n",
    "    K = samples.shape[0] // mu.shape[0]\n",
    "    n = mu.shape[0]\n",
    "    d = mu.shape[1]\n",
    "    # ваш код здесь\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_log_likelihood_monte_carlo(batch, model, K):\n",
    "    \"\"\"\n",
    "    Функция, оценку логарифма правдоподобия вероятностной модели по батчу методом Монте-Карло.\n",
    "    Подсказка: оценка логарифма правдоподобия модели не должна зависеть от размера батча.\n",
    "    Вход: batch, FloatTensor - матрица размера n x D\n",
    "    Вход: model, Module - объект, имеющий методы sample_prior и generative_distr, описанные в VAE.\n",
    "    Вход: K, int - количество сэмплов.\n",
    "    Возвращаемое значение: float - оценка логарифма правдоподобия.\n",
    "    \"\"\"\n",
    "    # ваш код здесь\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_log_likelihood_importance_sampling(batch, model, K):\n",
    "    \"\"\"\n",
    "    Функция, оценку логарифма правдоподобия вероятностной модели по батчу методом Importance Sampling.\n",
    "    Подсказка: оценка логарифма правдоподобия модели не должна зависеть от размера батча.\n",
    "    Вход: batch, FloatTensor - матрица размера n x D\n",
    "    Вход: model, Module - объект, имеющий методы prior_distr, proposal_distr, sample_latent и generative_distr,\n",
    "    описанные в VAE.\n",
    "    Вход: K, int - количество сэмплов.\n",
    "    Возвращаемое значение: float - оценка логарифма правдоподобия.\n",
    "    \"\"\"\n",
    "    # ваш код здесь\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vae_tests = [\n",
    "    {\n",
    "        'name': 'MC',\n",
    "        'func': lambda model:\n",
    "                model_test_loss(lambda batch:\n",
    "                                compute_log_likelihood_monte_carlo(batch, model, K=10),\n",
    "                                max_batches=20)\n",
    "    },\n",
    "    {\n",
    "        'name': 'IS',\n",
    "        'func': lambda model:\n",
    "                model_test_loss(lambda batch:\n",
    "                                compute_log_likelihood_importance_sampling(batch, model, K=10),\n",
    "                                max_batches=20)\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model_d2 = train_model(VAE(2, 784), tests=vae_tests, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model_d10 = train_model(VAE(10, 784), tests=vae_tests, num_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка качества модели\n",
    "\n",
    "Визуальная оценка генерируемых объектов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(vae_model_d2['model'].generate_samples(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(vae_model_d10['model'].generate_samples(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализация латентного пространства (с точки зрения декодера)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_manifold_vae(model):\n",
    "    generator = lambda z: model.generative_distr(Variable(torch.from_numpy(z).float())).view(28, 28).data.numpy()\n",
    "    return draw_manifold(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_manifold_vae(vae_model_d2['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализация латетного пространства (с точки зрения энкодера)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_encoder = lambda data, model: model.sample_latent(model.proposal_distr(Variable(data)))\n",
    "draw_latent_space(test_data.data_tensor[::10], test_data.target_tensor[::10],\n",
    "                  lambda data: vae_encoder(data, vae_model_d2['model']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_encoder_d10 = lambda data: TSNE().fit_transform(vae_encoder(data, vae_model_d10['model']).data.numpy())\n",
    "draw_latent_space(test_data.data_tensor[::25], test_data.target_tensor[::25], vae_encoder_d10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценки логарифма правдоподобия на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "for label, name, model in [\n",
    "    ('VAE, Monte-Carlo, $d = 2$', 'MC', vae_model_d2),\n",
    "    ('VAE, Monte-Carlo, $d = 10$', 'MC', vae_model_d10),\n",
    "    ('VAE, Importance Sampling, $d = 2$', 'IS', vae_model_d2),\n",
    "    ('VAE, Importance Sampling, $d = 10$', 'IS', vae_model_d10),\n",
    "]:\n",
    "    data = [x[name] for x in model['test_results']]\n",
    "    x_labels = (1 + np.arange(len(data))) / 6\n",
    "    plt.plot(x_labels, data, label=label)\n",
    "plt.xlabel('Epoch')\n",
    "plt.xlim(xmax=x_labels[-1])\n",
    "plt.ylabel('Log-likelihood estimation, 10 samples')\n",
    "plt.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_results = []\n",
    "for K in [1, 5, 10, 50, 100, 500, 1000]:\n",
    "    print('K =', K, flush=True)\n",
    "    vae_tests_sampling = [\n",
    "        {\n",
    "            'name': 'D10MC',\n",
    "            'func': model_test_loss(lambda batch:\n",
    "                                    compute_log_likelihood_monte_carlo(batch, vae_model_d10['model'], K=K),\n",
    "                                    batch_size=10,\n",
    "                                    max_batches=50)\n",
    "        },\n",
    "        {\n",
    "            'name': 'D10IS',\n",
    "            'func': model_test_loss(lambda batch:\n",
    "                                    compute_log_likelihood_importance_sampling(batch, vae_model_d10['model'], K=K),\n",
    "                                    batch_size=10,\n",
    "                                    max_batches=50)\n",
    "        },\n",
    "        {\n",
    "            'name': 'D2MC',\n",
    "            'func': model_test_loss(lambda batch:\n",
    "                                    compute_log_likelihood_monte_carlo(batch, vae_model_d2['model'], K=K),\n",
    "                                    batch_size=10,\n",
    "                                    max_batches=50)\n",
    "        },\n",
    "        {\n",
    "            'name': 'D2IS',\n",
    "            'func': model_test_loss(lambda batch:\n",
    "                                    compute_log_likelihood_importance_sampling(batch, vae_model_d2['model'], K=K),\n",
    "                                    batch_size=10,\n",
    "                                    max_batches=50)\n",
    "        }\n",
    "    ]\n",
    "    cur_test_results = {'K': K}\n",
    "    for test in vae_tests_sampling:\n",
    "        cur_test_results[test['name']] = test['func']\n",
    "    test_results.append(cur_test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "for label, name in [\n",
    "    ('VAE, Monte-Carlo, $d = 2$', 'D2MC'),\n",
    "    ('VAE, Importance Sampling, $d = 2$', 'D2IS'),\n",
    "    ('VAE, Monte-Carlo, $d = 10$', 'D10MC'),\n",
    "    ('VAE, Importance Sampling, $d = 10$', 'D10IS'),\n",
    "]:\n",
    "    data = [x[name] for x in test_results]\n",
    "    x_labels = [x['K'] for x in test_results]\n",
    "    plt.plot(x_labels, data, label=label)\n",
    "plt.xlabel('Number of samples')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('Log-likelihood estimation')\n",
    "plt.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Место для ваших выводов, наблюдений, гипотез."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
