{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://s1.postimg.org/6fl45xnvnj/pytorch-logo-dark.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом занятии помимо базового функционала `pytorch` будет использоваться библиотека для зрения **`torchvision`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Еще немного про переменные и тензоры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уже со следующей версии 0.4 механика tensor <-> Variable станет прозрачной и Variable как отдельной сущности больше не будет.\n",
    "\n",
    "![img](https://cs8.pikabu.ru/post_img/2016/10/22/3/1477104625110531972.jpg)\n",
    "\n",
    "А пока стоит знать про особый режим у Variable — **`volatile`** в дополнение к **`requires_grad`**.\n",
    "\n",
    "**`volatile`** используется в случае чистого инференса модели, механика наследования флага отличается от **`requires_grad`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "regular_input = Variable(torch.randn(1, 3, 224, 224))\n",
    "volatile_input = Variable(torch.randn(1, 3, 224, 224), volatile=True)\n",
    "model = models.resnet18(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(regular_input).requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(volatile_input).requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(volatile_input).volatile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(volatile_input).grad_fn is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Главное отличие:\n",
    "* Переменная с флагом **`requires_grad=False`** останавливает обратное распространение ошибки.\n",
    "* Переменная с флагом **`volatile=True`** полностью выключает backprop для нее самой и всего, зависящего от нее.\n",
    "\n",
    "По умолчанию соответственно наоборот — **`requires_grad=True`** и **`volatile=False`**.\n",
    "\n",
    "Где-то во время обучения:\n",
    "\n",
    "    images = Variable(images).cuda()\n",
    "    targets = Variable(targets).cuda(async=True)\n",
    "   \n",
    "Где-то еще во время валидации/предсказания:\n",
    "\n",
    "    images = Variable(images, volatile=True).cuda()\n",
    "    targets = Variable(targets, volatile=True).cuda(async=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch как конструктор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с датасетами\n",
    "\n",
    "Для загрузки данных pytorch опирается на такую сущность, как **`Dataset`**.\n",
    "\n",
    "Этот абстрактный класс определен в `torch.utils.data.dataset`:\n",
    "\n",
    "```python\n",
    "class Dataset(object):\n",
    "    \"\"\"An abstract class representing a Dataset.\n",
    "\n",
    "    All other datasets should subclass it. All subclasses should override\n",
    "    ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n",
    "    supporting integer indexing in range from 0 to len(self) exclusive.\n",
    "    \"\"\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return ConcatDataset([self, other])\n",
    "```\n",
    "При определении нового источника данных мы создаем наследника данного класса и реализуем методы `__getitem__` и `__len__`.\n",
    "\n",
    "Пример готового такого класса — `torchvision.datasets.ImageFolder`, который позволяет создать датасет на основе директории с imagenet-подобной структурой поддиректорий (`./train/{class}` и `./val/{class}`):\n",
    "\n",
    "```python\n",
    "imagenet = torchvision.datasets.ImageFolder('path/to/imagenet_root/')\n",
    "```\n",
    "\n",
    "Кастомный пример — датасет, загружающий изображения на основе путей и классов из текстового файла:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "# default_loader - стандартная функция для загрузки изображений, использует accimage или PIL\n",
    "from torchvision.datasets.folder import default_loader\n",
    "\n",
    "class TxtList(Dataset):\n",
    "    def __init__(self, path, transform=None, loader=default_loader):\n",
    "        with open(path) as fin:\n",
    "            self.imgs = [s.strip().split() for s in fin.readlines()]\n",
    "\n",
    "        print(f'=> Found {len(self.imgs)} entries in {path}')\n",
    "\n",
    "        self.classes = sorted(set([_[1] for _ in self.imgs]))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.transform = transform\n",
    "        self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.imgs[index]\n",
    "        target = self.class_to_idx[target]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!echo '/tmp/1.jpg\\tcat' > /tmp/dataset.tsv\n",
    "!echo '/tmp/2.jpg\\tcat' >> /tmp/dataset.tsv\n",
    "!echo '/tmp/3.jpg\\tdog' >> /tmp/dataset.tsv\n",
    "!echo '/tmp/4.jpg\\tcat' >> /tmp/dataset.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Found 4 entries in /tmp/dataset.tsv\n"
     ]
    }
   ],
   "source": [
    "catdog = TxtList('/tmp/dataset.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'dog']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catdog.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['/tmp/1.jpg', 'cat'],\n",
       " ['/tmp/2.jpg', 'cat'],\n",
       " ['/tmp/3.jpg', 'dog'],\n",
       " ['/tmp/4.jpg', 'cat']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catdog.imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(catdog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FileNotFoundError\n",
    "# catdog[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torchvision` также содержит и другие готовые классы для стандартных датасетов: \n",
    "http://pytorch.org/docs/master/torchvision/datasets.html.\n",
    "\n",
    "Некоторые из них можно сразу и загрузить — например, **MNIST**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !rm -r '/tmp/mnist/' 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.81 ms, sys: 20.5 ms, total: 27.3 ms\n",
      "Wall time: 28.9 ms\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "%time mnist = MNIST('/tmp/mnist/', train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "image, target = mnist[0]\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIvxAUIQfCIrwA0ERfiAowg8E9f/Ex0YKZYOZcwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fad1e8ee9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.array(image), 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#cc6666'>Внимание, задача!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Релизуйте ниже датасет **`UrlList`**, конструктор которого на вход принимает список ссылок на изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UrlList(Dataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажите его работу на примере:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преобразования данных\n",
    "\n",
    "Конструктор примера, приведенного выше, также как и стандартного `ImageFolder`, принимают параметр `transform` (и `target_transform`).\n",
    "\n",
    "Они служат для того, чтобы загружаемые изображения (обычно это `PIL.Image`) или таргеты преобразовывать в тензоры нужного вида.\n",
    "\n",
    "В `torchvision` входит модуль `transforms` для стандартных примеров таких преобразований:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, `transforms.ToTensor()` преобразует `PIL`-изображение типа uint8 с доменом [0, 256) в тензор с доменом [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_tensor = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), 0.0, 1.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pil_image = mnist[0][0]\n",
    "th_image = to_tensor(pil_image)\n",
    "th_image.shape, th_image.min(), th_image.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А для того, чтобы нормализовывать изображения из ImageNet'а по стандартной схеме, можно объявить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для объединения нескольких преобразваний в одно есть `Compose`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crop_and_tensorize = transforms.Compose([\n",
    "    transforms.CenterCrop(16),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAD5pJREFUeJzt3X+s1fV9x/HXayDtVBSYq1g1Q4zBIHFqCLHVYB1D0SkU\nrQlGN6ZV0mxMWWxajMnaTP+Y69a5aa2h6mQrwWZWJ6m6QqTELJlMxKuo2IrOIXgrOscv/QOR9/44\n37tcrudc7vl8f3Cvn+cjuTm/vu/7efM998X3nO853+/HESEA+fmNw90AgMOD8AOZIvxApgg/kCnC\nD2SK8AOZIvxApgg/kCnCD2RqdJOD2W7s64RjxoxJqrPddc3RRx+dNFZq3ahRo7quGT9+fNJYn1X7\n9u1Lqvvoo4+S6saNG9d1zYEDB7qu2bp1q95///0h/RE3Gv4mTZw4Maku5T+N8847L2ms888/P6ku\nJchXXnll0lifVdu2bUuqe+6555Lq5s+f33XN3r17u66ZOXPmkJflZT+QqVLhtz3H9i9tb7G9tKqm\nANQvOfy2R0n6gaRLJE2VdLXtqVU1BqBeZbb8MyRtiYg3I2KfpIclzaumLQB1KxP+EyW93e/2tuI+\nACNAmb397T5O+NRHebYXSVpUYhwANSgT/m2STu53+yRJ7wxcKCKWSVomNfs5P4DBlXnZ/5yk02yf\nYnuMpAWSVlXTFoC6JW/5I2K/7cWSfi5plKQHI+KVyjoDUKtS3/CLiCclPVlRLwAaxDf8gEwRfiBT\nbvK8/al7+88666yua9atW5cylI499tikOhw+KUe/XXfddUlj7dmzJ6kuRW9vb9c1mzZt0t69e4d0\nVB9bfiBThB/IFOEHMkX4gUwRfiBThB/IFOEHMkX4gUwRfiBThB/IFOEHMkX4gUyNiAN7JkyY0HVN\n6swqkydPTqr7rFq/fn1S3c6dO7uuufDCC5PGSpl6a+zYsUljjQQRwYE9ADoj/ECmCD+QqTLTdZ1s\n+xe2N9t+xfbNVTYGoF5lTuC5X9ItEbHR9lhJz9teExGvVtQbgBolb/kjojciNhbX90jaLKbrAkaM\nUqfu7mN7kqSzJX3qcyGm6wKGp9Lht320pJ9KWhIRuwc+znRdwPBUam+/7SPUCv6KiHi0mpYANKHM\n3n5LekDS5oj4fnUtAWhCmS3/eZL+UNLv2e4pfi6tqC8ANSszUee/SxrSd4gBDD98ww/IVCUf9dXt\ngw8+6LrmlltuSRrr8ssv77rmhRdeSBrr7rvvTqpL0dPTk1Q3a9aspLoPP/yw65ozzjgjaawlS5Yk\n1eWOLT+QKcIPZIrwA5ki/ECmCD+QKcIPZIrwA5ki/ECmCD+QKcIPZIrwA5ki/ECmRsR0XU065phj\nuq7Zs2dP0ljLli1Lqrvhhhu6rrn22muTxlqxYkVSHQ4fpusCMCjCD2SK8AOZKh1+26Nsv2D7Z1U0\nBKAZVWz5b1Zrth4AI0jZ8/afJOkPJN1fTTsAmlJ2y3+XpG9JOlBBLwAaVGbSjssk7YiI5w+x3CLb\nG2xvSB0LQPXKTtox1/Zbkh5Wa/KOHw9cKCKWRcT0iJheYiwAFSszRfetEXFSREyStEDS2ohI+xoZ\ngMbxOT+QqUom7YiIdZLWVfG7ADSDLT+QqRExXVeTdu/e3dhYu3btamysG2+8Malu5cqVSXUHDvDp\n73DHlh/IFOEHMkX4gUwRfiBThB/IFOEHMkX4gUwRfiBThB/IFOEHMkX4gUwRfiBThB/IFHP1HUZH\nHXVUUt0TTzzRdc0FF1yQNNbFF1+cVLd69eqkOpTHXH0ABkX4gUyVnbRjnO1HbL9me7PtL1XVGIB6\nlT2Tz99L+reI+JrtMZKOrKAnAA1IDr/tYyTNlPTHkhQR+yTtq6YtAHUr87J/sqT3JP1jMUvv/bbT\ndl8DaFyZ8I+WdI6kH0bE2ZI+lLR04EJM1wUMT2XCv03StohYX9x+RK3/DA7CdF3A8FRmuq5fS3rb\n9pTirlmSXq2kKwC1K7u3/88krSj29L8p6bryLQFoQqnwR0SPJF7OAyMQ3/ADMsWBPSPQqaee2nVN\nT09P0lg7d+5Mqlu7dm3XNRs2pH0gdM8993Rd0+TffdM4sAfAoAg/kCnCD2SK8AOZIvxApgg/kCnC\nD2SK8AOZIvxApgg/kCnCD2SK8AOZIvxApjiqLxPz589Pqlu+fHlS3dixY5PqUtx6661d16T+u3p7\ne5PqmsRRfQAGRfiBTJWdruvPbb9i+2XbK21/vqrGANQrOfy2T5R0k6TpETFN0ihJC6pqDEC9yr7s\nHy3pN22PVmuevnfKtwSgCWXO279d0t9I2iqpV9KuiFhdVWMA6lXmZf94SfMknSLpi5KOsn1tm+WY\nrgsYhsq87P99Sf8VEe9FxMeSHpX05YELMV0XMDyVCf9WSefaPtK21Zqua3M1bQGoW5n3/OvVmpxz\no6RNxe9aVlFfAGpWdrqu70j6TkW9AGgQ3/ADMkX4gUxxVB8GNW3atKS6u+66q+uaWbNmJY2V4r77\n7kuqu+OOO5Lqtm/fnlSXgqP6AAyK8AOZIvxApgg/kCnCD2SK8AOZIvxApgg/kCnCD2SK8AOZIvxA\npgg/kCkO7EEtxo0b13XN3Llzk8Z66KGHuq5pnXyqe2vXrk2qa/KgJQ7sATAowg9k6pDht/2g7R22\nX+533wTba2y/XlyOr7dNAFUbypb/IUlzBty3VNLTEXGapKeL2wBGkEOGPyKekfTBgLvnSeqb4Hy5\npK9W3BeAmqW+5z8+Inolqbj8QnUtAWhCqVN3D4XtRZIW1T0OgO6kbvnftX2CJBWXOzotyHRdwPCU\nGv5VkhYW1xdKeryadgA0ZSgf9a2U9B+SptjeZvvrkv5K0mzbr0uaXdwGMIIc8j1/RFzd4aHmvq8I\noHJ8ww/IFOEHMsVRfRjxPv74465rRo9O+5R7//79SXWzZ8/uumbdunVJY3FUH4BBEX4gU4QfyBTh\nBzJF+IFMEX4gU4QfyBThBzJF+IFMEX4gU4QfyBThBzJV+zn8MLKdeeaZSXVXXXVV1zUzZsxIGiv1\nIJ0Ur776alLdM888U3En5bHlBzJF+IFMEX4gU6lz9X3P9mu2X7L9mO3u52MGcFilztW3RtK0iDhT\n0q8k3VpxXwBqljRXX0Ssjoi+8xk9K+mkGnoDUKMq3vNfL+mpTg/aXmR7g+0NFYwFoCKlPiC1fZuk\n/ZJWdFomIpZJWlYszwk8gWEiOfy2F0q6TNKsaPIUwAAqkRR+23MkfVvSBRHxUbUtAWhC6lx990ga\nK2mN7R7b99XcJ4CKpc7V90ANvQBoEN/wAzLFUX0j0JQpU7quuemmm5LGuuKKK5LqJk6cmFTXlE8+\n+SSprre3N6nuwIEDSXV1YssPZIrwA5ki/ECmCD+QKcIPZIrwA5ki/ECmCD+QKcIPZIrwA5ki/ECm\nCD+QKcIPZIqj+iqQegTbNddck1S3ePHirmsmTZqUNNZIsGFD9+eGvf3225PGWrVqVVLdcMSWH8gU\n4QcylTRdV7/Hvmk7bB9XT3sA6pI6XZdsnyxptqStFfcEoAFJ03UV/k7StyRxzn5gBEo9b/9cSdsj\n4kXbh1p2kaRFKeMAqE/X4bd9pKTbJF00lOWZrgsYnlL29p8q6RRJL9p+S60ZejfaHt6nawVwkK63\n/BGxSdIX+m4X/wFMj4j3K+wLQM1Sp+sCMMKlTtfV//FJlXUDoDF8ww/I1Gf2wJ7jjz8+qW7q1Kld\n19x7771JY51++ulJdSPB+vXru6658847k8Z6/PHHu64ZjtNnNY0tP5Apwg9kivADmSL8QKYIP5Ap\nwg9kivADmSL8QKYIP5Apwg9kivADmSL8QKYIP5ApRzR3Wj3b70n67w4PHydpOJwNiD4ORh8HG+59\n/E5E/PZQfkGj4R+M7Q0RMZ0+6IM+mumDl/1Apgg/kKnhFP5lh7uBAn0cjD4O9pnpY9i85wfQrOG0\n5QfQoEbDb3uO7V/a3mJ7aZvHP2f7J8Xj621PqqGHk23/wvZm26/YvrnNMl+xvct2T/HzF1X30W+s\nt2xvKsbZ0OZx2/6HYp28ZPucisef0u/f2WN7t+0lA5apbX20mwLe9gTba2y/XlyO71C7sFjmddsL\na+jje7ZfK9b7Y7bHdagd9DmsoI/v2t7eb/1f2qF20Hx9SkQ08iNplKQ3JE2WNEbSi5KmDljmTyTd\nV1xfIOknNfRxgqRziutjJf2qTR9fkfSzhtbLW5KOG+TxSyU9JcmSzpW0vubn6NdqfVbcyPqQNFPS\nOZJe7nffX0taWlxfKunONnUTJL1ZXI4vro+vuI+LJI0urt/Zro+hPIcV9PFdSd8cwnM3aL4G/jS5\n5Z8haUtEvBkR+yQ9LGnegGXmSVpeXH9E0iwfahrgLkVEb0RsLK7vkbRZ0olVjlGxeZL+KVqelTTO\n9gk1jTVL0hsR0emLWJWL9lPA9/87WC7pq21KL5a0JiI+iIj/lbRG0pwq+4iI1RGxv7j5rFrzUtaq\nw/oYiqHk6yBNhv9ESW/3u71Nnw7d/y9TrPRdkn6rroaKtxVnS2p3kvkv2X7R9lO2z6irB0khabXt\n54vpzAcaynqrygJJKzs81tT6kKTjI6JXav1nrX5zQ/bT5HqRpOvVegXWzqGewyosLt5+PNjhbVDX\n66PJ8Lfbgg/8qGEoy1TC9tGSfippSUTsHvDwRrVe+v6upLsl/WsdPRTOi4hzJF0i6U9tzxzYapua\nyteJ7TGS5kr6lzYPN7k+hqrJv5XbJO2XtKLDIod6Dsv6oVqzY58lqVfS37Zrs819g66PJsO/TdLJ\n/W6fJOmdTsvYHi3pWKW9BBqU7SPUCv6KiHh04OMRsTsi9hbXn5R0hO3jqu6j+P3vFJc7JD2m1su3\n/oay3qpwiaSNEfFumx4bWx+Fd/ve2hSXO9os08h6KXYkXibpmijeXA80hOewlIh4NyI+iYgDkn7U\n4fd3vT6aDP9zkk6zfUqxlVkgadWAZVZJ6ttr+zVJazut8FTFPoQHJG2OiO93WGZi374G2zPUWk//\nU2Ufxe8+yvbYvutq7WB6ecBiqyT9UbHX/1xJu/peElfsanV4yd/U+uin/9/BQknt5uP6uaSLbI8v\nXgZfVNxXGdtzJH1b0tyI+KjDMkN5Dsv20X8fz/wOv38o+TpYFXsou9iTealae9ffkHRbcd9fqrVy\nJenzar3s3CLpPyVNrqGH89V6OfSSpJ7i51JJ35D0jWKZxZJeUWuP6bOSvlzT+phcjPFiMV7fOunf\niyX9oFhnmyRNr6GPI9UK87H97mtkfaj1H06vpI/V2np9Xa39PE9Ler24nFAsO13S/f1qry/+VrZI\nuq6GPrao9T667++k75OoL0p6crDnsOI+/rl47l9SK9AnDOyjU74G++EbfkCm+IYfkCnCD2SK8AOZ\nIvxApgg/kCnCD2SK8AOZIvxApv4Pf6l38pzIRKMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fad1ab3a208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(crop_and_tensorize(pil_image)[0].numpy(), 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При определении кастомного трансформера помимо конструктора нужно реализовать лишь метод `__call__`:\n",
    "\n",
    "```python\n",
    "class HorizontalFlip(object):\n",
    "    def __init__(self, mode=0):\n",
    "        self.method = mode\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be flipped.\n",
    "\n",
    "        Returns:\n",
    "            PIL.Image: Randomly flipped image.\n",
    "        \"\"\"\n",
    "        if self.method:\n",
    "            return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        return img\n",
    "\n",
    "```\n",
    "\n",
    "С полным списком стандартных преобразований можно ознакомиться в http://pytorch.org/docs/master/torchvision/transforms.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#cc6666'>Внимание, задача!</font>\n",
    "\n",
    "Реализуйте класс-трансформер, осуществляющий с подаваемым на вход изображением случайное преобразование из группы диэдра $D_4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomD4(object):\n",
    "    def __call__(self, img):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажите его работу на примере из MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузчики данных\n",
    "\n",
    "Основная магия, для которой и нужны датасеты в приведенном выше виде, это загрузчики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузчики создаются на основе датасета и позволяют итерироваться по батчам из него.\n",
    "\n",
    "При этом батчи готовятся (загружаются картинки, обрабатываются и т.д.) сразу в нескольких фоновых процессах!\n",
    "\n",
    "Рассмотрим их на примере с MNIST, при этом добавим к нему трансформер, т.к. лоадеры работают с тензорами или скалярами, но не `PIL.Image`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformed_mnist = MNIST('/tmp/mnist/', train=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_loader = DataLoader(transformed_mnist, batch_size=16, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стандартные параметры приведены выше. При добавлении **`pin_memory=True`** батчи еще и сразу раскладываются по видеокартам.\n",
    "\n",
    "Посмотрим на работу в деле:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [00:05<00:00, 746.33it/s]\n"
     ]
    }
   ],
   "source": [
    "for images, targets in tqdm(mnist_loader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На выходе из лоадера получаем батчи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 28, 28])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество батчей заранее известно (чему был рад `tqdm`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3750"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример данных (при каждом повторении прохода по мнисту пример будет случайным):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fad1aa185f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[0][0].numpy(), 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание моделей\n",
    "\n",
    "Для создания моделей наследуемся от `torch.nn.Module`, `torch.nn` также содержит стандартные \"кирпичики\" моделей.\n",
    "\n",
    "Функциональные версии кирпичей скрыты в `torch.nn.functional`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для макспулинга, активаций, апсемплинга и некоторых других операций есть как \"модульные\", так и \"функциональные\" версии:\n",
    "* `nn.MaxPool2d` <-> `F.max_pool2d`\n",
    "* `nn.ReLU` <-> `F.relu`\n",
    "* `nn.Upsample(mode='bilinar')` <-> `F.upsample(mode='bilinar')`\n",
    "\n",
    "При использовании модульных версий слоев во время создания простой модели можно обходиться `nn.Sequential` для их \"склейки\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d (3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    nn.Conv2d(3, 64, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, 3, padding=1),\n",
    "    nn.ReLU()\n",
    "]\n",
    "unet_down1 = nn.Sequential(*layers)\n",
    "print(unet_down1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение моделей\n",
    "\n",
    "Для обучения моделей необходимо определить функцию потерь, их примеры содержатся все в том же модуле `torch.nn`:\n",
    "\n",
    "```python\n",
    "output = model(torch.cat(x, 1))\n",
    "target = Variable(torch.arange(1, 1001))\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Веса обновлять можно как вручную (или воспользоваться реализованным ранее оптимизатором):\n",
    "\n",
    "```python\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так и пользоваться стандартным оптимизатором из модуля `torch.optim`:\n",
    "```python\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для зануления градиентов теперь можно обращаться к оптимизатору:\n",
    "```python\n",
    "optimizer.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инференс, подсчет ошибки и бэкпроп делаем как раньше:\n",
    "```python\n",
    "output = net(x)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А для обновления весов используем оптимизатор:\n",
    "```python\n",
    "optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы с шагом обучения в **`optim`** есть подмодуль **`torch.optim.lr_scheduler`**:\n",
    "```python\n",
    "from torch.optim import lr_scheduler\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, обучение ResNet на ImageNet по стандартной схеме будет работать примерно так:\n",
    "```python\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "for epoch in range(100):\n",
    "     scheduler.step()\n",
    "     # train(...)\n",
    "     # validate(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch в бою"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание модели сегментации на примере U-Net\n",
    "\n",
    "![img](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто используемые свертки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, dilation=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один **блок кодировщика** состоит из двух последовательных сверток, активаций и опционального батчнорма:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, batch_norm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.conv1 = conv3x3(in_channels, out_channels)\n",
    "        if self.batch_norm:\n",
    "            self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        if self.batch_norm:\n",
    "            self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderBlock(\n",
       "  (conv1): Conv2d (3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = EncoderBlock(3, 64)\n",
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 128, 128])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.zeros(4, 3, 128, 128), volatile=True)\n",
    "\n",
    "block(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Альтернативное объявление:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, batch_norm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential()\n",
    "        self.block.add_module('conv1', conv3x3(in_channels, out_channels))\n",
    "        if batch_norm:\n",
    "            self.block.add_module('bn1', nn.BatchNorm2d(out_channels))\n",
    "        self.block.add_module('relu1', nn.ReLU())\n",
    "        self.block.add_module('conv2', conv3x3(out_channels, out_channels))\n",
    "        if batch_norm:\n",
    "            self.block.add_module('bn2', nn.BatchNorm2d(out_channels))\n",
    "        self.block.add_module('relu2', nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderBlock(\n",
       "  (block): Sequential(\n",
       "    (conv1): Conv2d (3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = EncoderBlock(3, 64)\n",
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 128, 128])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.zeros(4, 3, 128, 128), volatile=True)\n",
    "\n",
    "block(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И \"функциональная версия\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder_block(in_channels, out_channels, batch_norm=False):\n",
    "    block = nn.Sequential()\n",
    "    block.add_module('conv1', conv3x3(in_channels, out_channels))\n",
    "    if batch_norm:\n",
    "        block.add_module('bn1', nn.BatchNorm2d(out_channels))\n",
    "    block.add_module('relu1', nn.ReLU())\n",
    "    block.add_module('conv2', conv3x3(out_channels, out_channels))\n",
    "    if batch_norm:\n",
    "        block.add_module('bn2', nn.BatchNorm2d(out_channels))\n",
    "    block.add_module('relu2', nn.ReLU())\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv1): Conv2d (3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = encoder_block(3, 64)\n",
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 128, 128])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.zeros(4, 3, 128, 128), volatile=True)\n",
    "\n",
    "block(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кодировщик в целом состоит из таких блоков.\n",
    "\n",
    "Его конструкция определяется входными каналами, количеством фильтров в первом блоке и количеством блоков.\n",
    "\n",
    "Помним также, что для работы сети нам нужно запоминать промежуточные активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_filters, num_blocks):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_blocks = num_blocks\n",
    "        for i in range(num_blocks):\n",
    "            in_channels = in_channels if not i else num_filters * 2 ** (i - 1)\n",
    "            out_channels = num_filters * 2**i\n",
    "            self.add_module(f'block{i + 1}', encoder_block(in_channels, out_channels))\n",
    "            if i != num_blocks - 1:\n",
    "                self.add_module(f'pool{i + 1}', nn.MaxPool2d(2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        acts = []\n",
    "        for i in range(self.num_blocks):\n",
    "            x = self.__getattr__(f'block{i + 1}')(x)\n",
    "            acts.append(x)\n",
    "            if i != self.num_blocks - 1:\n",
    "                x = self.__getattr__(f'pool{i + 1}')(x)\n",
    "        return acts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь как раз помогает подход к построению через с **`add_module`**, т.к. их количество переменно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (block1): Sequential(\n",
       "    (conv1): Conv2d (3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d (8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (block2): Sequential(\n",
       "    (conv1): Conv2d (8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d (16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (block3): Sequential(\n",
       "    (conv1): Conv2d (16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d (32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (pool3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (block4): Sequential(\n",
       "    (conv1): Conv2d (32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(in_channels=3, num_filters=8, num_blocks=4)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([4, 8, 512, 512]),\n",
       " torch.Size([4, 16, 256, 256]),\n",
       " torch.Size([4, 32, 128, 128]),\n",
       " torch.Size([4, 64, 64, 64])]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.zeros(4, 3, 512, 512), volatile=True)\n",
    "\n",
    "[_.shape for _ in encoder(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Блок декодировщика состоит из апскейлинга входа \"снизу\", объединения двух входов и сверток как в кодировщике:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.uppool = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.upconv = conv3x3(out_channels * 2, out_channels)\n",
    "        self.conv1 = conv3x3(out_channels * 2, out_channels)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, down, left):\n",
    "        x = self.uppool(down)\n",
    "        x = self.upconv(x)\n",
    "        x = torch.cat([left, x], 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "block = DecoderBlock(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 16, 256, 256]), torch.Size([4, 8, 512, 512]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1].shape, y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 512, 512])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block(y[1], y[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Декодировщик собираем из таких блоков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_filters, num_blocks):\n",
    "        super().__init__()\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            self.add_module(f'block{num_blocks - i}', DecoderBlock(num_filters * 2**i))\n",
    "\n",
    "    def forward(self, acts):\n",
    "        up = acts[-1]\n",
    "        for i, left in enumerate(acts[-2::-1]):\n",
    "            up = self.__getattr__(f'block{i + 1}')(up, left)\n",
    "        return up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(8, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 512, 512])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([4, 8, 512, 512]),\n",
       " torch.Size([4, 16, 256, 256]),\n",
       " torch.Size([4, 32, 128, 128]),\n",
       " torch.Size([4, 64, 64, 64])]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_.shape for _ in encoder(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 512, 512])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(encoder(x)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net состоит из такого кодировщика и декодировщика, а также финального слоя классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=3, num_filters=64, num_blocks=4):\n",
    "        super().__init__()\n",
    "\n",
    "        print(f'=> Building {num_blocks}-blocks {num_filters}-filter U-Net')\n",
    "\n",
    "        self.encoder = Encoder(in_channels, num_filters, num_blocks)\n",
    "        self.decoder = Decoder(num_filters, num_blocks - 1)\n",
    "        self.final = nn.Conv2d(num_filters, num_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        acts = self.encoder(x)\n",
    "        x = self.decoder(acts)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Интеграционное тестирование\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building 4-blocks 64-filter U-Net\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 416, 416])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet(num_classes=1)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "images = Variable(torch.randn(4, 3, 416, 416), volatile=True)\n",
    "if torch.cuda.is_available():\n",
    "    images = images.cuda()\n",
    "\n",
    "model.forward(images).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На выходе получаем бинарную маску из линейных активаций.\n",
    "\n",
    "Для обучения такой модели используются функции потерь с **WithLogits** в названии.\n",
    "\n",
    "В вероятности их можно превращать с помощью `torch.nn.functional.sigmoid`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование готового кодировщика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Структура блоков кодировщика рассмотренной только что сети сильно походит на таковую в сетях VGG:\n",
    "\n",
    "![img](https://www.pyimagesearch.com/wp-content/uploads/2017/03/imagenet_vgg16.png)\n",
    "\n",
    "Посмотрим на неё же из недр `torchvision`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision.models import vgg13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG13 - версия сети с 2 сверткаим на каждый блок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = vgg13()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d (3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (5): Conv2d (64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (10): Conv2d (128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (15): Conv2d (256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): ReLU(inplace)\n",
       "    (17): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace)\n",
       "    (19): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (20): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): ReLU(inplace)\n",
       "    (22): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): ReLU(inplace)\n",
       "    (24): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=4096)\n",
       "    (4): ReLU(inplace)\n",
       "    (5): Dropout(p=0.5)\n",
       "    (6): Linear(in_features=4096, out_features=1000)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификатор нам не нужен, интересуют только признаки.\n",
    "\n",
    "Они в свою очередь делятся на блоки conv-relu-conv-relu + maxpooling.\n",
    "\n",
    "Реализуем кодировщик на основе вычленения нужных блоков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VGG13Encoder(nn.Module):\n",
    "    def __init__(self, num_blocks, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        backbone = vgg13(pretrained=pretrained).features\n",
    "\n",
    "        self.num_blocks = num_blocks\n",
    "        for i in range(self.num_blocks):\n",
    "            block = nn.Sequential(*[backbone[j] for j in range(i * 5, i * 5 + 4)])\n",
    "            self.add_module(f'block{i + 1}', block)\n",
    "            if i != num_blocks - 1:\n",
    "                self.add_module(f'pool{i + 1}', nn.MaxPool2d(2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        acts = []\n",
    "        for i in range(self.num_blocks):\n",
    "            x = self.__getattr__(f'block{i + 1}')(x)\n",
    "            acts.append(x)\n",
    "            if i != self.num_blocks - 1:\n",
    "                x = self.__getattr__(f'pool{i + 1}')(x)\n",
    "        return acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG13Encoder(\n",
       "  (block1): Sequential(\n",
       "    (0): Conv2d (3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (block2): Sequential(\n",
       "    (0): Conv2d (64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (block3): Sequential(\n",
       "    (0): Conv2d (128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "  )\n",
       "  (pool3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (block4): Sequential(\n",
       "    (0): Conv2d (256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_encoder = VGG13Encoder(num_blocks=4)\n",
    "vgg_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним с \"ванильным\" кодировщиком:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (block1): Sequential(\n",
       "    (conv1): Conv2d (3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (block2): Sequential(\n",
       "    (conv1): Conv2d (64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (block3): Sequential(\n",
       "    (conv1): Conv2d (128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (pool3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (block4): Sequential(\n",
       "    (conv1): Conv2d (256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(in_channels=3, num_filters=64, num_blocks=4)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили идентичную структуру!\n",
    "\n",
    "Только теперь у нас уже есть предобученные слои для выделения признаков в кодировщике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#cc6666'>Внимание, задача!</font>\n",
    "\n",
    "\n",
    "**Реализуйте датасет** для игрушечной задачи сегментации, реализующий такие данные:\n",
    "\n",
    "![img](https://raw.githubusercontent.com/jakeret/tf_unet/master/docs/toy_problem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.е. необходимо генерировать цветные эллипсы на цветном фоне и к итоговой картинке добавлять шум разной природы.\n",
    "\n",
    "При этом датасет выдает как изображение, так и его бинарную маску."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ellipses(Dataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения этой задачи **реализуйте сеть LinkNet** на основе кодировщика двух предобученных сетей `torchvision` из разных семейств (не VGG).\n",
    "\n",
    "В качестве лосса рекомендуется использовать `BCEWithLogitsLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinkNet(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=3, num_filters=64, num_blocks=4):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проиллюстрируйте процесс обучения** сети скриншотами из `tensorboard` (см. `lanpa/tensorboard-pytorch`).\n",
    "\n",
    "Необходимо в частности отобразить кривую обучения и примеры эволюции качества выдаваемых масок."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
