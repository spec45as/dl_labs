{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часть материала украдена из курса \"Глубинное обучение\" ФКН ВШЭ https://www.hse.ru/ba/ami/courses/205504078.html, за что им большое спасибо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Актуальная версия этого ноутбука обретается по адресу\n",
    "https://github.com/nadiinchi/dl_labs/blob/master/lab_pytorch.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Устанавливаем pytorch\n",
    "\n",
    "## Linux/OSX\n",
    "\n",
    "\n",
    "На оффсайте http://pytorch.org/ надо выбрать подходящую конфигурацию и скачать.\n",
    "\n",
    "Версию python можно узнать в терминале:\n",
    "```\n",
    "python --version\n",
    "```\n",
    "\n",
    "\n",
    "## Windows without GPU\n",
    "\n",
    "Проще всего поставить при помощи конды:\n",
    "```\n",
    "conda install -c peterjc123 pytorch\n",
    "```\n",
    "\n",
    "## Windows with GPU\n",
    "\n",
    "Смотрите https://github.com/peterjc123/pytorch-scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![img](https://s1.postimg.org/6fl45xnvnj/pytorch-logo-dark.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :\n",
      " [[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]]\n",
      "add 5 :\n",
      "[[ 5  6  7  8]\n",
      " [ 9 10 11 12]\n",
      " [13 14 15 16]\n",
      " [17 18 19 20]]\n",
      "X*X^T  :\n",
      " [[ 14  38  62  86]\n",
      " [ 38 126 214 302]\n",
      " [ 62 214 366 518]\n",
      " [ 86 302 518 734]]\n",
      "mean over cols :\n",
      "[ 1.5  5.5  9.5 13.5]\n",
      "cumsum of cols :\n",
      "[[ 0  1  2  3]\n",
      " [ 4  6  8 10]\n",
      " [12 15 18 21]\n",
      " [24 28 32 36]]\n"
     ]
    }
   ],
   "source": [
    "# numpy world\n",
    "\n",
    "x = np.arange(16).reshape(4, 4)\n",
    "\n",
    "print(\"X :\\n %s\" % x)\n",
    "print(\"add 5 :\\n%s\" % (x + 5))\n",
    "print(\"X*X^T  :\\n\", np.dot(x, x.T))\n",
    "print(\"mean over cols :\\n%s\" % (x.mean(axis=-1)))\n",
    "print(\"cumsum of cols :\\n%s\" % (np.cumsum(x, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of y: <class 'torch.Tensor'>\n",
      "Type of y <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# pytorch world\n",
    "\n",
    "x = np.arange(16).reshape(4, 4)\n",
    "\n",
    "y = torch.from_numpy(x)\n",
    "print('Type of y:', type(y))\n",
    "y = torch.from_numpy(x).type(torch.FloatTensor)\n",
    "print('Type of y', type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of x: <class 'torch.Tensor'>\n",
      "X :\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]])\n",
      "add 5 :\n",
      "tensor([[ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12],\n",
      "        [13, 14, 15, 16],\n",
      "        [17, 18, 19, 20]])\n",
      "X*X^T  :\n",
      " tensor([[ 14,  38,  62,  86],\n",
      "        [ 38, 126, 214, 302],\n",
      "        [ 62, 214, 366, 518],\n",
      "        [ 86, 302, 518, 734]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can only calculate the mean of floating types. Got Long instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-24a7b83ee581>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"add 5 :\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X*X^T  :\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mean over cols :\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cumsum of cols :\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can only calculate the mean of floating types. Got Long instead."
     ]
    }
   ],
   "source": [
    "x = torch.arange(0,16).view(4,4)\n",
    "print('Type of x:', type(x))\n",
    "\n",
    "print(\"X :\\n%s\" % x)\n",
    "print(\"add 5 :\\n%s\" % (x + 5))\n",
    "print(\"X*X^T  :\\n\", torch.matmul(x, x.transpose(1, 0)))\n",
    "print(\"mean over cols :\\n\", torch.mean(x, dim=-1))\n",
    "print(\"cumsum of cols :\\n\", torch.cumsum(x, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy vs Pytorch\n",
    "\n",
    "Numpy и Pytorch не требуют описания статического графа вычислений. \n",
    "\n",
    "Можно отлаживаться с помощью pdb или просто print.\n",
    "\n",
    "API несколько различается:\n",
    "\n",
    "```\n",
    "x.reshape([1,2,8]) -> x.view(1,2,8)\n",
    "x.sum(axis=-1) -> x.sum(dim=-1)\n",
    "x.astype('int64') -> x.type(torch.LongTensor)\n",
    "```\n",
    "\n",
    "\n",
    "Легко конвертировать между собой:\n",
    "\n",
    "```\n",
    "torch.from_numpy(npx) -- вернет Tensor\n",
    "tt.numpy() -- вернет Numpy Array\n",
    "```\n",
    "\n",
    "\n",
    "Если что:\n",
    "- смотрите документацию\n",
    "- гуглите (Stackoverflow/tutorials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         1.         0.9999999  1.         0.9999999  1.\n",
      " 0.99999994 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "x = torch.linspace(0, 2 * np.pi, 16)\n",
    "\n",
    "# Mini-task: compute a vector of sin^2(x) + cos^2(x)\n",
    "out = torch.sin(x)**2 + torch.cos(x)**2\n",
    "\n",
    "print(out.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.4189, 0.8378, 1.2566, 1.6755, 2.0944, 2.5133, 2.9322, 3.3510,\n",
       "        3.7699, 4.1888, 4.6077, 5.0265, 5.4454, 5.8643, 6.2832])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-place operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда работаем с большими массивами, память надо экономить.\n",
    "Некоторые операции происходят с созданием нового объекта – результата вычислений,\n",
    "некоторые изменяют данный объект (in-place операции).\n",
    "В pytorch обычно эти операции различаются добавлением подчеркивания:\n",
    "```\n",
    "x.exp()   # not-in-place operation\n",
    "x.exp_()  # in-place operation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not-in-place:\n",
      "\tx.exp():\t\t [  1.           2.71828175   7.38905621  20.08553696]\n",
      "\tx:\t\t\t [ 0.  1.  2.  3.]\n",
      "In-place:\n",
      "\tx.exp_():\t\t [  1.           2.71828175   7.38905621  20.08553696]\n",
      "\tx after x.exp_():\t [  1.           2.71828175   7.38905621  20.08553696]\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "print('Not-in-place:')\n",
    "print('\\tx.exp():\\t\\t', x.exp().numpy())\n",
    "print('\\tx:\\t\\t\\t', x.numpy())\n",
    "print('In-place:')\n",
    "print('\\tx.exp_():\\t\\t', x.exp_().numpy())\n",
    "print('\\tx after x.exp_():\\t', x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  2.]\n",
      " [ 4.  6.]]\n",
      "[[ 0.  2.]\n",
      " [ 4.  6.]]\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 4).view(2, 2)\n",
    "y = torch.arange(4, 8).view(2, 2)\n",
    "z = torch.arange(8, 12).view(2, 2)\n",
    "\n",
    "# Not-in-place:\n",
    "u = x + 2 * y - z    # 3 array allocations?\n",
    "print(u.numpy())\n",
    "\n",
    "# In-place\n",
    "u = y.clone()        # 1 array allocation\n",
    "u.mul_(2)\n",
    "u.add_(x)\n",
    "u.sub_(z)\n",
    "print(u.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting на pytorch (аналогично numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b: tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "c: tensor([[-0.9896, -0.5006,  0.3738,  0.2424],\n",
      "        [ 0.5608, -0.7114,  0.5221,  2.1434],\n",
      "        [-0.9807, -0.5838,  0.1753,  0.2581],\n",
      "        [-1.1661, -0.7064,  1.2918,  1.1869]])\n",
      "b + c: tensor([[ 0.0104,  0.4994,  1.3738,  1.2424],\n",
      "        [ 0.5608, -0.7114,  0.5221,  2.1434],\n",
      "        [ 0.0193,  0.4162,  1.1753,  1.2581],\n",
      "        [-1.1661, -0.7064,  1.2918,  1.1869]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([1, 1, 1, 2]).view(4, 1)\n",
    "b = torch.Tensor([1, 0, 1, 0]).view(4, 1)\n",
    "c = torch.randn(16).view(4, 4)\n",
    "#print('a:', a)\n",
    "print('b:', b)\n",
    "#print('a + b:', a + b)\n",
    "print('c:', c)\n",
    "print('b + c:', b + c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более подробную информацию можно найти на http://pytorch.org/docs/master/notes/broadcasting.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с тензорами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дано 100 объектов, каждый из которых описывается 10-мерным вектором, и 5 точек, каждая из которых также задается 10-мерным вектором. Объекты лежат в матрице X, точки – в матрице Y.\n",
    "\n",
    "Надо для каждого объекта из X найти индекс ближайшей точки из Y только с помощью операций над тензорами\n",
    "(нельзя использовать циклы, list comprehensions, рекурсию, etc,\n",
    "потому что решение с ними будет работать в несколько раз или на несколько порядков медленнее)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(100, 10)\n",
    "Y = torch.randn(5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3604, -1.3042, -1.2882, -1.1517, -1.7014, -0.8171,  1.1845,  0.3310,\n",
       "         -0.1032,  0.0948],\n",
       "        [ 2.1113, -1.2634,  1.3762, -0.6529,  1.7268,  1.2292,  0.7847, -0.9610,\n",
       "          1.4614,  1.1020],\n",
       "        [-0.9165, -1.9001, -0.4126,  0.0477,  0.2103, -1.4175, -0.2743, -0.1500,\n",
       "         -0.3288, -2.0448],\n",
       "        [-0.5917,  0.1087, -1.7812,  0.3754,  0.9525, -0.7125,  0.2131, -1.1248,\n",
       "         -0.6093, -0.2496],\n",
       "        [-0.1642, -0.7829,  1.7409,  0.1859,  0.5054, -0.8616, -0.0278,  0.0549,\n",
       "         -0.6184,  0.6957]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.view(1, 5, 10)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 3, 3, 3, 4, 3, 3, 3, 3, 0, 3, 4, 4, 3, 2, 3, 4, 3, 2, 1, 2,\n",
       "       4, 3, 3, 3, 1, 3, 3, 1, 4, 4, 3, 0, 0, 4, 1, 1, 4, 2, 4, 4, 4, 4,\n",
       "       4, 4, 0, 2, 4, 3, 4, 4, 4, 4, 2, 3, 0, 4, 3, 2, 4, 4, 4, 3, 0, 3,\n",
       "       2, 4, 4, 4, 4, 3, 4, 2, 4, 2, 4, 4, 1, 3, 3, 0, 4, 2, 4, 2, 2, 4,\n",
       "       0, 0, 4, 0, 1, 4, 1, 1, 3, 1, 0, 4])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((X.view(100, 1, 10) - Y.view(1, 5, 10))**2).sum(dim=2).min(dim=1)[1].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение с семинара:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 3, 3, 3, 4, 3, 3, 3, 3, 0, 3, 4, 4, 3, 2, 3, 4, 3, 2, 1, 2,\n",
       "       4, 3, 3, 3, 1, 3, 3, 1, 4, 4, 3, 0, 0, 4, 1, 1, 4, 2, 4, 4, 4, 4,\n",
       "       4, 4, 0, 2, 4, 3, 4, 4, 4, 4, 2, 3, 0, 4, 3, 2, 4, 4, 4, 3, 0, 3,\n",
       "       2, 4, 4, 4, 4, 3, 4, 2, 4, 2, 4, 4, 1, 3, 3, 0, 4, 2, 4, 2, 2, 4,\n",
       "       0, 0, 4, 0, 1, 4, 1, 1, 3, 1, 0, 4])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((X.view(100, 1, 10) - Y.view(1, 5, 10)) ** 2).sum(dim=-1).min(dim=-1)[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.view(10000)*Y.view(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.view(100, 1, 10)*Y.view(1, 5, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это решение плохо тем, что в качестве промежуточного результата вычилений в нем присутствует трехмерный тензор,\n",
    "который занимает $O(NMD)$ памяти, где N – число объектов, M – число точек, D – размерность пространства."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Внимание, задача!\n",
    "Утверждается, что есть другое решение с такой же скоростью работы,\n",
    "но использующее $O(NM)$ памяти для результатов промежуточных вычислений.\n",
    "Предлагается найти его.\n",
    "\n",
    "Подсказка: найти матрицу попарных скалярных произведений между объектами\n",
    "и точками можно с помощью одного матричного умножения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1]) torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "y_len = (Y*Y).sum(dim=1).view(1, 5)\n",
    "x_len = (X*X).sum(dim=1).view(100, 1)\n",
    "print(x_len.shape, y_len.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([15.0292, 16.1677, 10.0831, 14.2180,  8.0362, 11.1215,  7.3847, 17.0944,\n",
       "         13.2651,  9.6069, 15.8578, 11.0606,  9.0031, 12.2729,  8.2995,  6.1342,\n",
       "         15.3748, 16.6749, 13.0416, 11.7368,  8.1519, 14.3150, 15.5513,  9.6241,\n",
       "         11.5827,  8.6665, 17.5197, 18.2771, 15.9614, 21.0543, 16.2268,  8.1630,\n",
       "         17.9217,  6.6945, 13.5139, 30.5042, 20.0702, 18.2512,  7.3144,  7.4630,\n",
       "          9.3129, 14.2937, 11.3682,  6.1920, 10.5531,  7.7465,  8.3368,  4.8958,\n",
       "          8.5424, 11.0976,  8.6775,  8.6621,  6.9408, 12.7528, 13.7475,  8.4501,\n",
       "         10.1724,  8.3857, 10.2466, 15.5640,  5.6488, 19.4874,  7.7306, 24.5776,\n",
       "         17.0355,  6.9286, 15.7280, 12.1196,  9.7770,  8.5610, 12.4588, 24.6974,\n",
       "         15.4345, 10.0344, 12.9253, 25.5975, 13.2550,  9.8348,  9.6431,  5.4228,\n",
       "          3.7890,  6.3498, 26.2010,  6.3754, 13.3537,  7.9534, 14.3830,  8.6567,\n",
       "         17.3364, 20.3070, 13.9177, 32.4125, 12.4101, 11.1068, 13.8100, 17.9025,\n",
       "         17.1490, 11.4814, 11.1500, 20.1775]),\n",
       " tensor([0, 0, 3, 3, 3, 4, 3, 3, 3, 3, 0, 3, 4, 4, 3, 2, 3, 4, 3, 2, 1, 2, 4, 3,\n",
       "         3, 3, 1, 3, 3, 1, 4, 4, 3, 0, 0, 4, 1, 1, 4, 2, 4, 4, 4, 4, 4, 4, 0, 2,\n",
       "         4, 3, 4, 4, 4, 4, 2, 3, 0, 4, 3, 2, 4, 4, 4, 3, 0, 3, 2, 4, 4, 4, 4, 3,\n",
       "         4, 2, 4, 2, 4, 4, 1, 3, 3, 0, 4, 2, 4, 2, 2, 4, 0, 0, 4, 0, 1, 4, 1, 1,\n",
       "         3, 1, 0, 4]))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((x_len + y_len) - 2 * torch.matmul(X,Y.t())).min(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA\n",
    "`x.cuda()` копирует тензор на GPU и возвращает объект, соответствующий этому скопированному тензору.\n",
    "Можно явно указать номер GPU, на который нужно скопировать тензор: `x.cuda(gpu_id)`.\n",
    "Если тензор уже лежал на нужном GPU, то возвращается сам тензор, копирования не производится.\n",
    "Аналогично работает `x.cpu()`. \n",
    "\n",
    "Операции можно осуществлять только над тензорами, лежащими на одном устройстве.\n",
    "Нарушение этого правила приводит к ошибке.\n",
    "Результат операции находится на том же устройстве, что и операнды."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor vs Variable\n",
    "\n",
    "http://pytorch.org/docs/master/autograd.html#variable\n",
    "\n",
    "`Variable` – обертка над Tensor для использования в вычислительных графах. Позволяет вычислять градиенты автоматически.\n",
    "\n",
    "Tensor и Variable конвертируются друг в друга:\n",
    "```\n",
    "tensor to variable: Variable(x)\n",
    "variable to tensor: x.data\n",
    "```\n",
    "\n",
    "Нельзя смешивать Tensor и Variable в одной операции.\n",
    "\n",
    "Некоторые операции могут работать только с тензорами, некоторые только с переменными (torch.nn.functional.whatever)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "sequence = torch.randn(1, 8, 10)\n",
    "filters = torch.randn(2, 8, 3)\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of Variables:\n",
      "Variable containing:\n",
      " 0.9240\n",
      " 0.9564\n",
      " 4.3621\n",
      "-1.7138\n",
      " 0.0694\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# works:\n",
    "print('sum of Variables:')\n",
    "print(Variable(a) + Variable(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of Variable and Tensor:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "add() received an invalid combination of arguments - got (torch.FloatTensor), but expected one of:\n * (float other, float alpha)\n * (Variable other, float alpha)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2fb7f3cf0dec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# will not work:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sum of Variable and Tensor:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: add() received an invalid combination of arguments - got (torch.FloatTensor), but expected one of:\n * (float other, float alpha)\n * (Variable other, float alpha)\n"
     ]
    }
   ],
   "source": [
    "# will not work:\n",
    "print('sum of Variable and Tensor:')\n",
    "print(Variable(a) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1d over Variables:\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -6.3957 -0.9862  1.7453  5.0739 -5.6403 -0.8131 -1.6775 -3.7941\n",
      " -0.0211 -3.5564 -4.0188  6.0429 -2.6536  1.4192 -0.7251 -3.7977\n",
      "[torch.FloatTensor of size 1x2x8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# works:\n",
    "print('conv1d over Variables:')\n",
    "print(torch.nn.functional.conv1d(Variable(sequence), Variable(filters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1d (tensors):\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument 0 is not a Variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b8774cf17ce8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# will not work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conv1d (tensors):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/soft/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv1d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0m_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 0 is not a Variable"
     ]
    }
   ],
   "source": [
    "# will not work\n",
    "print(\"conv1d (tensors):\")\n",
    "print(torch.nn.functional.conv1d(sequence, filters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic gradients\n",
    "\n",
    "Автоматическое вычисление градиентов:\n",
    "\n",
    "1. Создать переменную: `a = Variable(..., requires_grad=True)`\n",
    "\n",
    "2. Определить какую-нибудь дифференцируемую _скалярную_ функцию `loss = whatever(a)`\n",
    "\n",
    "3. Запросить обратный проход `loss.backward()`\n",
    "\n",
    "4. Градиенты будут доступны в `a.grads`\n",
    "\n",
    "\n",
    "Есть два важных отличия Pytorch от Theano/TF:\n",
    "\n",
    "1. Функцию ошибки можно изменять динамически, например на каждом минибатче.\n",
    "\n",
    "2. После вычисления `.backward()` градиенты сохраняются в `.grad` каждой задействованной переменной, при повторных вызовах градиенты суммируются. Это позволяет использовать несколько функций ошибок или виртуально увеличивать batch_size. Поэтому после каждого шага оптимизатора градиенты стоит обнулять."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Простой пример использования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: tensor(3.3911, grad_fn=<StdBackward0>)\n",
      "z: tensor([ 9.3401, 10.0412, 16.8573, 11.9494], grad_fn=<AddBackward>)\n",
      "x: tensor([-0.6284,  0.3176,  2.1090,  1.2199], requires_grad=True)\n",
      "y: tensor([-1.0247, -0.3602,  1.8032,  1.2641])\n",
      "dp / dx: tensor([-0.2794, -0.0256,  1.5374, -0.0153])\n",
      "dp / dy: tensor([-0.3427,  0.0451,  3.5963, -0.0296])\n"
     ]
    }
   ],
   "source": [
    "x_tensor = torch.randn(4)\n",
    "y_tensor = torch.randn(4)\n",
    "x = Variable(x_tensor, requires_grad=True)\n",
    "y = Variable(y_tensor, requires_grad=True)\n",
    "z = x * y * y + 10\n",
    "p = z.std()\n",
    "p.backward()\n",
    "\n",
    "print('p:', p)\n",
    "print('z:', z)\n",
    "\n",
    "print('x:', x)\n",
    "print('y:', y.data)\n",
    "\n",
    "print('dp / dx:', x.grad.data)\n",
    "print('dp / dy:', y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обнуление градиентов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [1. 1. 1. 1.]\n",
      "dp / dx: [2. 2. 2. 2.]\n",
      "x: [0.5 0.5 0.5 0.5]\n",
      "dp / dx: [-2. -2. -2. -2.]\n"
     ]
    }
   ],
   "source": [
    "x_tensor = torch.Tensor([1, 1, 1, 1])\n",
    "x = Variable(x_tensor, requires_grad=True)\n",
    "y = x ** 2\n",
    "p = y.sum()\n",
    "p.backward()\n",
    "print('x:', x.data.numpy())\n",
    "print('dp / dx:', x.grad.data.numpy())\n",
    "x.data -= 0.5\n",
    "y = 1 / x\n",
    "p = y.sum()\n",
    "p.backward()\n",
    "print('x:', x.data.numpy())\n",
    "print('dp / dx:', x.grad.data.numpy())\n",
    "# суммируется. понятно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [ 1.  1.  1.  1.]\n",
      "dp / dx: [ 2.  2.  2.  2.]\n",
      "x: [ 0.5  0.5  0.5  0.5]\n",
      "dp / dx: [-4. -4. -4. -4.]\n"
     ]
    }
   ],
   "source": [
    "x_tensor = torch.Tensor([1, 1, 1, 1])\n",
    "x = Variable(x_tensor, requires_grad=True)\n",
    "y = x ** 2\n",
    "p = y.sum()\n",
    "p.backward()\n",
    "print('x:', x.data.numpy())\n",
    "print('dp / dx:', x.grad.data.numpy())\n",
    "x.grad.detach_()       # extracting gradient Variable from the previous computational graph (optional)\n",
    "x.grad.data.zero_()    # zero gradinents\n",
    "x.data -= 0.5\n",
    "y = 1 / x\n",
    "p = y.sum()\n",
    "p.backward()\n",
    "print('x:', x.data.numpy())\n",
    "print('dp / dx:', x.grad.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaf vs Non-leaf Variable\n",
    "\n",
    "Градиенты будут сохранены и доступны для использования только для `leaf-variable`.\n",
    "Такое поведение по умолчанию сделано ради экономии памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.data: [-1.4239416 -0.6444806  0.9575765  0.793133 ]\n",
      "y.data: [-0.4239416  0.3555194  1.9575765  1.793133 ]\n",
      "p.data: 3.6822872\n",
      "x.grad: [1. 1. 1. 1.]\n",
      "y.grad: None\n",
      "p.grad: None\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.randn(4), requires_grad=True)  # leaf variable\n",
    "y = x + 1                                         # not a leaf variable\n",
    "p = y.sum()                                       # not a leaf variable\n",
    "p.backward()\n",
    "print('x.data:', x.data.numpy())\n",
    "print('y.data:', y.data.numpy())\n",
    "print('p.data:', p.data.numpy())\n",
    "print('x.grad:', x.grad.data.numpy())\n",
    "print('y.grad:', y.grad)\n",
    "print('p.grad:', p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: [1. 1. 1. 1.]\n",
      "y.grad: [1. 1. 1. 1.]\n",
      "z.grad: None\n",
      "p.grad: None\n",
      "x.is_leaf: True\n",
      "y.is_leaf: True\n",
      "z.is_leaf: False\n",
      "p.is_leaf: False\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.randn(4), requires_grad=True)  # leaf variable\n",
    "y = Variable(torch.randn(4), requires_grad=True)  # leaf variable\n",
    "z = x + y    # not a leaf variable\n",
    "p = z.sum()  # not a leaf variable\n",
    "p.backward()\n",
    "print('x.grad:', x.grad.data.numpy())\n",
    "print('y.grad:', y.grad.data.numpy())\n",
    "print('z.grad:', z.grad)\n",
    "print('p.grad:', p.grad)\n",
    "print('x.is_leaf:', x.is_leaf)\n",
    "print('y.is_leaf:', y.is_leaf)\n",
    "print('z.is_leaf:', z.is_leaf)\n",
    "print('p.is_leaf:', p.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Листовые вершины без градиентов\n",
    "Листовые вершины, в которых не требуется вычислять градиент, создаются с помощью `Variable(..., requires_grad=False)`.\n",
    "Для корректного вызова `.backward()` требуется, чтобы хотя бы для одной листовой вершины требовался градиент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: [ 1.  1.  1.  1.]\n",
      "y.grad: None\n",
      "z.grad: None\n",
      "p.grad: None\n",
      "x.is_leaf: True\n",
      "y.is_leaf: True\n",
      "z.is_leaf: False\n",
      "p.is_leaf: False\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.randn(4), requires_grad=True)   # leaf variable\n",
    "y = Variable(torch.randn(4), requires_grad=False)  # leaf variable\n",
    "z = x + y    # not a leaf variable\n",
    "p = z.sum()  # not a leaf variable\n",
    "p.backward()\n",
    "print('x.grad:', x.grad.data.numpy())\n",
    "print('y.grad:', y.grad)\n",
    "print('z.grad:', z.grad)\n",
    "print('p.grad:', p.grad)\n",
    "print('x.is_leaf:', x.is_leaf)\n",
    "print('y.is_leaf:', y.is_leaf)\n",
    "print('z.is_leaf:', z.is_leaf)\n",
    "print('p.is_leaf:', p.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что для вычисления градиента нужно, чтобы хотя бы одна листовая вершина графа вычисления функции\n",
    "имела `requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-173-6e420832deb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m    \u001b[0;31m# not a leaf variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# not a leaf variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# will not work:\n",
    "x = Variable(torch.randn(4), requires_grad=False)  # leaf variable\n",
    "y = Variable(torch.randn(4), requires_grad=False)  # leaf variable\n",
    "z = x + y    # not a leaf variable\n",
    "p = z.sum()  # not a leaf variable\n",
    "p.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиенты промежуточных вершин\n",
    "Для промежуточных вершин мы можем запросить сохранение градиентов с помощью функции `.retain_grad()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp / dx: [ 2.64347     1.3539231  -0.43842167  0.9418467 ]\n",
      "dp / dw: [-1.3434205  -0.9072092   0.32689726 -0.85105234]\n",
      "dp / dz: [-2.686841  -1.8144184  0.6537945 -1.7021047]\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.randn(4), requires_grad=True)   # leaf variable\n",
    "z = Variable(torch.randn(4), requires_grad=True)   # leaf variable\n",
    "w = z * 2      # not a leaf variable\n",
    "y = x * w + 1  # forward pass before retaining gradient is ok\n",
    "p = y.sum()\n",
    "\n",
    "w.retain_grad()\n",
    "\n",
    "p.backward()\n",
    "print('dp / dx:', x.grad.data.numpy())\n",
    "print('dp / dw:', w.grad.data.numpy())\n",
    "print('dp / dz:', z.grad.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратим внимание, что даже при наличии в графе вычислений не-листовых вершин, требующих вычисления градиентов,\n",
    "`.backward()` выдает ошибку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-290c0473d24e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretain_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# will not work\n",
    "x = Variable(torch.randn(4), requires_grad=False)   # leaf variable\n",
    "z = Variable(torch.randn(4), requires_grad=False)   # leaf variable\n",
    "w = z * 2      # not a leaf variable\n",
    "y = x * w + 1  # forward pass before retaining gradient is ok\n",
    "p = y.sum()\n",
    "\n",
    "w.retain_grad()\n",
    "\n",
    "p.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отстреливаем себе ноги (НЕ НАДО так делать)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конвертировать Variable в Tensor и обратно:\n",
    "backward pass не проходит через Tensor, даже если он был сконвертирован из другого Variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dp / dx: None\n",
      "dp / dy: [0.75 0.75 0.75 0.75]\n"
     ]
    }
   ],
   "source": [
    "# x out of the computational graph\n",
    "x = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "y = torch.autograd.Variable(x.data * 2, requires_grad=True)   # the bad conversion is here\n",
    "z = 3 * y + 1\n",
    "p = z.mean()\n",
    "p.backward()\n",
    "print('dp / dx:', x.grad)\n",
    "print('dp / dy:', y.grad.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Менять размерность тензоров в Variable, но не обнулять градиенты (`.grad.zero_()` сохраняет размер, `.grad = None` не сохраняет)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz / dx: [1. 2. 3. 4.]\n",
      "dz / dy: [1. 2. 3. 4.]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'zero_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-191-ff60670b866e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#x.grad = None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#z.grad = None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'zero_'"
     ]
    }
   ],
   "source": [
    "x = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "y = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "z = x * y + 1\n",
    "z.sum().backward()\n",
    "print('dz / dx:', x.grad.data.numpy())\n",
    "print('dz / dy:', y.grad.data.numpy())\n",
    "\n",
    "x.grad.zero_()\n",
    "z.grad.zero_()\n",
    "#x.grad = None\n",
    "#z.grad = None\n",
    "\n",
    "x.data = torch.Tensor([1, 2, 3])\n",
    "y.data = torch.Tensor([1, 2, 3])\n",
    "z = x * y + 1\n",
    "z.sum().backward()\n",
    "print('dz / dx:', x.grad.data.numpy())\n",
    "print('dz / dy:', y.grad.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Менять значения Variable после вычисления каких-то других выражений с ним и рассчитывать,\n",
    "что градиент от тех выражений будет учитывать новое значение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d p_sum / dx: [1. 2. 3. 4.]\n",
      "d p_sum / dy: [ 2.  8. 18. 32.]\n"
     ]
    }
   ],
   "source": [
    "x = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "y = torch.autograd.Variable(torch.Tensor([1, 2, 3, 4]), requires_grad=True)\n",
    "z = y ** 2\n",
    "\n",
    "z.data = torch.Tensor([1, 2, 3, 4])  # changing .data before computation matters\n",
    "p = x * z\n",
    "x.data = torch.Tensor([1, 1, 1])     # changing .data after computation doesn't affect gradients\n",
    "\n",
    "p.sum().backward()\n",
    "print('d p_sum / dx:', x.grad.data.numpy())\n",
    "print('d p_sum / dy:', y.grad.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тысячи способов прострелить себе ногу, если использовать механизм автоматического дифференцирования\n",
    "любым другим нетрадиционным образом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наркоманская функция потерь, вынуждающая линейное преобразование переводить точки из многомерного пространства в двумерное на единичную окружность. Для оптимизации использовать градиентный спуск по параметрам преобразования.\n",
    "\n",
    "Линейное преобразование точки $x$ из десятимерного пространства в точку $y$ двумерного пространства с весами преобразования $W$ и $b$:\n",
    "$$y = Wx + b$$\n",
    "\n",
    "Норма в двумерном пространстве – евклидова:\n",
    "$$||y||_2 = \\sqrt{y_1^2 + y_2^2}$$\n",
    "\n",
    "Функция потерь $f_0$ штрафует расстояние от получившейся точки $y$ до единичной окружности:\n",
    "$$f_0(x, W, b) = 0.5 \\cdot \\big| ||y||_2 - 1 \\big| + \\big( ||y||_2 - 1 \\big)^2$$\n",
    "\n",
    "К сожалению, оптимизация функции $f_0$ по $W$ и $b$ может быть проведена аналитически\n",
    "и приводит к тривиальному решению $W = 0$, $b = (1, 0)$.\n",
    "Чтобы избежать такого решения, вводим штраф на близость получившейся точки к вектору $b$, который обращается в 0, если расстояние до вектора $b$ более 1:\n",
    "$$f_1(x, W, b) = \\max\\big(0, \\frac{1}{||y - b||_2} - 1\\big)$$\n",
    "\n",
    "Итоговая функция потерь:\n",
    "$$f(x, W, b) = f_0(x, W, b) + f_1(x, W, b)$$\n",
    "\n",
    "Нужно решить следующую оптимизационную задачу:\n",
    "$$\\frac{1}{N}\\sum\\limits_{i = 1}^N f(x_i, W, b) \\to \\min\\limits_{W, b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2485,  0.2977],\n",
       "        [ 0.7630, -0.9363],\n",
       "        [ 0.5224, -3.2463],\n",
       "        [ 1.2058, -0.5503],\n",
       "        [ 0.8977, -0.4894],\n",
       "        [-0.2024,  0.2627],\n",
       "        [-0.7642,  0.2374],\n",
       "        [-0.6849, -0.0416],\n",
       "        [-0.8653,  1.0926],\n",
       "        [-0.7958,  1.4162]])"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(50, 10)\n",
    "b = torch.randn(2)\n",
    "W = torch.randn(10, 2)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_f_0(z):\n",
    "    distance = torch.sqrt(z[:,0] ** 2 + z[:,1] ** 2)\n",
    "    return 0.5 * torch.abs(distance - 1) + (distance - 1) ** 2\n",
    "\n",
    "def loss_f_1(z, b):\n",
    "    distance = torch.sqrt((z-b)[:,0] ** 2 + (z-b)[:,1] ** 2)\n",
    "    return torch.nn.functional.relu(1/distance - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X, W, b):\n",
    "    z = torch.matmul(X, W) + b\n",
    "    loss_0, loss_1 = loss_f_0(z), loss_f_1(z, b)\n",
    "    loss = loss_0 + loss_1\n",
    "    return torch.mean(loss_0 + loss_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f3d11882278>"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "Y = X.mm(W).add(b)\n",
    "plt.scatter(Y[:, 0], Y[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 10.561440467834473\n",
      "10: 4.3883562088012695\n",
      "20: 0.8428959846496582\n",
      "30: 0.9408329129219055\n",
      "40: 1.0060242414474487\n",
      "50: 1.8850796222686768\n",
      "60: 0.504711925983429\n",
      "70: 0.4383833408355713\n",
      "80: 0.28698092699050903\n",
      "90: 0.25381773710250854\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "X_V = torch.autograd.Variable(torch.Tensor(X), requires_grad=True)\n",
    "W_V = torch.autograd.Variable(torch.Tensor(W), requires_grad=True)\n",
    "B_V = torch.autograd.Variable(torch.Tensor(b), requires_grad=True)\n",
    "optimizer = torch.optim.SGD([W_V, B_V], lr=0.1, momentum=0.9)\n",
    "for i in range(0,100):\n",
    "    optimizer.zero_grad()\n",
    "    loss = f(X_V, W_V, B_V)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 10 == 0:\n",
    "        print('{}: {}'.format(i, f(X_V, W_V, B_V).data))\n",
    "W = W_V.data\n",
    "b = B_V.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5852,  0.0859],\n",
       "        [ 0.3940,  0.1002],\n",
       "        [-0.0755,  0.2460],\n",
       "        [-0.3016, -0.3325],\n",
       "        [ 0.5672,  0.0046],\n",
       "        [ 0.2590,  0.3879],\n",
       "        [ 0.0873, -0.1912],\n",
       "        [ 0.4155, -0.1138],\n",
       "        [-0.2469,  0.2681],\n",
       "        [ 0.0445, -0.3049]], requires_grad=True)"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f3d1185b208>"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "Y = X.mm(W).add(b)\n",
    "plt.scatter(Y[:, 0], Y[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Время писать нейросеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64), (1797,))"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1347, 64), (450, 64))"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
    "y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "X_test = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсказка: нейросети крайне плохо обучаются, если подаваемые им на вход значения велики по модулю.\n",
    "Поэтому перед обучением нейросети каждый признак независимо нормируют\n",
    "(исключение – сверточные нейросети, там нормируют изображение поканально, а не попиксельно, но об этом потом).\n",
    "\n",
    "Можно использовать разные нормировки.\n",
    "Наиболее популярно вычитать среднее и делить на дисперсию (нужно внимательно подходить к этому методу,\n",
    "когда выборочная дисперсия мала или равна нулю, и обрабатывать такие случаи отдельно).\n",
    "Можно также вычитать медиану и делить на интерквартильный размах, масштабировать все данные в отрезок $[-1, 1]$, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно реализовать свою нормировку данных здесь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "pixel_means = torch.mean(X_train, dim=0)\n",
    "pixel_std = torch.std(X_train, dim=0)\n",
    "X_train = (X_train - pixel_means) / pixel_std\n",
    "X_test = (X_test - pixel_means) / pixel_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1347, 64]), torch.Size([450, 64]))"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определяем слои нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def __init__(self):\n",
    "        self.training = True\n",
    "        self.children = []\n",
    "\n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Returns list of parameters of module and its children.\"\"\"\n",
    "        res = []\n",
    "        for submodule in self.children:\n",
    "            res += submodule.parameters()\n",
    "        for param in res:\n",
    "            if not isinstance(param, Variable):\n",
    "                raise Exception('Parameters must be Variables.')\n",
    "        return res\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Sets gradients of all model parameters to zero.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.grad is not None:\n",
    "                p.grad.detach_()   # detachs gradient Variable from the computational graph\n",
    "                p.grad.zero_()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Sets module into train mode (for DropOut, BatchNorm, etc).\"\"\"\n",
    "        self.training = True\n",
    "        for submodule in self.children:\n",
    "            submodule.train()\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"Sets module into evaluation mode.\"\"\"\n",
    "        self.training = False\n",
    "        for submodule in self.children:\n",
    "            submodule.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Module):\n",
    "    def __init__(self, input_units, output_units):\n",
    "        \"\"\"A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = W x + b\n",
    "        \"\"\"\n",
    "        super(Dense, self).__init__()\n",
    "        self.weights = torch.randn(input_units, output_units)*0.00001\n",
    "        self.biases = torch.randn(1, output_units)*0.00001\n",
    "        print(self.weights.shape, self.biases.shape)\n",
    "        self.weights = Variable(self.weights, requires_grad = True)\n",
    "        self.biases = Variable(self.biases, requires_grad = True)\n",
    "        \n",
    "    def parameters(self):\n",
    "        return [self.weights, self.biases]\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"Performs an affine transformation:\n",
    "        f(x) = W x + b\n",
    "        input shape:  [batch, input_units]  (Variable)\n",
    "        output shape: [batch, output units] (Variable)\n",
    "        \"\"\"\n",
    "        # your code here       \n",
    "        output = torch.matmul(input, self.weights)\n",
    "        output = output  + self.biases\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs.\"\"\"\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def parameters(self):\n",
    "        return []  # ReLU has no parameters\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Applies elementwise ReLU to [batch, num_units] Variable matrix.\"\"\"\n",
    "        # your code here\n",
    "        output = torch.nn.functional.relu(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogSoftmax(Module):\n",
    "    def __init__(self):\n",
    "        super(LogSoftmax, self).__init__()\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"Applies softmax to each row and then applies component-wise log.\n",
    "        Input shape:  [batch, num_units] (Variable)\n",
    "        Output shape: [batch, num_units] (Variable)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        output = torch.nn.functional.log_softmax(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetwork(Module):\n",
    "    def __init__(self, input_size, hidden_layers_size, hidden_layers_number, output_size):\n",
    "        super(MyNetwork, self).__init__()\n",
    "\n",
    "        network = []\n",
    "        network.append(Dense(input_size, hidden_layers_size))\n",
    "        network.append(ReLU())\n",
    "        for i in range(hidden_layers_number - 1):\n",
    "            network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "        network.append(Dense(hidden_layers_size, output_size))\n",
    "        network.append(LogSoftmax())\n",
    "\n",
    "        self.children = network\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Applies all layers of neural network to the input.\n",
    "        Input shape:  [batch, num_units] (Variable)\n",
    "        Output shape: [batch, num_units] (Variable)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        output = input\n",
    "        for layer in self.children:\n",
    "            output = layer.forward(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32]) torch.Size([1, 32])\n",
      "torch.Size([32, 10]) torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "network = MyNetwork(X_train.shape[1], 32, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определяем функцию потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy(activations, target):\n",
    "    \"\"\"Returns negative log-likelihood of target under model\n",
    "    represented by activations (log probabilities of classes).\n",
    "    Activations shape: [batch, num_classes] (Variable)\n",
    "    Target shape:      [batch]              (Variable)\n",
    "    Output shape: 1 (scalar, Variable)\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    loss = -torch.sum(activations[np.arange(len(target)), target])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизатор SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDOptimizer:\n",
    "    def __init__(self, parameters, learning_rate):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Make one optimization step for parameters in-place.\n",
    "        Assumes that all parameters are Variable with computed gradient.\n",
    "        \"\"\"\n",
    "        for param in self.parameters:\n",
    "            param.data = param.data - self.learning_rate * param.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(dataset, network, prefix='Test loss:', optimizer=None):\n",
    "    # Change mode for all layers.\n",
    "    if optimizer:\n",
    "        network.train()\n",
    "    else:\n",
    "        network.eval()\n",
    "\n",
    "    batch_size = 10\n",
    "    batchgenerator = torch.utils.data.DataLoader(dataset, batch_size, True)\n",
    "    avg_loss = 0\n",
    "    for i, (batch_data, batch_target) in enumerate(batchgenerator):\n",
    "        #print(\"batch_data: \", batch_data)\n",
    "        #print(\"batch_target: \", batch_target)\n",
    "\n",
    "        batch_output = network.forward(Variable(batch_data))\n",
    "        batch_loss = crossentropy(batch_output, Variable(batch_target))\n",
    "        batch_loss.backward()\n",
    "        batch_loss = batch_loss.data.numpy()\n",
    "        avg_loss += (batch_loss - avg_loss) / (i + 1)\n",
    "        if optimizer:\n",
    "            optimizer.step()\n",
    "            network.zero_grad()\n",
    "    print(prefix, avg_loss, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32]) torch.Size([1, 32])\n",
      "torch.Size([32, 10]) torch.Size([1, 10])\n",
      "Train loss: 23.00744956687645\n",
      "Test loss: 23.052502822875972\n",
      "Train loss: 21.108521518000845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxim/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 15.765296300252277\n",
      "Train loss: 10.013544907393275\n",
      "Test loss: 5.031533193588256\n",
      "Train loss: 6.610915092185698\n",
      "Test loss: 2.5187732802497016\n",
      "Train loss: 2.5746363162994377\n",
      "Test loss: 1.8760148578219942\n",
      "Train loss: 2.5320748117234957\n",
      "Test loss: 1.2698424127366805\n",
      "Train loss: 1.2021667180237945\n",
      "Test loss: 1.08026032977634\n",
      "Train loss: 1.0079155074225532\n",
      "Test loss: 0.9394749641418456\n",
      "Train loss: 0.8359256885669842\n",
      "Test loss: 0.7756443606482611\n",
      "Train loss: 0.7509635183546275\n",
      "Test loss: 0.6718398465050591\n",
      "Train loss: 0.5980838722652858\n",
      "Test loss: 0.7267479843563504\n",
      "Train loss: 0.679471782401756\n",
      "Test loss: 0.5586263868543836\n",
      "Train loss: 0.4605495029025606\n",
      "Test loss: 0.5505016803741455\n",
      "Train loss: 0.48743937986868374\n",
      "Test loss: 0.479070912467109\n",
      "Train loss: 0.3959339706986038\n",
      "Test loss: 0.4546795050303142\n",
      "Train loss: 0.3541888908103661\n",
      "Test loss: 0.3975416395399306\n",
      "Train loss: 0.3686563385857475\n",
      "Test loss: 0.4531346956888835\n",
      "Train loss: 0.5088110658857562\n",
      "Test loss: 0.31343410280015743\n",
      "Train loss: 0.2683816892129404\n",
      "Test loss: 0.3354389508565267\n",
      "Train loss: 0.23029586297494384\n",
      "Test loss: 0.26714082294040253\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "network = MyNetwork(X_train.shape[1], 32, 1, 10)\n",
    "sgd = SGDOptimizer(network.parameters(), 0.01)\n",
    "\n",
    "num_epochs = 20\n",
    "for i in range(num_epochs):\n",
    "    run_epoch(train_dataset, network, 'Train loss:', sgd)\n",
    "    run_epoch(test_dataset, network, 'Test loss:',  None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Больше оптимизаторов Б-гу Оптимизации!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentumOptimizer:\n",
    "    def __init__(self, parameters, learning_rate=0.01, momentum=0.9):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.params_prev_step = parameters\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Make one optimization step for parameters in-place.\n",
    "        Assumes that all parameters are Variable with computed gradient.\n",
    "        http://ruder.io/optimizing-gradient-descent/index.html#stochasticgradientdescent\n",
    "        \"\"\"\n",
    "        for param, param_prev in zip(self.parameters, self.params_prev_step):\n",
    "            param.data = param.data + self.momentum*param.data - self.learning_rate * param.grad.data - self.momentum*param_prev.data\n",
    "        self.params_prev_step = self.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSPropOptimizer:\n",
    "    def __init__(self, parameters, learning_rate=0.01, beta=0.9, eps=1e-8):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "        # your code here\n",
    "        self.v = list(parameters)\n",
    "        for i in range(len(self.v)):\n",
    "            self.v[i] = self.v[i]*0.0\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Make one optimization step for parameters in-place.\n",
    "        Assumes that all parameters are Variable with computed gradient.\n",
    "        http://ruder.io/optimizing-gradient-descent/index.html#stochasticgradientdescent\n",
    "        \"\"\"\n",
    "        for param, v in zip(self.parameters, self.v):\n",
    "            grad = param.grad.data\n",
    "            v.data = self.beta*v.data + (1-self.beta)*grad**2\n",
    "            param.data -= self.learning_rate*grad/(v.data+self.eps)**(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, parameters, learning_rate=0.01, beta=0.999, gamma=0.9, eps=1e-8, align=False):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = learning_rate\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma,\n",
    "        self.eps = eps\n",
    "        self.step_state = 0\n",
    "        self.correlation_v = 1.0\n",
    "        self.correlation_mu = 1.0\n",
    "        self.v = list(parameters)\n",
    "        self.mu = list(parameters)\n",
    "        self.align = align\n",
    "        for i, param in enumerate(parameters):\n",
    "            self.v[i] = torch.zeros_like(param.data)\n",
    "            self.mu[i] = torch.zeros_like(param.data)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Make one optimization step for parameters in-place.\n",
    "        Assumes that all parameters are Variable with computed gradient.\n",
    "        http://ruder.io/optimizing-gradient-descent/index.html#stochasticgradientdescent\n",
    "        \"\"\"\n",
    "        self.step_state += 1\n",
    "        for param, v, mu in zip(self.parameters, self.v, self.mu):\n",
    "            grad = param.grad.data\n",
    "            mu.mul_(self.gamma[0]).add_(1-self.gamma[0], grad)\n",
    "            v.mul_(self.beta).addcmul_(1-self.beta, grad, grad)\n",
    "\n",
    "            if self.align:\n",
    "                self.correlation_v = 1 - self.beta**self.step_state\n",
    "                self.correlation_mu = 1 - self.gamma**self.step_state\n",
    "\n",
    "            demon = v.sqrt().add_(self.eps)\n",
    "            step_size = -self.alpha * np.sqrt(self.correlation_v) / self.correlation_mu\n",
    "            param.data.addcdiv_(step_size, mu, demon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32]) torch.Size([1, 32])\n",
      "torch.Size([32, 10]) torch.Size([1, 10])\n",
      "Train loss: 11.550270118536776\n",
      "Test loss: 5.362275097105238\n",
      "Train loss: 4.615763681023208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxim/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.3864012877146408\n",
      "Train loss: 2.2009906706986597\n",
      "Test loss: 2.0507345504230923\n",
      "Train loss: 1.9690176824728651\n",
      "Test loss: 1.6828252573808034\n",
      "Train loss: 1.5922296852977187\n",
      "Test loss: 1.152261851893531\n",
      "Train loss: 1.1818954988762185\n",
      "Test loss: 1.1787352045377097\n",
      "Train loss: 1.1195643876437784\n",
      "Test loss: 1.055274424288008\n",
      "Train loss: 0.9647450057996645\n",
      "Test loss: 0.9871711035569509\n",
      "Train loss: 0.9347368639928324\n",
      "Test loss: 0.9776430971092647\n",
      "Train loss: 1.0075062125921252\n",
      "Test loss: 0.8821025712622538\n",
      "Train loss: 0.9343150010815373\n",
      "Test loss: 0.8547069046232435\n",
      "Train loss: 0.9448930954491642\n",
      "Test loss: 0.7442688842614493\n",
      "Train loss: 0.6313598758644531\n",
      "Test loss: 0.6536732892195383\n",
      "Train loss: 0.730219905464738\n",
      "Test loss: 0.7545821925004321\n",
      "Train loss: 0.7193002515644933\n",
      "Test loss: 0.5574320859379237\n",
      "Train loss: 0.5024581739609992\n",
      "Test loss: 0.6003503017955355\n",
      "Train loss: 0.5456442618811572\n",
      "Test loss: 0.5918080064985486\n",
      "Train loss: 0.5485679304917102\n",
      "Test loss: 0.45088643514447735\n",
      "Train loss: 0.34593789400877784\n",
      "Test loss: 0.48137501503030455\n",
      "Train loss: 0.49977529965065165\n",
      "Test loss: 0.4106776823600133\n"
     ]
    }
   ],
   "source": [
    "network = MyNetwork(X_train.shape[1], 32, 1, 10)\n",
    "optim = AdamOptimizer(network.parameters(), learning_rate=0.01)\n",
    "\n",
    "num_epochs = 20\n",
    "for i in range(num_epochs):\n",
    "    run_epoch(train_dataset, network, 'Train loss:', optim)\n",
    "    run_epoch(test_dataset, network, 'Test loss:',  None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32]) torch.Size([1, 32])\n",
      "torch.Size([32, 10]) torch.Size([1, 10])\n",
      "Train loss: 23.007320093225555\n",
      "Test loss: 23.045870378282334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxim/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 21.07345387494123\n",
      "Test loss: 16.017906104193795\n",
      "Train loss: 7.694070789549083\n",
      "Test loss: 4.509342638651528\n",
      "Train loss: 6.165715687363231\n",
      "Test loss: 2.1390827920701763\n",
      "Train loss: 2.017830583784314\n",
      "Test loss: 1.3935632811652292\n",
      "Train loss: 1.3598693918298796\n",
      "Test loss: 1.0985609160529246\n",
      "Train loss: 1.0622499201032851\n",
      "Test loss: 1.0896295653449166\n",
      "Train loss: 1.5846584814566151\n",
      "Test loss: 1.0422496848636202\n",
      "Train loss: 0.943475428333989\n",
      "Test loss: 0.7767877790662979\n",
      "Train loss: 0.6805328192534272\n",
      "Test loss: 0.6827105257246229\n",
      "Train loss: 0.6803910590984206\n",
      "Test loss: 0.5683909098307292\n",
      "Train loss: 0.4746061024842437\n",
      "Test loss: 0.5073436154259574\n",
      "Train loss: 0.46955177695662886\n",
      "Test loss: 0.45588010152180997\n",
      "Train loss: 0.4244313946476688\n",
      "Test loss: 0.4234677791595459\n",
      "Train loss: 0.33412643538580994\n",
      "Test loss: 0.43042678833007825\n",
      "Train loss: 0.3495713357572204\n",
      "Test loss: 0.2996117062038845\n",
      "Train loss: 0.24544976375721123\n",
      "Test loss: 0.29370762507120773\n",
      "Train loss: 0.22602417557327842\n",
      "Test loss: 0.2627123408847385\n",
      "Train loss: 0.23751195978235304\n",
      "Test loss: 0.23229960335625546\n",
      "Train loss: 0.19116700843528467\n",
      "Test loss: 0.21855591668023\n"
     ]
    }
   ],
   "source": [
    "network = MyNetwork(X_train.shape[1], 32, 1, 10)\n",
    "optim = SGDMomentumOptimizer(network.parameters(), 0.01)\n",
    "\n",
    "num_epochs = 20\n",
    "for i in range(num_epochs):\n",
    "    run_epoch(train_dataset, network, 'Train loss:', optim)\n",
    "    run_epoch(test_dataset, network, 'Test loss:',  None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32]) torch.Size([1, 32])\n",
      "torch.Size([32, 10]) torch.Size([1, 10])\n",
      "Train loss: 9.556692511064032\n",
      "Test loss: 3.8225896120071403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxim/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.346870297414286\n",
      "Test loss: 2.539155358407233\n",
      "Train loss: 1.5078265450618886\n",
      "Test loss: 1.4906566636429897\n",
      "Train loss: 1.1303135281911612\n",
      "Test loss: 2.4093160609404247\n",
      "Train loss: 0.8385739331720051\n",
      "Test loss: 1.8621725176771484\n",
      "Train loss: 0.772095270137544\n",
      "Test loss: 1.3677793297502732\n",
      "Train loss: 0.6052298207111934\n",
      "Test loss: 1.3916612995167572\n",
      "Train loss: 0.5064556025796467\n",
      "Test loss: 1.3269052647882038\n",
      "Train loss: 0.4073336012351017\n",
      "Test loss: 1.2806157026853828\n",
      "Train loss: 0.3896139399320991\n",
      "Test loss: 0.8844823828587931\n",
      "Train loss: 0.3874978173938062\n",
      "Test loss: 0.8744230279492006\n",
      "Train loss: 0.28329865074440563\n",
      "Test loss: 0.8986197074254354\n",
      "Train loss: 0.24777853150886514\n",
      "Test loss: 0.685988880776697\n",
      "Train loss: 0.24760448968520873\n",
      "Test loss: 0.7980573491917717\n",
      "Train loss: 0.16714099364148252\n",
      "Test loss: 1.151031160934104\n",
      "Train loss: 0.12342461645603177\n",
      "Test loss: 0.7677819591429499\n",
      "Train loss: 0.2168033082314112\n",
      "Test loss: 0.5954013261530136\n",
      "Train loss: 0.13134067260004853\n",
      "Test loss: 0.6577982895904119\n",
      "Train loss: 0.1463552870684199\n",
      "Test loss: 0.5090266660476723\n",
      "Train loss: 0.09423000293059482\n",
      "Test loss: 0.6917285718851618\n"
     ]
    }
   ],
   "source": [
    "network = MyNetwork(X_train.shape[1], 32, 1, 10)\n",
    "optim = RMSPropOptimizer(network.parameters())\n",
    "\n",
    "num_epochs = 20\n",
    "for i in range(num_epochs):\n",
    "    run_epoch(train_dataset, network, 'Train loss:', optim)\n",
    "    run_epoch(test_dataset, network, 'Test loss:',  None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперименты с DropOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот пункт обязателен к выполнению.\n",
    "Для того, чтобы получить бонусный балл за этот пункт, нужно эффективно реализовать DropOut:\n",
    "не вычислять активации выкинутых нейронов, прежде чем их обнулить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseWithDropOut(Module):\n",
    "    def __init__(self, input_units, output_units, dropout_rate, nonlinearity, mode):\n",
    "        \"\"\"A dense layer is a layer which performs a learned\n",
    "        affine transformation and applies dropout:\n",
    "        m ~ Bernoulli(1 - p, size=output_units)\n",
    "        f(x) = g(W x + b) o m\n",
    "        \"\"\"\n",
    "        super(DenseWithDropOut, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.nonlinearity = nonlinearity\n",
    "        # initialize weights with small random numbers from normal distribution\n",
    "        self.input_u = input_units\n",
    "        self.output_u = output_units\n",
    "        self.mode = mode\n",
    "        self.weights = torch.randn(input_units, output_units)*0.00001  # your code here\\n\",\n",
    "        self.biases = torch.randn(1, output_units)*0.00001   # your code here\\n\",\n",
    "        self.weights = Variable(self.weights, requires_grad = True)\n",
    "        self.biases = Variable(self.biases, requires_grad = True)\n",
    "    def parameters(self):\n",
    "        return [self.weights, self.biases]\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"Performs an affine transformation with dropout.\n",
    "        In training mode:\n",
    "        m ~ Bernoulli(1 - p, size=output_units)\n",
    "        f(x) = g(W x + b) o m\n",
    "        In evaluation mode:\n",
    "        f(x) = g(W x + b) (1 - p)\n",
    "        input shape:  [batch, input_units]  (Variable)\n",
    "        output shape: [batch, output units] (Variable)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        if self.training:\n",
    "            if self.mode != 0:\n",
    "                input, in_indx = input[0], input[1]\n",
    "                weights = self.weights.index_select(0, in_indx)\n",
    "            else:\n",
    "                weights = self.weights\n",
    "            batch = input.shape[0]\n",
    "            n = self.output_u\n",
    "            output = Variable(torch.zeros(batch, n), requires_grad = False)\n",
    "            p = 1-self.dropout_rate\n",
    "            selected_indexes = Variable(torch.from_numpy(np.random.choice(n, np.random.binomial(n-1, p)+1, replace=False)))\n",
    "            selected_weights = weights.index_select(1, selected_indexes)\n",
    "            selected_biases = self.biases.index_select(1, selected_indexes)\n",
    "            if self.mode==2:\n",
    "                output[:, selected_indexes] = self.nonlinearity(torch.matmul(input, selected_weights) + selected_biases)\n",
    "            else:\n",
    "                output = (self.nonlinearity(torch.matmul(input, selected_weights) + selected_biases), selected_indexes)\n",
    "        else:\n",
    "            output = self.nonlinearity(torch.matmul(input, self.weights) + self.biases)*(1-self.dropout_rate)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем, верно ли, что полносвязная сеть с dropout работает быстрее, чем обычная полносвязная сеть, поскольку на каждом проходе вычисляются произведения матриц меньшего размера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 2000]) torch.Size([1, 2000])\n",
      "torch.Size([2000, 2000]) torch.Size([1, 2000])\n",
      "torch.Size([2000, 2000]) torch.Size([1, 2000])\n",
      "torch.Size([2000, 1]) torch.Size([1, 1])\n",
      "345 ms ± 20 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "5.12 s ± 223 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "width = 2000\n",
    "network1 = [\n",
    "    DenseWithDropOut(width, width, 0.9, lambda x: ReLU().forward(x), mode = 0),\n",
    "    DenseWithDropOut(width, width, 0.9, lambda x: ReLU().forward(x), mode = 1),\n",
    "    DenseWithDropOut(width, width, 0.9, lambda x: ReLU().forward(x), mode = 1),\n",
    "    DenseWithDropOut(width, 1, 0, lambda x: x, mode = 2)\n",
    "]\n",
    "network2 = [\n",
    "    Dense(width, width),\n",
    "    ReLU(),\n",
    "    Dense(width, width),\n",
    "    ReLU(),\n",
    "    Dense(width, width),\n",
    "    ReLU(),\n",
    "    Dense(width, 1)\n",
    "]\n",
    "X = torch.randn(10000, width)\n",
    "\n",
    "# check whether DenseWithDropOut works faster than Dense\n",
    "def test_network(network):\n",
    "    x = Variable(X)\n",
    "    for layer in network:\n",
    "        x = layer.forward(x)\n",
    "    x.mean().backward()\n",
    "    for layer in network:\n",
    "        x = layer.zero_grad()\n",
    "\n",
    "test_network(network1)\n",
    "%timeit test_network(network1)\n",
    "%timeit test_network(network2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для более узких слоев, меньших dropout rate и меньших размеров батча увеличение производительности не настолько существенно или может вообще отсутствовать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
