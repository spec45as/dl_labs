{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наркоманская функция потерь, вынуждающая линейное преобразование переводить точки из многомерного пространства в двумерное на единичную окружность. Для оптимизации использовать градиентный спуск по параметрам преобразования.\n",
    "\n",
    "Линейное преобразование точки $x$ из десятимерного пространства в точку $y$ двумерного пространства с весами преобразования $W$ и $b$:\n",
    "$$y = Wx + b$$\n",
    "\n",
    "Норма в двумерном пространстве – евклидова:\n",
    "$$||y||_2 = \\sqrt{y_1^2 + y_2^2}$$\n",
    "\n",
    "Функция потерь $f_0$ штрафует расстояние от получившейся точки $y$ до единичной окружности:\n",
    "$$f_0(x, W, b) = 0.5 \\cdot \\big| ||y||_2 - 1 \\big| + \\big( ||y||_2 - 1 \\big)^2$$\n",
    "\n",
    "К сожалению, оптимизация функции $f_0$ по $W$ и $b$ может быть проведена аналитически\n",
    "и приводит к тривиальному решению $W = 0$, $b = (1, 0)$.\n",
    "Чтобы избежать такого решения, вводим штраф на близость получившейся точки к вектору $b$, который обращается в 0, если расстояние до вектора $b$ более 1:\n",
    "$$f_1(x, W, b) = \\max\\big(0, \\frac{1}{||y - b||_2} - 1\\big)$$\n",
    "\n",
    "Итоговая функция потерь:\n",
    "$$f(x, W, b) = f_0(x, W, b) + f_1(x, W, b)$$\n",
    "\n",
    "Нужно решить следующую оптимизационную задачу:\n",
    "$$\\frac{1}{N}\\sum\\limits_{i = 1}^N f(x_i, W, b) \\to \\min\\limits_{W, b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_f_0(z):\n",
    "    distance = torch.sqrt(z[:,0] ** 2 + z[:,1] ** 2)\n",
    "    return 0.5 * torch.abs(distance - 1) + (distance - 1) ** 2\n",
    "\n",
    "def loss_f_1(z, b):\n",
    "    distance = torch.sqrt((z-b)[:,0] ** 2 + (z-b)[:,1] ** 2)\n",
    "    return torch.nn.functional.relu(1/distance - 1)\n",
    "\n",
    "def f(X, W, b):\n",
    "    z = torch.matmul(X, W) + b\n",
    "    loss_0, loss_1 = loss_f_0(z), loss_f_1(z, b)\n",
    "    loss = loss_0 + loss_1\n",
    "    return torch.mean(loss_0 + loss_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(50, 10)\n",
    "b = torch.randn(2)\n",
    "W = torch.randn(10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.7380)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f4445000898>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Результат на случайных весах\n",
    "print(f(X, W, b))\n",
    "plt.figure(figsize=(5, 5))\n",
    "Y = X.mm(W).add(b)\n",
    "plt.scatter(Y[:, 0], Y[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 12.737998962402344\n",
      "10: 3.4813077449798584\n",
      "20: 36.52226257324219\n",
      "30: 9.999650001525879\n",
      "40: 1.9745256900787354\n",
      "50: 1.1720629930496216\n",
      "60: 0.875336766242981\n",
      "70: 0.9066555500030518\n",
      "80: 0.6306901574134827\n",
      "90: 0.45122891664505005\n",
      "100: 0.3626193106174469\n",
      "110: 0.31948322057724\n",
      "120: 0.2642340660095215\n",
      "130: 0.23902525007724762\n",
      "140: 0.23475965857505798\n",
      "150: 0.23339225351810455\n",
      "160: 0.2321547269821167\n",
      "170: 0.2322722226381302\n",
      "180: 0.2330823838710785\n",
      "190: 0.23159746825695038\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "x_tensor = torch.tensor(X, requires_grad=True)\n",
    "w_tensor = torch.tensor(W, requires_grad=True)\n",
    "b_tensor = torch.tensor(b, requires_grad=True)\n",
    "optimizer = torch.optim.SGD([w_tensor, b_tensor], lr=0.05, momentum=0.9)\n",
    "for i in range(0,200):\n",
    "    if i % 10 == 0:\n",
    "        print('{}: {}'.format(i, f(x_tensor, w_tensor, b_tensor).data))\n",
    "    optimizer.zero_grad()\n",
    "    loss = f(x_tensor, w_tensor, b_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f4444eb2898>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Результат после обучения\n",
    "plt.figure(figsize=(5, 5))\n",
    "Y = X.mm(w_tensor.data).add(b_tensor.data)\n",
    "plt.scatter(Y[:, 0], Y[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Время писать нейросеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n",
      "(1347, 64) (450, 64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсказка: нейросети крайне плохо обучаются, если подаваемые им на вход значения велики по модулю.\n",
    "Поэтому перед обучением нейросети каждый признак независимо нормируют\n",
    "(исключение – сверточные нейросети, там нормируют изображение поканально, а не попиксельно, но об этом потом).\n",
    "\n",
    "Можно использовать разные нормировки.\n",
    "Наиболее популярно вычитать среднее и делить на дисперсию (нужно внимательно подходить к этому методу,\n",
    "когда выборочная дисперсия мала или равна нулю, и обрабатывать такие случаи отдельно).\n",
    "Можно также вычитать медиану и делить на интерквартильный размах, масштабировать все данные в отрезок $[-1, 1]$, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно реализовать свою нормировку данных здесь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n",
      "(1347, 64) (450, 64)\n"
     ]
    }
   ],
   "source": [
    "# нормировка делением на дисперсию\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "X_train = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
    "y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "X_test = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "\n",
    "pixel_means = torch.mean(X_train, dim=0)\n",
    "#print(\"pixel_std: {}\".format(pixel_means))\n",
    "pixel_std = torch.std(X_train, dim=0)\n",
    "for i in range(len(pixel_std)):\n",
    "    if pixel_std[i] == 0:\n",
    "        pixel_std[i] = 1\n",
    "        \n",
    "#print(\"pixel_std: {}\".format(pixel_std))\n",
    "X_train = (X_train - pixel_means) / pixel_std\n",
    "X_test = (X_test - pixel_means) / pixel_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n",
      "(1347, 64) (450, 64)\n"
     ]
    }
   ],
   "source": [
    "# Из коробки с помощью интерквартильного размаха\n",
    "#https://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "quantile_transformer = QuantileTransformer(random_state=0)\n",
    "X_train = quantile_transformer.fit_transform(X_train)\n",
    "X_test = quantile_transformer.transform(X_test)\n",
    "\n",
    "X_train = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
    "y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "X_test = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определяем слои нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def __init__(self):\n",
    "        self.training = True\n",
    "        self.children = []\n",
    "\n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Returns list of parameters of module and its children.\"\"\"\n",
    "        res = []\n",
    "        for submodule in self.children:\n",
    "            res += submodule.parameters()\n",
    "        for param in res:\n",
    "            if not isinstance(param, torch.Tensor):\n",
    "                raise Exception('Parameters must be Tensors.')\n",
    "        return res\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Sets gradients of all model parameters to zero.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.grad is not None:\n",
    "                p.grad.detach_()   # detachs gradient Tensor from the computational graph\n",
    "                p.grad.zero_()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Sets module into train mode (for DropOut, BatchNorm, etc).\"\"\"\n",
    "        self.training = True\n",
    "        for submodule in self.children:\n",
    "            submodule.train()\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"Sets module into evaluation mode.\"\"\"\n",
    "        self.training = False\n",
    "        for submodule in self.children:\n",
    "            submodule.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Module):\n",
    "    def __init__(self, input_units, output_units):\n",
    "        \"\"\"A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = W x + b\n",
    "        \"\"\"\n",
    "        super(Dense, self).__init__()\n",
    "        # initialize weights with small random numbers from normal distribution\n",
    "        self.weights = torch.empty(input_units, output_units, dtype=torch.float).normal_(mean=0, std=1)  # your code here\n",
    "        self.weights = torch.tensor(self.weights, requires_grad=True)\n",
    "        \n",
    "        self.biases = torch.empty(output_units, dtype=torch.float).normal_(mean=0, std=1)   # your code here\n",
    "        self.biases = torch.tensor(self.biases, requires_grad=True)\n",
    "        \n",
    "    def parameters(self):\n",
    "        return [self.weights, self.biases]\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"Performs an affine transformation:\n",
    "        f(x) = W x + b\n",
    "        input shape:  [batch, input_units]  (Tensor)\n",
    "        output shape: [batch, output units] (Tensor)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        output = input.mm(self.weights).add(self.biases)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs.\"\"\"\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def parameters(self):\n",
    "        return []  # ReLU has no parameters\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Applies elementwise ReLU to [batch, num_units] Tensor matrix.\"\"\"\n",
    "        # your code here\n",
    "        output = torch.max(input, torch.zeros_like(input))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class LogSoftmax(Module):\n",
    "    def __init__(self):\n",
    "        super(LogSoftmax, self).__init__()\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"Applies softmax to each row and then applies component-wise log.\n",
    "        Input shape:  [batch, num_units] (Tensor)\n",
    "        Output shape: [batch, num_units] (Tensor)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        #output = input.data.numpy()\n",
    "        #output = output - np.max(output, axis=1)[:, np.newaxis] \n",
    "        #output = np.log(np.exp(output) / np.sum(np.exp(output), axis=1)[:, np.newaxis])\n",
    "        #return output\n",
    "        \n",
    "        # ^Не получилось сделать чисто на torch переменных функцию, только np и в ней не работали бы фичи из торча\n",
    "        # решил выпилить и вставить готовую\n",
    "        output = F.log_softmax(input, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetwork(Module):\n",
    "    def __init__(self, input_size, hidden_layers_size, hidden_layers_number, output_size):\n",
    "        super(MyNetwork, self).__init__()\n",
    "\n",
    "        network = []\n",
    "        network.append(Dense(input_size, hidden_layers_size))\n",
    "        network.append(ReLU())\n",
    "        for i in range(hidden_layers_number - 1):\n",
    "            network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "        network.append(Dense(hidden_layers_size, output_size))\n",
    "        network.append(LogSoftmax())\n",
    "\n",
    "        self.children = network\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Applies all layers of neural network to the input.\n",
    "        Input shape:  [batch, num_units] (Tensor)\n",
    "        Output shape: [batch, num_units] (Tensor)\n",
    "        \"\"\"\n",
    "        output = input\n",
    "        # your code here\n",
    "        for layer in self.children:\n",
    "            output = layer.forward(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MyNetwork(X_train.shape[1], 32, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определяем функцию потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy(activations, target):\n",
    "    return -1*torch.mean(activations*torch.Tensor(torch.eye(activations.shape[1])[target.data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизатор SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDOptimizer:\n",
    "    def __init__(self, parameters, learning_rate):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Make one optimization step for parameters in-place.\n",
    "        Assumes that all parameters are Variable with computed gradient.\n",
    "        \"\"\"\n",
    "        for param in self.parameters:\n",
    "            param.data -= self.learning_rate * param.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(dataset, network, prefix='Test loss:', optimizer=None):\n",
    "    # Change mode for all layers.\n",
    "    if optimizer:\n",
    "        network.train()\n",
    "    else:\n",
    "        network.eval()\n",
    "\n",
    "    batch_size = 100\n",
    "    batchgenerator = torch.utils.data.DataLoader(dataset, batch_size, True)\n",
    "\n",
    "    avg_loss = 0\n",
    "    for i, (batch_data, batch_target) in enumerate(batchgenerator):\n",
    "        batch_output = network.forward(torch.tensor(batch_data))\n",
    "        batch_loss = crossentropy(batch_output, torch.tensor(batch_target))\n",
    "        batch_loss.backward()\n",
    "        batch_loss = batch_loss.data.numpy()\n",
    "        avg_loss += (batch_loss - avg_loss) / (i + 1)\n",
    "        if optimizer:\n",
    "            optimizer.step()\n",
    "            network.zero_grad()\n",
    "    print(prefix, avg_loss, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.3224943748542242\n",
      "Test loss: 0.7270618915557862\n",
      "Train loss: 0.5076459433351245\n",
      "Test loss: 0.36908066272735596\n",
      "Train loss: 0.301526950938361\n",
      "Test loss: 0.2610673666000366\n",
      "Train loss: 0.21695233987910406\n",
      "Test loss: 0.20664595067501068\n",
      "Train loss: 0.17594877311161589\n",
      "Test loss: 0.1719949334859848\n",
      "Train loss: 0.1476305526282106\n",
      "Test loss: 0.1453218162059784\n",
      "Train loss: 0.12818284811718123\n",
      "Test loss: 0.12881805151700973\n",
      "Train loss: 0.11138748058250972\n",
      "Test loss: 0.10774884819984436\n",
      "Train loss: 0.10098956472107343\n",
      "Test loss: 0.10085771232843399\n",
      "Train loss: 0.09135710022279195\n",
      "Test loss: 0.09121232479810715\n",
      "Train loss: 0.08511429013950483\n",
      "Test loss: 0.08060893416404724\n",
      "Train loss: 0.07742203399538994\n",
      "Test loss: 0.07724456787109375\n",
      "Train loss: 0.07352124713361265\n",
      "Test loss: 0.07183605208992958\n",
      "Train loss: 0.0661870211895023\n",
      "Test loss: 0.06559795588254928\n",
      "Train loss: 0.06419315401996883\n",
      "Test loss: 0.05996919944882393\n",
      "Train loss: 0.06047454449747291\n",
      "Test loss: 0.05795879513025284\n",
      "Train loss: 0.05758575429873808\n",
      "Test loss: 0.06170530915260315\n",
      "Train loss: 0.05456079808729036\n",
      "Test loss: 0.06135432943701744\n",
      "Train loss: 0.0538611566381795\n",
      "Test loss: 0.053023967891931534\n",
      "Train loss: 0.05073075395609651\n",
      "Test loss: 0.05136542245745659\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "network = MyNetwork(X_train.shape[1], 32, 1, 10)\n",
    "sgd = SGDOptimizer(network.parameters(), 0.5)\n",
    "\n",
    "num_epochs = 20\n",
    "for i in range(num_epochs):\n",
    "    run_epoch(train_dataset, network, 'Train loss:', sgd)\n",
    "    run_epoch(test_dataset, network, 'Test loss:',  None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Больше оптимизаторов Б-гу Оптимизации!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentumOptimizer:\n",
    "    def __init__(self, parameters, learning_rate=0.01, momentum=0.9):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        # your code here\n",
    "        self.rates = [torch.zeros_like(x) for x in self.parameters]\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Make one optimization step for parameters in-place.\n",
    "        Assumes that all parameters are Variable with computed gradient.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        for ix in range(len(self.parameters)):\n",
    "            self.rates[ix] = - 1 * self.learning_rate * self.parameters[ix].grad.numpy() + self.momentum * self.rates[ix] \n",
    "            self.parameters[ix].data = self.parameters[ix].data + self.rates[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSPropOptimizer:\n",
    "    def __init__(self, parameters, learning_rate=0.01, beta=0.9, eps=1e-8):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "        # your code here\n",
    "        self.running_averages = [torch.zeros_like(x) for x in self.parameters]\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Make one optimization step for parameters in-place.\n",
    "        Assumes that all parameters are Variable with computed gradient.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        for ix in range(len(self.parameters)):\n",
    "            self.running_averages[ix] = self.beta * self.running_averages[ix] + (1-self.beta) * (self.parameters[ix].grad.data**2)\n",
    "            self.parameters[ix].data = self.parameters[ix].data - self.learning_rate/torch.sqrt(self.eps+self.running_averages[ix]) * self.parameters[ix].grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, parameters, learning_rate=0.01, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        # your code here\n",
    "        \n",
    "        self.m = [torch.zeros_like(x) for x in self.parameters]\n",
    "        self.v = [torch.zeros_like(x) for x in self.parameters]\n",
    "        \n",
    "    def step(self):\n",
    "        \"\"\"Make one optimization step for parameters in-place.\n",
    "        Assumes that all parameters are Variable with computed gradient.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "                    \n",
    "        for ix in range(len(self.parameters)):\n",
    "            self.m[ix] = (self.beta1 * self.m[ix] + (1-self.beta1)*self.parameters[ix].grad.data)\n",
    "            self.v[ix] = (self.beta2 * self.v[ix] + (1-self.beta2)*self.parameters[ix].grad.data**2)\n",
    "            \n",
    "            m, v = self.m[ix] / (1-self.beta1), self.v[ix] / (1-self.beta2)\n",
    "            self.parameters[ix].data = self.parameters[ix].data - self.learning_rate * m / (torch.sqrt(v) + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.8861593838248935\n",
      "Test loss: 0.20479665398597718\n",
      "Train loss: 0.17421326573405946\n",
      "Test loss: 0.14796899855136872\n",
      "Train loss: 0.12490521584238323\n",
      "Test loss: 0.10206914991140366\n",
      "Train loss: 0.0837956563170467\n",
      "Test loss: 0.07655073627829552\n",
      "Train loss: 0.06571268662810326\n",
      "Test loss: 0.05869651436805725\n",
      "Train loss: 0.052306749458823895\n",
      "Test loss: 0.04741026982665062\n",
      "Train loss: 0.046197399363986084\n",
      "Test loss: 0.04226075559854507\n",
      "Train loss: 0.039615299486156025\n",
      "Test loss: 0.03747780472040176\n",
      "Train loss: 0.03563952765294484\n",
      "Test loss: 0.03190916888415814\n",
      "Train loss: 0.03253141351576362\n",
      "Test loss: 0.028747232630848885\n",
      "Train loss: 0.029644607965435298\n",
      "Test loss: 0.028675852715969084\n",
      "Train loss: 0.02776321349665523\n",
      "Test loss: 0.026266885921359062\n",
      "Train loss: 0.02516721681292568\n",
      "Test loss: 0.02386121600866318\n",
      "Train loss: 0.024048338777252605\n",
      "Test loss: 0.021739119291305543\n",
      "Train loss: 0.022381451513086046\n",
      "Test loss: 0.019869045168161393\n",
      "Train loss: 0.022064340699996264\n",
      "Test loss: 0.020457010716199875\n",
      "Train loss: 0.020877285594386712\n",
      "Test loss: 0.017987142875790597\n",
      "Train loss: 0.018818164709955454\n",
      "Test loss: 0.017115558683872222\n",
      "Train loss: 0.019107332519654716\n",
      "Test loss: 0.014783755131065845\n",
      "Train loss: 0.017123012071741477\n",
      "Test loss: 0.01483362279832363\n"
     ]
    }
   ],
   "source": [
    "network = MyNetwork(X_train.shape[1], 32, 1, 10)\n",
    "optim = SGDMomentumOptimizer(network.parameters(), 0.5)\n",
    "\n",
    "num_epochs = 20\n",
    "for i in range(num_epochs):\n",
    "    run_epoch(train_dataset, network, 'Train loss:', optim)\n",
    "    run_epoch(test_dataset, network, 'Test loss:',  None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.9974486018930161\n",
      "Test loss: 0.48690699934959414\n",
      "Train loss: 0.3242223784327507\n",
      "Test loss: 0.2738447844982147\n",
      "Train loss: 0.2037073490875108\n",
      "Test loss: 0.18880587220191955\n",
      "Train loss: 0.13974261709621974\n",
      "Test loss: 0.13339255899190902\n",
      "Train loss: 0.10189123983894077\n",
      "Test loss: 0.09618145674467087\n",
      "Train loss: 0.07282346008079392\n",
      "Test loss: 0.0762104794383049\n",
      "Train loss: 0.057651125958987644\n",
      "Test loss: 0.06216159537434578\n",
      "Train loss: 0.043891087906169045\n",
      "Test loss: 0.05350874662399292\n",
      "Train loss: 0.03454694970111762\n",
      "Test loss: 0.03845533058047294\n",
      "Train loss: 0.03007998038083315\n",
      "Test loss: 0.040523811429739\n",
      "Train loss: 0.02542869267719133\n",
      "Test loss: 0.030320070497691632\n",
      "Train loss: 0.021569779941013882\n",
      "Test loss: 0.02578662596642971\n",
      "Train loss: 0.021035370150847096\n",
      "Test loss: 0.021347801387310027\n",
      "Train loss: 0.0164237154780754\n",
      "Test loss: 0.027851981483399867\n",
      "Train loss: 0.016127016860991716\n",
      "Test loss: 0.01899546030908823\n",
      "Train loss: 0.012630351519744313\n",
      "Test loss: 0.016489765234291555\n",
      "Train loss: 0.01287772147251027\n",
      "Test loss: 0.014599594473838805\n",
      "Train loss: 0.010363629686513116\n",
      "Test loss: 0.011981517169624567\n",
      "Train loss: 0.010661627764680557\n",
      "Test loss: 0.009443106688559055\n",
      "Train loss: 0.008424311044758985\n",
      "Test loss: 0.01125115118920803\n"
     ]
    }
   ],
   "source": [
    "network = MyNetwork(X_train.shape[1], 32, 1, 10)\n",
    "optim = RMSPropOptimizer(network.parameters())\n",
    "\n",
    "num_epochs = 20\n",
    "for i in range(num_epochs):\n",
    "    run_epoch(train_dataset, network, 'Train loss:', optim)\n",
    "    run_epoch(test_dataset, network, 'Test loss:',  None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.5308962932654788\n",
      "Test loss: 0.6812158226966858\n",
      "Train loss: 0.39744293051106594\n",
      "Test loss: 0.2385811150074005\n",
      "Train loss: 0.1660539640911988\n",
      "Test loss: 0.1477382645010948\n",
      "Train loss: 0.1085784448576825\n",
      "Test loss: 0.10334560424089431\n",
      "Train loss: 0.08135044388473034\n",
      "Test loss: 0.07799172103404999\n",
      "Train loss: 0.06676233054271766\n",
      "Test loss: 0.06133635714650154\n",
      "Train loss: 0.05689226569873946\n",
      "Test loss: 0.049878443405032155\n",
      "Train loss: 0.05074265213417155\n",
      "Test loss: 0.04375402368605137\n",
      "Train loss: 0.046708913653024604\n",
      "Test loss: 0.03705931566655636\n",
      "Train loss: 0.04094179879341806\n",
      "Test loss: 0.03237900622189045\n",
      "Train loss: 0.03793670862380948\n",
      "Test loss: 0.031139878556132317\n",
      "Train loss: 0.036193248350173235\n",
      "Test loss: 0.027106764167547225\n",
      "Train loss: 0.032818896363356276\n",
      "Test loss: 0.025554902851581573\n",
      "Train loss: 0.02961194328963756\n",
      "Test loss: 0.022148158773779868\n",
      "Train loss: 0.029320692710046253\n",
      "Test loss: 0.0201693594455719\n",
      "Train loss: 0.027039149975670237\n",
      "Test loss: 0.02080587912350893\n",
      "Train loss: 0.026011472501392876\n",
      "Test loss: 0.020097282156348227\n",
      "Train loss: 0.026040035898664167\n",
      "Test loss: 0.017211951687932013\n",
      "Train loss: 0.023666197034929483\n",
      "Test loss: 0.018052729964256286\n",
      "Train loss: 0.021871779845761403\n",
      "Test loss: 0.016401336900889872\n"
     ]
    }
   ],
   "source": [
    "network = MyNetwork(X_train.shape[1], 32, 1, 10)\n",
    "optim = AdamOptimizer(network.parameters())\n",
    "\n",
    "num_epochs = 20\n",
    "for i in range(num_epochs):\n",
    "    run_epoch(train_dataset, network, 'Train loss:', optim)\n",
    "    run_epoch(test_dataset, network, 'Test loss:',  None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперименты с DropOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот пункт обязателен к выполнению.\n",
    "Для того, чтобы получить бонусный балл за этот пункт, нужно эффективно реализовать DropOut:\n",
    "не вычислять активации выкинутых нейронов, прежде чем их обнулить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseWithDropOut(Module):\n",
    "    def __init__(self, input_units, output_units, dropout_rate, nonlinearity):\n",
    "        \"\"\"A dense layer is a layer which performs a learned\n",
    "        affine transformation and applies dropout:\n",
    "        m ~ Bernoulli(1 - p, size=output_units)\n",
    "        f(x) = g(W x + b) o m\n",
    "        \"\"\"\n",
    "        super(DenseWithDropOut, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.nonlinearity = nonlinearity\n",
    "        # initialize weights with small random numbers from normal distribution\n",
    "        self.weights = torch.empty(input_units, output_units, dtype=torch.float).normal_(mean=0, std=1)  # your code here\n",
    "        self.weights = torch.tensor(self.weights, requires_grad=True)\n",
    "        \n",
    "        self.biases = torch.empty(output_units, dtype=torch.float).normal_(mean=0, std=1)   # your code here\n",
    "        self.biases = torch.tensor(self.biases, requires_grad=True)\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weights, self.biases]\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"Performs an affine transformation with dropout.\n",
    "        In training mode:\n",
    "        m ~ Bernoulli(1 - p, size=output_units)\n",
    "        f(x) = g(W x + b) o m\n",
    "        In evaluation mode:\n",
    "        f(x) = g(W x + b) (1 - p)\n",
    "        input shape:  [batch, input_units]  (Variable)\n",
    "        output shape: [batch, output units] (Variable)\n",
    "        \"\"\"\n",
    "        # your code here        \n",
    "        if self.training:\n",
    "            m = torch.empty(self.weights.shape[1]).bernoulli_(1.0-self.dropout_rate)\n",
    "            nonzero_indices = m.nonzero().squeeze().long()\n",
    "            if nonzero_indices.shape and nonzero_indices.shape[0] > 0:\n",
    "                output = self.nonlinearity((input[nonzero_indices]).t().mm(self.weights[nonzero_indices]).add(self.biases))\n",
    "            else:\n",
    "                output = self.nonlinearity(input.mm(self.weights).add(self.biases))\n",
    "                \n",
    "            # Не сработает - тк все равно нужно будет ходить по всем элементам\n",
    "            # return output = self.nonlinearity(input.mm(self.weights).add(self.biases)) * m\n",
    "        else:\n",
    "            output = (1 - self.dropout_rate) * self.nonlinearity(input.mm(self.weights).add(self.biases))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем, верно ли, что полносвязная сеть с dropout работает быстрее, чем обычная полносвязная сеть, поскольку на каждом проходе вычисляются произведения матриц меньшего размера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseWithDropOut:\n",
      "355 ms ± 16.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Dense:\n"
     ]
    }
   ],
   "source": [
    "width = 2000\n",
    "network1 = [\n",
    "    DenseWithDropOut(width, width, 0.9, lambda x: ReLU().forward(x)),\n",
    "    DenseWithDropOut(width, width, 0.9, lambda x: ReLU().forward(x)),\n",
    "    DenseWithDropOut(width, width, 0.9, lambda x: ReLU().forward(x)),\n",
    "    DenseWithDropOut(width, 1, 0, lambda x: x)\n",
    "]\n",
    "network2 = [\n",
    "    Dense(width, width),\n",
    "    ReLU(),\n",
    "    Dense(width, width),\n",
    "    ReLU(),\n",
    "    Dense(width, width),\n",
    "    ReLU(),\n",
    "    Dense(width, 1)\n",
    "]\n",
    "X = torch.randn(10000, width)\n",
    "\n",
    "# check whether DenseWithDropOut works faster than Dense\n",
    "def test_network(network):\n",
    "    x = torch.tensor(X)\n",
    "    for layer in network:\n",
    "        x = layer.forward(x)\n",
    "    x.mean().backward()\n",
    "    for layer in network:\n",
    "        x = layer.zero_grad()\n",
    "\n",
    "test_network(network1)\n",
    "print(\"DenseWithDropOut:\")\n",
    "%timeit test_network(network1)\n",
    "print(\"Dense:\")\n",
    "%timeit test_network(network2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для более узких слоев, меньших dropout rate и меньших размеров батча увеличение производительности не настолько существенно или может вообще отсутствовать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
